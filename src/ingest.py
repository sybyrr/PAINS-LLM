"""
Ingest module - ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸

JSON ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBì— ì ì¬í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
ë…¼ë¬¸ Section 4.2 (Embedding the data) ì „ëµì„ êµ¬í˜„í•©ë‹ˆë‹¤.

í•µì‹¬ ì „ëµ:
1. JSON ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì„ë² ë”©í•˜ì§€ ì•ŠìŒ
2. ê° íŒŒì¼ì— ëŒ€í•´ "ì„¤ëª… ë¬¸ì¥(Descriptive Sentence)"ì„ ìƒì„±í•˜ì—¬ page_contentë¡œ ì €ì¥
3. ì›ë³¸ JSON ë°ì´í„°ëŠ” metadata í•„ë“œì— ì €ì¥
"""

import json
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import re

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from tqdm import tqdm

from .config import (
    CHROMA_DB_DIR, 
    COLLECTION_NAME, 
    EMBEDDING_MODEL,
    SEASON_DATA_DIR,
    MATCH_DATA_DIR,
    PROCESSED_DATA_DIR
)

from .utils import generate_game_description, generate_descriptive_sentence, TEAM_MAP

# =============================================================================
# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
# =============================================================================

def get_embedding_model() -> HuggingFaceEmbeddings:
    """
    ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    
    ë…¼ë¬¸ì—ì„œ ê²€ì¦ëœ multilingual-e5-large-instruct ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    L2 ì •ê·œí™”ë¥¼ ì ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì— ìµœì í™”í•©ë‹ˆë‹¤.
    
    Returns:
        HuggingFaceEmbeddings: ì´ˆê¸°í™”ëœ ì„ë² ë”© ëª¨ë¸
    """
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},  # GPU ì—†ëŠ” í™˜ê²½ ì§€ì›
        encode_kwargs={
            "normalize_embeddings": True  # L2 ì •ê·œí™” - ë…¼ë¬¸ ê¶Œì¥
        }
    )


# =============================================================================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# =============================================================================

def load_season_data(data_dir: Path = None) -> List[Dict]:
    """
    ì‹œì¦Œ ëˆ„ì  ë°ì´í„°(Global Dataset)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    í˜„ì¬ JSON êµ¬ì¡°:
    [
      {
        "dataset_id": "2025_REGULAR_PITCHING_STATS",
        "name": "2025 Regular league Pitching Stats",
        "type": "player",
        "headers": [...],
        "data": [
          { "Rank": 1, "Name": "...", "Team": "...", "ERA": ..., ... },
          ...
        ]
      }
    ]
    
    Args:
        data_dir: ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        List[Dict]: ë¡œë“œëœ ê°œë³„ ì„ ìˆ˜ ì‹œì¦Œ ë°ì´í„° ëª©ë¡
    """
    if data_dir is None:
        data_dir = SEASON_DATA_DIR
    
    season_data = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âš ï¸ ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        return season_data
    
    json_files = list(data_path.glob("*.json"))
    print(f"ğŸ“‚ ì‹œì¦Œ ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                file_content = json.load(f)
            
            # íŒŒì¼ëª…ì—ì„œ ì‹œì¦Œ íƒ€ì… ì¶”ì¶œ (ì˜ˆ: 2025_POST_PITCHING_STATS)
            filename = json_file.stem
            is_postseason = "POST" in filename.upper()
            season_type = "Post" if is_postseason else "Regular"
            
            # ì—°ë„ ì¶”ì¶œ
            year_match = re.search(r'(\d{4})', filename)
            year = year_match.group(1) if year_match else "2025"
            
            # í†µê³„ íƒ€ì… ì¶”ì¶œ (PITCHING, BATTING ë“±)
            stat_type = "pitching" if "PITCHING" in filename.upper() else "batting"
            
            # ë°°ì—´ ë‚´ ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
            if isinstance(file_content, list):
                for dataset in file_content:
                    dataset_name = dataset.get("name", "")
                    dataset_id = dataset.get("dataset_id", "")
                    
                    # data ë°°ì—´ ë‚´ ê° ë ˆì½”ë“œë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì²˜ë¦¬
                    records = dataset.get("data", [])
                    print(f"   - {json_file.name}: {len(records)}ê°œ ì„ ìˆ˜ ë ˆì½”ë“œ ë°œê²¬")
                    
                    for record in records:
                        # ê°œë³„ ë ˆì½”ë“œì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                        record["_source_file"] = json_file.name
                        record["_data_type"] = "season"
                        record["_loaded_at"] = datetime.now().isoformat()
                        record["_dataset_name"] = dataset_name
                        record["_dataset_id"] = dataset_id
                        record["_season_type"] = season_type
                        record["_stat_type"] = stat_type
                        record["season"] = year
                        
                        # Team í•„ë“œê°€ ìˆìœ¼ë©´ teamìœ¼ë¡œë„ ì €ì¥
                        if "Team" in record:
                            record["team"] = record["Team"]
                            record["teams"] = [record["Team"]]
                        
                        season_data.append(record)
            else:
                # ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° ê¸°ì¡´ ë¡œì§ ìœ ì§€
                file_content["_source_file"] = json_file.name
                file_content["_data_type"] = "season"
                file_content["_loaded_at"] = datetime.now().isoformat()
                
                season_match = re.search(r'(\d{4})', filename)
                if season_match:
                    file_content["season"] = season_match.group(1)
                
                season_data.append(file_content)
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
    
    return season_data


def load_match_data(data_dir: Path = None) -> List[Dict]:
    """
    ê²½ê¸°ë³„ íˆ¬ìˆ˜/íƒ€ì ê¸°ë¡ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    ê°™ì€ ë‚ ì§œ + ê°™ì€ íŒ€ì˜ ê¸°ë¡ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    ì˜ˆ: 2025-10-06 NCíŒ€ì˜ ëª¨ë“  íˆ¬ìˆ˜ ê¸°ë¡ â†’ í•˜ë‚˜ì˜ ë¬¸ì„œ
    
    í˜„ì¬ JSON êµ¬ì¡°:
    [
      {
        "dataset_id": "2025_POST_MATCH_PITCHING_STATS",
        "name": "2025 Post Match Pitching Data",
        "headers": [...],
        "data": [
          { "Season": 2025, "Date": "2025-10-06", "Team": "NC", "Name": "êµ¬ì°½ëª¨", 
            "IP": 22, "ER": 1, "SO": 0, "Result": "ìŠ¹", ... },
          ...
        ]
      }
    ]
    
    Args:
        data_dir: ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        List[Dict]: ë‚ ì§œ+íŒ€ìœ¼ë¡œ ê·¸ë£¹í™”ëœ ê²½ê¸° ê¸°ë¡ ëª©ë¡
    """
    if data_dir is None:
        data_dir = MATCH_DATA_DIR
    
    match_data = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âš ï¸ ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        return match_data
    
    json_files = list(data_path.glob("*.json"))
    print(f"ğŸ“‚ ê²½ê¸° ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                file_content = json.load(f)
            
            # íŒŒì¼ëª…ì—ì„œ ì‹œì¦Œ íƒ€ì… ë° ê¸°ë¡ íƒ€ì… ì¶”ì¶œ
            filename = json_file.stem.upper()
            is_postseason = "POST" in filename
            season_type = "Post" if is_postseason else "Regular"
            
            # íˆ¬ìˆ˜/íƒ€ì ê¸°ë¡ íƒ€ì… ì¶”ì¶œ
            is_pitching = "PITCHING" in filename
            record_type = "pitcher" if is_pitching else "batter"
            
            # ì—°ë„ ì¶”ì¶œ
            year_match = re.search(r'(\d{4})', filename)
            year = year_match.group(1) if year_match else "2025"
            
            # ë°°ì—´ ë‚´ ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
            if isinstance(file_content, list):
                for dataset in file_content:
                    dataset_name = dataset.get("name", "")
                    dataset_id = dataset.get("dataset_id", "")
                    
                    # data ë°°ì—´ ë‚´ ê° ë ˆì½”ë“œë¥¼ ë‚ ì§œ+íŒ€ìœ¼ë¡œ ê·¸ë£¹í™”
                    records = dataset.get("data", [])
                    print(f"   - {json_file.name}: {len(records)}ê°œ ë ˆì½”ë“œ ë°œê²¬")
                    
                    # ë‚ ì§œ+íŒ€ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
                    grouped = {}
                    for record in records:
                        date = record.get("Date", "Unknown")
                        team = record.get("Team", "Unknown")
                        key = (date, team)
                        
                        if key not in grouped:
                            grouped[key] = []
                        grouped[key].append(record)
                    
                    print(f"     â†’ {len(grouped)}ê°œ ê²½ê¸°(ë‚ ì§œ+íŒ€)ë¡œ ê·¸ë£¹í™”")
                    
                    # ê·¸ë£¹í™”ëœ ë°ì´í„°ë¥¼ ë¬¸ì„œë¡œ ë³€í™˜
                    for (date, team), player_records in grouped.items():
                        grouped_doc = {
                            "_source_file": json_file.name,
                            "_data_type": "match",
                            "_loaded_at": datetime.now().isoformat(),
                            "_dataset_name": dataset_name,
                            "_dataset_id": dataset_id,
                            "_season_type": season_type,
                            "_record_type": record_type,
                            "_year": year,
                            "Date": date,
                            "date": date,
                            "Team": team,
                            "teams": [team],
                            "players": player_records  # í•´ë‹¹ ê²½ê¸°ì˜ ëª¨ë“  ì„ ìˆ˜ ê¸°ë¡
                        }
                        match_data.append(grouped_doc)
            else:
                print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ JSON êµ¬ì¡°: {json_file.name}")
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
    
    return match_data


# === ì „ì²˜ë¦¬ëœ ê²½ê¸°ë³„ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ===

def load_processed_game_data(data_dir: Path = None) -> List[Dict]:
    """
    ì „ì²˜ë¦¬ëœ ê²½ê¸°ë³„ JSON ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    preprocess.ipynbì—ì„œ ìƒì„±í•œ data/processed/matches/ í´ë”ì˜ JSON íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.
    ê° JSON íŒŒì¼ì€ ê²½ê¸°ë³„ë¡œ ì–‘ íŒ€ íˆ¬ìˆ˜ ê¸°ë¡ì´ í†µí•©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    
    Args:
        data_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ (Noneì¼ ê²½ìš° PROCESSED_DATA_DIR ì‚¬ìš©)
    
    Returns:
        List[Dict]: ê²½ê¸°ë³„ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    if data_dir is None:
        data_dir = PROCESSED_DATA_DIR
    
    game_data = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âš ï¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        print("   ë¨¼ì € preprocess.ipynbë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ì„¸ìš”.")
        return game_data
    
    json_files = list(data_path.glob("*.json"))
    print(f"ğŸ“‚ ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                content = json.load(f)
            
            # JSON êµ¬ì¡°: [ { "dataset_id": ..., "data": [...] }, ... ]
            if isinstance(content, list):
                for dataset in content:
                    games = dataset.get('data', [])
                    print(f"   - {json_file.name}: {len(games)}ê°œ ê²½ê¸°")
                    game_data.extend(games)
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
    
    return game_data


def prepare_game_documents(game_data: List[Dict]) -> List[Document]:
    """
    ì „ì²˜ë¦¬ëœ ê²½ê¸°ë³„ ë°ì´í„°ë¥¼ LangChain Documentë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    ê° ê²½ê¸°(ì–‘ íŒ€ íˆ¬ìˆ˜ ê¸°ë¡ í†µí•©)ì— ëŒ€í•´ í•˜ë‚˜ì˜ Documentë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        game_data: ê²½ê¸°ë³„ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        List[Document]: LangChain Document ë¦¬ìŠ¤íŠ¸
    """
    documents = []
    
    print(f"ğŸ”„ {len(game_data)}ê°œì˜ ê²½ê¸° ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜ ì¤‘...")
    
    for game in game_data:
        # 1. ì„¤ëª… ë¬¸ì¥ ìƒì„± (ì„ë² ë”© ëŒ€ìƒ)
        description = generate_game_description(game)
        
        # 2. ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "type": "game",
            "game_id": game.get("game_id", ""),
            "date": game.get("date", ""),
            "home_team": game.get("home_team", ""),
            "away_team": game.get("away_team", ""),
            "home_runs": game.get("home_runs", ""),
            "away_runs": game.get("away_runs", ""),
            "season_type": game.get("season_type", "Regular"),
            "home_pitcher_count": game.get("home_pitcher_count", 0),
            "away_pitcher_count": game.get("away_pitcher_count", 0),
            "total_pitcher_count": game.get("total_pitcher_count", 0),
            # ì›ë³¸ ë°ì´í„° ì €ì¥ (RAG ê²€ìƒ‰ í›„ ë³µì›ìš©)
            "original_data": json.dumps(game, ensure_ascii=False)
        }
        
        doc = Document(page_content=description, metadata=metadata)
        documents.append(doc)
    
    return documents


def prepare_season_documents(season_data: List[Dict]) -> List[Document]:
    """
    ì‹œì¦Œ ë°ì´í„°ë¥¼ LangChain Documentë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        season_data: ì‹œì¦Œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        List[Document]: LangChain Document ë¦¬ìŠ¤íŠ¸
    """
    documents = []
    
    print(f"ğŸ”„ {len(season_data)}ê°œì˜ ì‹œì¦Œ ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜ ì¤‘...")
    
    for record in season_data:
        # 1. ì„¤ëª… ë¬¸ì¥ ìƒì„±
        description = generate_descriptive_sentence(record, "season")
        
        # 2. ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "type": "season",
            "season": str(record.get("season", "2025")),
            "season_type": record.get("_season_type", "Regular"),
            "team": record.get("Team", record.get("team", "")),
            "teams": record.get("Team", record.get("team", "")), # For consistency with match data
            "name": record.get("Name", ""),
            "stat_type": record.get("_stat_type", "pitching"),
            # ì›ë³¸ ë°ì´í„° ì €ì¥
            "original_data": json.dumps(record, ensure_ascii=False)
        }
        
        doc = Document(page_content=description, metadata=metadata)
        documents.append(doc)
        
    return documents


# =============================================================================
# ChromaDB ì ì¬ í•¨ìˆ˜
# =============================================================================

def initialize_vector_store(
    documents: List[Document] = None,
    persist_directory: str = None,
    collection_name: str = None,
    batch_size: int = 100
) -> Chroma:
    """
    ChromaDB ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    ì„±ëŠ¥ ìµœì í™”:
    - ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    - tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    
    Args:
        documents: ì ì¬í•  Document ëª©ë¡ (Noneì´ë©´ ê¸°ì¡´ ìŠ¤í† ì–´ ë¡œë“œ)
        persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 100)
    
    Returns:
        Chroma: ì´ˆê¸°í™”ëœ ë²¡í„° ìŠ¤í† ì–´
    """
    if persist_directory is None:
        persist_directory = str(CHROMA_DB_DIR)
    
    if collection_name is None:
        collection_name = COLLECTION_NAME
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = get_embedding_model()
    
    if documents:
        # ìƒˆë¡œìš´ ë¬¸ì„œë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print(f"\nğŸ“¦ ChromaDB ì´ˆê¸°í™” ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)})")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œì”© ì²˜ë¦¬")
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        first_batch = documents[:batch_size]
        vector_store = Chroma.from_documents(
            documents=first_batch,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=persist_directory
        )
        
        # ë‚˜ë¨¸ì§€ ë°°ì¹˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€
        remaining_docs = documents[batch_size:]
        if remaining_docs:
            total_batches = (len(remaining_docs) + batch_size - 1) // batch_size
            print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ë° ì ì¬ ì¤‘... ({total_batches}ê°œ ë°°ì¹˜)")
            
            for i in tqdm(range(0, len(remaining_docs), batch_size), desc="Batches"):
                batch = remaining_docs[i:i + batch_size]
                vector_store.add_documents(batch)
        
        print(f"âœ… ChromaDB ì ì¬ ì™„ë£Œ: {persist_directory}")
        
    else:
        # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        print(f"\nğŸ“‚ ê¸°ì¡´ ChromaDB ë¡œë“œ ì¤‘: {persist_directory}")
        
        vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=embeddings,
            persist_directory=persist_directory
        )
    
    return vector_store


def clear_vector_store(persist_directory: str = None, collection_name: str = None):
    """
    ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
    """
    if persist_directory is None:
        persist_directory = str(CHROMA_DB_DIR)
    
    if collection_name is None:
        collection_name = COLLECTION_NAME
    
    import shutil
    persist_path = Path(persist_directory)
    
    if persist_path.exists():
        shutil.rmtree(persist_path)
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ: {persist_directory}")


# =============================================================================
# ë©”ì¸ ì ì¬ íŒŒì´í”„ë¼ì¸
# =============================================================================

def ingest_all_data(
    processed_dir: Path = None,
    season_dir: Path = None,
    clear_existing: bool = True
) -> Chroma:
    """
    ì „ì²˜ë¦¬ëœ ê²½ê¸°ë³„ ë°ì´í„°ì™€ ì‹œì¦Œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBì— ì ì¬í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    
    Args:
        processed_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ (Noneì¼ ê²½ìš° config ê¸°ë³¸ê°’ ì‚¬ìš©)
        season_dir: ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ (Noneì¼ ê²½ìš° config ê¸°ë³¸ê°’ ì‚¬ìš©)
        clear_existing: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì—¬ë¶€ (ê¸°ë³¸ê°’ True)
    
    Returns:
        Chroma: ì´ˆê¸°í™”ëœ ë²¡í„° ìŠ¤í† ì–´
    
    Note:
        ì´ í•¨ìˆ˜ ì‹¤í–‰ ì „ì— preprocess.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ 
        data/processed/matches/ í´ë”ì— ì „ì²˜ë¦¬ëœ JSON íŒŒì¼ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    print("=" * 60)
    print("ğŸš€ KBO ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì„ íƒì )
    if clear_existing:
        clear_vector_store()
    
    # 2. ì „ì²˜ë¦¬ëœ ê²½ê¸°ë³„ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“¥ ì „ì²˜ë¦¬ëœ ê²½ê¸° ë°ì´í„° ë¡œë“œ ì¤‘...")
    game_data = load_processed_game_data(processed_dir)
    
    # 3. ì‹œì¦Œ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“¥ ì‹œì¦Œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    season_data = load_season_data(season_dir)
    
    if not game_data and not season_data:
        print("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € preprocess.ipynbë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê±°ë‚˜ ì‹œì¦Œ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None
    
    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ìš”ì•½:")
    print(f"   - ê²½ê¸° ìˆ˜: {len(game_data)}")
    print(f"   - ì‹œì¦Œ ë°ì´í„° ë ˆì½”ë“œ: {len(season_data)}")
    
    # ì‹œì¦Œë³„ í†µê³„
    regular_games = [g for g in game_data if g.get('season_type') == 'Regular']
    post_games = [g for g in game_data if g.get('season_type') == 'Post']
    print(f"   - ì •ê·œì‹œì¦Œ: {len(regular_games)}ê²½ê¸°")
    print(f"   - í¬ìŠ¤íŠ¸ì‹œì¦Œ: {len(post_games)}ê²½ê¸°")
    
    # 4. Document ë³€í™˜
    documents = []
    
    if game_data:
        documents.extend(prepare_game_documents(game_data))
        
    if season_data:
        documents.extend(prepare_season_documents(season_data))
    
    if not documents:
        print("âš ï¸ ìƒì„±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ë¬´ê²°ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

    # 5. ChromaDB ì ì¬
    vector_store = initialize_vector_store(documents)
    
    # 6. ê²°ê³¼ í™•ì¸
    doc_count = vector_store._collection.count()
    print(f"\nâœ… ì ì¬ ì™„ë£Œ! ì´ {doc_count}ê°œì˜ ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 60)
    
    return vector_store


# =============================================================================
# CLI ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    """
    ëª…ë ¹ì¤„ì—ì„œ ì§ì ‘ ì‹¤í–‰:
    python -m src.ingest
    """
    vector_store = ingest_all_data()
    
    if vector_store:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰...")
        results = vector_store.similarity_search("í•œí™” ì´ê¸€ìŠ¤ ì‹œì¦Œ ì„±ì ", k=3)
        
        for i, doc in enumerate(results, 1):
            print(f"\nê²°ê³¼ {i}:")
            print(f"  ë‚´ìš©: {doc.page_content[:100]}...")
            print(f"  íƒ€ì…: {doc.metadata.get('type')}")
