"""
Ingest module - ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸

JSON ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBì— ì ì¬í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
ë…¼ë¬¸ Section 4.2 (Embedding the data) ì „ëµì„ êµ¬í˜„í•©ë‹ˆë‹¤.

í•µì‹¬ ì „ëµ:
1. JSON ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì„ë² ë”©í•˜ì§€ ì•ŠìŒ
2. ê° íŒŒì¼ì— ëŒ€í•´ "ì„¤ëª… ë¬¸ì¥(Descriptive Sentence)"ì„ ìƒì„±í•˜ì—¬ page_contentë¡œ ì €ì¥
3. ì›ë³¸ JSON ë°ì´í„°ëŠ” metadata í•„ë“œì— ì €ì¥
"""

import json
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import re

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from tqdm import tqdm

from .config import (
    CHROMA_DB_DIR, 
    COLLECTION_NAME, 
    EMBEDDING_MODEL,
    SEASON_DATA_DIR,
    MATCH_DATA_DIR
)
from .utils import generate_descriptive_sentence, TEAM_EN_TO_KO


# =============================================================================
# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
# =============================================================================

def get_embedding_model() -> HuggingFaceEmbeddings:
    """
    ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    
    ë…¼ë¬¸ì—ì„œ ê²€ì¦ëœ multilingual-e5-large-instruct ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    L2 ì •ê·œí™”ë¥¼ ì ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì— ìµœì í™”í•©ë‹ˆë‹¤.
    
    Returns:
        HuggingFaceEmbeddings: ì´ˆê¸°í™”ëœ ì„ë² ë”© ëª¨ë¸
    """
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},  # GPU ì—†ëŠ” í™˜ê²½ ì§€ì›
        encode_kwargs={
            "normalize_embeddings": True  # L2 ì •ê·œí™” - ë…¼ë¬¸ ê¶Œì¥
        }
    )


# =============================================================================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# =============================================================================

def load_season_data(data_dir: Path = None) -> List[Dict]:
    """
    ì‹œì¦Œ ëˆ„ì  ë°ì´í„°(Global Dataset)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    í˜„ì¬ JSON êµ¬ì¡°:
    [
      {
        "dataset_id": "2025_REGULAR_PITCHING_STATS",
        "name": "2025 Regular league Pitching Stats",
        "type": "player",
        "headers": [...],
        "data": [
          { "Rank": 1, "Name": "...", "Team": "...", "ERA": ..., ... },
          ...
        ]
      }
    ]
    
    Args:
        data_dir: ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        List[Dict]: ë¡œë“œëœ ê°œë³„ ì„ ìˆ˜ ì‹œì¦Œ ë°ì´í„° ëª©ë¡
    """
    if data_dir is None:
        data_dir = SEASON_DATA_DIR
    
    season_data = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âš ï¸ ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        return season_data
    
    json_files = list(data_path.glob("*.json"))
    print(f"ğŸ“‚ ì‹œì¦Œ ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                file_content = json.load(f)
            
            # íŒŒì¼ëª…ì—ì„œ ì‹œì¦Œ íƒ€ì… ì¶”ì¶œ (ì˜ˆ: 2025_POST_PITCHING_STATS)
            filename = json_file.stem
            is_postseason = "POST" in filename.upper()
            season_type = "Post" if is_postseason else "Regular"
            
            # ì—°ë„ ì¶”ì¶œ
            year_match = re.search(r'(\d{4})', filename)
            year = year_match.group(1) if year_match else "2025"
            
            # í†µê³„ íƒ€ì… ì¶”ì¶œ (PITCHING, BATTING ë“±)
            stat_type = "pitching" if "PITCHING" in filename.upper() else "batting"
            
            # ë°°ì—´ ë‚´ ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
            if isinstance(file_content, list):
                for dataset in file_content:
                    dataset_name = dataset.get("name", "")
                    dataset_id = dataset.get("dataset_id", "")
                    
                    # data ë°°ì—´ ë‚´ ê° ë ˆì½”ë“œë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì²˜ë¦¬
                    records = dataset.get("data", [])
                    print(f"   - {json_file.name}: {len(records)}ê°œ ì„ ìˆ˜ ë ˆì½”ë“œ ë°œê²¬")
                    
                    for record in records:
                        # ê°œë³„ ë ˆì½”ë“œì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                        record["_source_file"] = json_file.name
                        record["_data_type"] = "season"
                        record["_loaded_at"] = datetime.now().isoformat()
                        record["_dataset_name"] = dataset_name
                        record["_dataset_id"] = dataset_id
                        record["_season_type"] = season_type
                        record["_stat_type"] = stat_type
                        record["season"] = year
                        
                        # Team í•„ë“œê°€ ìˆìœ¼ë©´ teamìœ¼ë¡œë„ ì €ì¥
                        if "Team" in record:
                            record["team"] = record["Team"]
                            record["teams"] = [record["Team"]]
                        
                        season_data.append(record)
            else:
                # ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° ê¸°ì¡´ ë¡œì§ ìœ ì§€
                file_content["_source_file"] = json_file.name
                file_content["_data_type"] = "season"
                file_content["_loaded_at"] = datetime.now().isoformat()
                
                season_match = re.search(r'(\d{4})', filename)
                if season_match:
                    file_content["season"] = season_match.group(1)
                
                season_data.append(file_content)
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
    
    return season_data


def load_match_data(data_dir: Path = None) -> List[Dict]:
    """
    ê²½ê¸°ë³„ íˆ¬ìˆ˜/íƒ€ì ê¸°ë¡ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    ê°™ì€ ë‚ ì§œ + ê°™ì€ íŒ€ì˜ ê¸°ë¡ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    ì˜ˆ: 2025-10-06 NCíŒ€ì˜ ëª¨ë“  íˆ¬ìˆ˜ ê¸°ë¡ â†’ í•˜ë‚˜ì˜ ë¬¸ì„œ
    
    í˜„ì¬ JSON êµ¬ì¡°:
    [
      {
        "dataset_id": "2025_POST_MATCH_PITCHING_STATS",
        "name": "2025 Post Match Pitching Data",
        "headers": [...],
        "data": [
          { "Season": 2025, "Date": "2025-10-06", "Team": "NC", "Name": "êµ¬ì°½ëª¨", 
            "IP": 22, "ER": 1, "SO": 0, "Result": "ìŠ¹", ... },
          ...
        ]
      }
    ]
    
    Args:
        data_dir: ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        List[Dict]: ë‚ ì§œ+íŒ€ìœ¼ë¡œ ê·¸ë£¹í™”ëœ ê²½ê¸° ê¸°ë¡ ëª©ë¡
    """
    if data_dir is None:
        data_dir = MATCH_DATA_DIR
    
    match_data = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âš ï¸ ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        return match_data
    
    json_files = list(data_path.glob("*.json"))
    print(f"ğŸ“‚ ê²½ê¸° ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                file_content = json.load(f)
            
            # íŒŒì¼ëª…ì—ì„œ ì‹œì¦Œ íƒ€ì… ë° ê¸°ë¡ íƒ€ì… ì¶”ì¶œ
            filename = json_file.stem.upper()
            is_postseason = "POST" in filename
            season_type = "Post" if is_postseason else "Regular"
            
            # íˆ¬ìˆ˜/íƒ€ì ê¸°ë¡ íƒ€ì… ì¶”ì¶œ
            is_pitching = "PITCHING" in filename
            record_type = "pitcher" if is_pitching else "batter"
            
            # ì—°ë„ ì¶”ì¶œ
            year_match = re.search(r'(\d{4})', filename)
            year = year_match.group(1) if year_match else "2025"
            
            # ë°°ì—´ ë‚´ ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
            if isinstance(file_content, list):
                for dataset in file_content:
                    dataset_name = dataset.get("name", "")
                    dataset_id = dataset.get("dataset_id", "")
                    
                    # data ë°°ì—´ ë‚´ ê° ë ˆì½”ë“œë¥¼ ë‚ ì§œ+íŒ€ìœ¼ë¡œ ê·¸ë£¹í™”
                    records = dataset.get("data", [])
                    print(f"   - {json_file.name}: {len(records)}ê°œ ë ˆì½”ë“œ ë°œê²¬")
                    
                    # ë‚ ì§œ+íŒ€ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
                    grouped = {}
                    for record in records:
                        date = record.get("Date", "Unknown")
                        team = record.get("Team", "Unknown")
                        key = (date, team)
                        
                        if key not in grouped:
                            grouped[key] = []
                        grouped[key].append(record)
                    
                    print(f"     â†’ {len(grouped)}ê°œ ê²½ê¸°(ë‚ ì§œ+íŒ€)ë¡œ ê·¸ë£¹í™”")
                    
                    # ê·¸ë£¹í™”ëœ ë°ì´í„°ë¥¼ ë¬¸ì„œë¡œ ë³€í™˜
                    for (date, team), player_records in grouped.items():
                        grouped_doc = {
                            "_source_file": json_file.name,
                            "_data_type": "match",
                            "_loaded_at": datetime.now().isoformat(),
                            "_dataset_name": dataset_name,
                            "_dataset_id": dataset_id,
                            "_season_type": season_type,
                            "_record_type": record_type,
                            "_year": year,
                            "Date": date,
                            "date": date,
                            "Team": team,
                            "teams": [team],
                            "players": player_records  # í•´ë‹¹ ê²½ê¸°ì˜ ëª¨ë“  ì„ ìˆ˜ ê¸°ë¡
                        }
                        match_data.append(grouped_doc)
            else:
                print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ JSON êµ¬ì¡°: {json_file.name}")
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
    
    return match_data


# =============================================================================
# ë¬¸ì„œ ë³€í™˜ í•¨ìˆ˜
# =============================================================================

def create_document_from_data(data: Dict, data_type: str) -> Document:
    """
    JSON ë°ì´í„°ë¥¼ LangChain Documentë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    í•µì‹¬ ì „ëµ (ë…¼ë¬¸ Section 4.2):
    - page_content: ì„¤ëª… ë¬¸ì¥ (ì„ë² ë”© ëŒ€ìƒ)
    - metadata: ì›ë³¸ JSON + í•„í„°ë§ìš© ë©”íƒ€ë°ì´í„°
    
    Args:
        data: JSON ë°ì´í„°
        data_type: "season" ë˜ëŠ” "match"
    
    Returns:
        Document: LangChain Document ê°ì²´
    """
    # 1. ì„¤ëª… ë¬¸ì¥ ìƒì„± (ì„ë² ë”© ëŒ€ìƒ)
    descriptive_sentence = generate_descriptive_sentence(data, data_type)
    
    # Instruct ëª¨ë¸ìš© ì¿¼ë¦¬ í¬ë§·
    # ë…¼ë¬¸ Section 4.4.1ì—ì„œ ê²€ì¦ëœ ë°©ì‹
    embedding_text = (
        f"Instruct: Find the baseball dataset covering the given team(s) and statistics. "
        f"Query: {descriptive_sentence}"
    )
    
    # 2. ë©”íƒ€ë°ì´í„° êµ¬ì„±
    metadata = {
        "type": data_type,
        "source_file": data.get("_source_file", ""),
        "raw_data": json.dumps(data, ensure_ascii=False),  # ì›ë³¸ JSON ì €ì¥
    }
    
    # ì‹œì¦Œ ë°ì´í„° ë©”íƒ€ë°ì´í„°
    if data_type == "season":
        metadata["season"] = data.get("season", "2025")
        metadata["team"] = data.get("Team", data.get("team", ""))
        metadata["stat_type"] = data.get("_stat_type", "")
        metadata["season_type"] = data.get("_season_type", "Regular")
        metadata["player_name"] = data.get("Name", "")
        # teams í•„ë“œ: ë©”íƒ€ë°ì´í„° í•„í„°ë§ìš© (ChromaDBëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‰¼í‘œ êµ¬ë¶„ ë¬¸ìì—´ë¡œ ì €ì¥)
        if data.get("teams"):
            metadata["teams"] = ",".join(data.get("teams"))
        elif data.get("Team"):
            metadata["teams"] = data.get("Team")
    
    # ê²½ê¸° ë°ì´í„° ë©”íƒ€ë°ì´í„° (ë‚ ì§œ+íŒ€ ê·¸ë£¹í™”ëœ íˆ¬ìˆ˜/íƒ€ì ê¸°ë¡)
    elif data_type == "match":
        metadata["date"] = data.get("date", data.get("Date", ""))
        # teams í•„ë“œ: ChromaDBëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‰¼í‘œ êµ¬ë¶„ ë¬¸ìì—´ë¡œ ì €ì¥
        teams_list = data.get("teams", [])
        metadata["teams"] = ",".join(teams_list) if teams_list else ""
        metadata["team"] = data.get("Team", "")
        metadata["season_type"] = data.get("_season_type", "Regular")
        metadata["year"] = data.get("_year", "2025")
        metadata["record_type"] = data.get("_record_type", "pitcher")
        
        # ê·¸ë£¹í™”ëœ ì„ ìˆ˜ ëª©ë¡ (ì´ë¦„ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥)
        players = data.get("players", [])
        player_names = [p.get("Name", "") for p in players if p.get("Name")]
        metadata["player_names"] = ",".join(player_names) if player_names else ""
        metadata["player_count"] = len(players)
    
    return Document(
        page_content=embedding_text,
        metadata=metadata
    )


def prepare_documents(season_data: List[Dict], match_data: List[Dict]) -> List[Document]:
    """
    ëª¨ë“  ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        season_data: ì‹œì¦Œ ë°ì´í„° ëª©ë¡
        match_data: ê²½ê¸° ë°ì´í„° ëª©ë¡
    
    Returns:
        List[Document]: ë³€í™˜ëœ Document ëª©ë¡
    """
    documents = []
    
    print("\nğŸ”„ ì‹œì¦Œ ë°ì´í„° ë³€í™˜ ì¤‘...")
    for data in tqdm(season_data, desc="Season"):
        doc = create_document_from_data(data, "season")
        documents.append(doc)
    
    print("\nğŸ”„ ê²½ê¸° ë°ì´í„° ë³€í™˜ ì¤‘...")
    for data in tqdm(match_data, desc="Match"):
        doc = create_document_from_data(data, "match")
        documents.append(doc)
    
    return documents


# =============================================================================
# ChromaDB ì ì¬ í•¨ìˆ˜
# =============================================================================

def initialize_vector_store(
    documents: List[Document] = None,
    persist_directory: str = None,
    collection_name: str = None,
    batch_size: int = 100
) -> Chroma:
    """
    ChromaDB ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    ì„±ëŠ¥ ìµœì í™”:
    - ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    - tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    
    Args:
        documents: ì ì¬í•  Document ëª©ë¡ (Noneì´ë©´ ê¸°ì¡´ ìŠ¤í† ì–´ ë¡œë“œ)
        persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 100)
    
    Returns:
        Chroma: ì´ˆê¸°í™”ëœ ë²¡í„° ìŠ¤í† ì–´
    """
    if persist_directory is None:
        persist_directory = str(CHROMA_DB_DIR)
    
    if collection_name is None:
        collection_name = COLLECTION_NAME
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = get_embedding_model()
    
    if documents:
        # ìƒˆë¡œìš´ ë¬¸ì„œë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print(f"\nğŸ“¦ ChromaDB ì´ˆê¸°í™” ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)})")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œì”© ì²˜ë¦¬")
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        first_batch = documents[:batch_size]
        vector_store = Chroma.from_documents(
            documents=first_batch,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=persist_directory
        )
        
        # ë‚˜ë¨¸ì§€ ë°°ì¹˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€
        remaining_docs = documents[batch_size:]
        if remaining_docs:
            total_batches = (len(remaining_docs) + batch_size - 1) // batch_size
            print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ë° ì ì¬ ì¤‘... ({total_batches}ê°œ ë°°ì¹˜)")
            
            for i in tqdm(range(0, len(remaining_docs), batch_size), desc="Batches"):
                batch = remaining_docs[i:i + batch_size]
                vector_store.add_documents(batch)
        
        print(f"âœ… ChromaDB ì ì¬ ì™„ë£Œ: {persist_directory}")
        
    else:
        # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        print(f"\nğŸ“‚ ê¸°ì¡´ ChromaDB ë¡œë“œ ì¤‘: {persist_directory}")
        
        vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=embeddings,
            persist_directory=persist_directory
        )
    
    return vector_store


def clear_vector_store(persist_directory: str = None, collection_name: str = None):
    """
    ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
    """
    if persist_directory is None:
        persist_directory = str(CHROMA_DB_DIR)
    
    if collection_name is None:
        collection_name = COLLECTION_NAME
    
    import shutil
    persist_path = Path(persist_directory)
    
    if persist_path.exists():
        shutil.rmtree(persist_path)
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ: {persist_directory}")


# =============================================================================
# ë©”ì¸ ì ì¬ íŒŒì´í”„ë¼ì¸
# =============================================================================

def ingest_all_data(
    season_dir: Path = None,
    match_dir: Path = None,
    clear_existing: bool = True
) -> Chroma:
    """
    ëª¨ë“  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBì— ì ì¬í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    
    Args:
        season_dir: ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬
        match_dir: ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬
        clear_existing: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì—¬ë¶€
    
    Returns:
        Chroma: ì´ˆê¸°í™”ëœ ë²¡í„° ìŠ¤í† ì–´
    
    Example:
        >>> from src.ingest import ingest_all_data
        >>> vector_store = ingest_all_data()
        >>> print(f"ì ì¬ëœ ë¬¸ì„œ ìˆ˜: {vector_store._collection.count()}")
    """
    print("=" * 60)
    print("ğŸš€ KBO ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì„ íƒì )
    if clear_existing:
        clear_vector_store()
    
    # 2. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
    season_data = load_season_data(season_dir)
    match_data = load_match_data(match_dir)
    
    if not season_data and not match_data:
        print("âš ï¸ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None
    
    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ìš”ì•½:")
    print(f"   - ì‹œì¦Œ ë°ì´í„°: {len(season_data)}ê±´")
    print(f"   - ê²½ê¸° ë°ì´í„°: {len(match_data)}ê±´")
    
    # 3. Document ë³€í™˜
    documents = prepare_documents(season_data, match_data)
    
    # 4. ChromaDB ì ì¬
    vector_store = initialize_vector_store(documents)
    
    # 5. ê²°ê³¼ í™•ì¸
    doc_count = vector_store._collection.count()
    print(f"\nâœ… ì ì¬ ì™„ë£Œ! ì´ {doc_count}ê°œì˜ ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 60)
    
    return vector_store


# =============================================================================
# CLI ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    """
    ëª…ë ¹ì¤„ì—ì„œ ì§ì ‘ ì‹¤í–‰:
    python -m src.ingest
    """
    vector_store = ingest_all_data()
    
    if vector_store:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰...")
        results = vector_store.similarity_search("í•œí™” ì´ê¸€ìŠ¤ ì‹œì¦Œ ì„±ì ", k=3)
        
        for i, doc in enumerate(results, 1):
            print(f"\nê²°ê³¼ {i}:")
            print(f"  ë‚´ìš©: {doc.page_content[:100]}...")
            print(f"  íƒ€ì…: {doc.metadata.get('type')}")
