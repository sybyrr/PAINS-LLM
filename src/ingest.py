"""
Ingest module - ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸

JSON ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBì— ì ì¬í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
ë…¼ë¬¸ Section 4.2 (Embedding the data) ì „ëµì„ êµ¬í˜„í•©ë‹ˆë‹¤.

í•µì‹¬ ì „ëµ:
1. JSON ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì„ë² ë”©í•˜ì§€ ì•ŠìŒ
2. ê° íŒŒì¼ì— ëŒ€í•´ "ì„¤ëª… ë¬¸ì¥(Descriptive Sentence)"ì„ ìƒì„±í•˜ì—¬ page_contentë¡œ ì €ì¥
3. ì›ë³¸ JSON ë°ì´í„°ëŠ” metadata í•„ë“œì— ì €ì¥
"""

import json
import csv # ì¶”ê°€
from itertools import groupby # ì¶”ê°€
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import re

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from tqdm import tqdm

from .config import (
    CHROMA_DB_DIR, 
    COLLECTION_NAME, 
    EMBEDDING_MODEL,
    SEASON_DATA_DIR,
    MATCH_DATA_DIR
)

# from .utils import generate_descriptive_sentence, TEAM_EN_TO_KO
from .utils import generate_game_description, TEAM_MAP

# =============================================================================
# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
# =============================================================================

def get_embedding_model() -> HuggingFaceEmbeddings:
    """
    ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    
    ë…¼ë¬¸ì—ì„œ ê²€ì¦ëœ multilingual-e5-large-instruct ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    L2 ì •ê·œí™”ë¥¼ ì ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì— ìµœì í™”í•©ë‹ˆë‹¤.
    
    Returns:
        HuggingFaceEmbeddings: ì´ˆê¸°í™”ëœ ì„ë² ë”© ëª¨ë¸
    """
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},  # GPU ì—†ëŠ” í™˜ê²½ ì§€ì›
        encode_kwargs={
            "normalize_embeddings": True  # L2 ì •ê·œí™” - ë…¼ë¬¸ ê¶Œì¥
        }
    )


# =============================================================================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# =============================================================================

def load_season_data(data_dir: Path = None) -> List[Dict]:
    """
    ì‹œì¦Œ ëˆ„ì  ë°ì´í„°(Global Dataset)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    í˜„ì¬ JSON êµ¬ì¡°:
    [
      {
        "dataset_id": "2025_REGULAR_PITCHING_STATS",
        "name": "2025 Regular league Pitching Stats",
        "type": "player",
        "headers": [...],
        "data": [
          { "Rank": 1, "Name": "...", "Team": "...", "ERA": ..., ... },
          ...
        ]
      }
    ]
    
    Args:
        data_dir: ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        List[Dict]: ë¡œë“œëœ ê°œë³„ ì„ ìˆ˜ ì‹œì¦Œ ë°ì´í„° ëª©ë¡
    """
    if data_dir is None:
        data_dir = SEASON_DATA_DIR
    
    season_data = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âš ï¸ ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        return season_data
    
    json_files = list(data_path.glob("*.json"))
    print(f"ğŸ“‚ ì‹œì¦Œ ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                file_content = json.load(f)
            
            # íŒŒì¼ëª…ì—ì„œ ì‹œì¦Œ íƒ€ì… ì¶”ì¶œ (ì˜ˆ: 2025_POST_PITCHING_STATS)
            filename = json_file.stem
            is_postseason = "POST" in filename.upper()
            season_type = "Post" if is_postseason else "Regular"
            
            # ì—°ë„ ì¶”ì¶œ
            year_match = re.search(r'(\d{4})', filename)
            year = year_match.group(1) if year_match else "2025"
            
            # í†µê³„ íƒ€ì… ì¶”ì¶œ (PITCHING, BATTING ë“±)
            stat_type = "pitching" if "PITCHING" in filename.upper() else "batting"
            
            # ë°°ì—´ ë‚´ ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
            if isinstance(file_content, list):
                for dataset in file_content:
                    dataset_name = dataset.get("name", "")
                    dataset_id = dataset.get("dataset_id", "")
                    
                    # data ë°°ì—´ ë‚´ ê° ë ˆì½”ë“œë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì²˜ë¦¬
                    records = dataset.get("data", [])
                    print(f"   - {json_file.name}: {len(records)}ê°œ ì„ ìˆ˜ ë ˆì½”ë“œ ë°œê²¬")
                    
                    for record in records:
                        # ê°œë³„ ë ˆì½”ë“œì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                        record["_source_file"] = json_file.name
                        record["_data_type"] = "season"
                        record["_loaded_at"] = datetime.now().isoformat()
                        record["_dataset_name"] = dataset_name
                        record["_dataset_id"] = dataset_id
                        record["_season_type"] = season_type
                        record["_stat_type"] = stat_type
                        record["season"] = year
                        
                        # Team í•„ë“œê°€ ìˆìœ¼ë©´ teamìœ¼ë¡œë„ ì €ì¥
                        if "Team" in record:
                            record["team"] = record["Team"]
                            record["teams"] = [record["Team"]]
                        
                        season_data.append(record)
            else:
                # ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° ê¸°ì¡´ ë¡œì§ ìœ ì§€
                file_content["_source_file"] = json_file.name
                file_content["_data_type"] = "season"
                file_content["_loaded_at"] = datetime.now().isoformat()
                
                season_match = re.search(r'(\d{4})', filename)
                if season_match:
                    file_content["season"] = season_match.group(1)
                
                season_data.append(file_content)
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
    
    return season_data


def load_match_data(data_dir: Path = None) -> List[Dict]:
    """
    ê²½ê¸°ë³„ íˆ¬ìˆ˜/íƒ€ì ê¸°ë¡ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    ê°™ì€ ë‚ ì§œ + ê°™ì€ íŒ€ì˜ ê¸°ë¡ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    ì˜ˆ: 2025-10-06 NCíŒ€ì˜ ëª¨ë“  íˆ¬ìˆ˜ ê¸°ë¡ â†’ í•˜ë‚˜ì˜ ë¬¸ì„œ
    
    í˜„ì¬ JSON êµ¬ì¡°:
    [
      {
        "dataset_id": "2025_POST_MATCH_PITCHING_STATS",
        "name": "2025 Post Match Pitching Data",
        "headers": [...],
        "data": [
          { "Season": 2025, "Date": "2025-10-06", "Team": "NC", "Name": "êµ¬ì°½ëª¨", 
            "IP": 22, "ER": 1, "SO": 0, "Result": "ìŠ¹", ... },
          ...
        ]
      }
    ]
    
    Args:
        data_dir: ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        List[Dict]: ë‚ ì§œ+íŒ€ìœ¼ë¡œ ê·¸ë£¹í™”ëœ ê²½ê¸° ê¸°ë¡ ëª©ë¡
    """
    if data_dir is None:
        data_dir = MATCH_DATA_DIR
    
    match_data = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âš ï¸ ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        return match_data
    
    json_files = list(data_path.glob("*.json"))
    print(f"ğŸ“‚ ê²½ê¸° ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                file_content = json.load(f)
            
            # íŒŒì¼ëª…ì—ì„œ ì‹œì¦Œ íƒ€ì… ë° ê¸°ë¡ íƒ€ì… ì¶”ì¶œ
            filename = json_file.stem.upper()
            is_postseason = "POST" in filename
            season_type = "Post" if is_postseason else "Regular"
            
            # íˆ¬ìˆ˜/íƒ€ì ê¸°ë¡ íƒ€ì… ì¶”ì¶œ
            is_pitching = "PITCHING" in filename
            record_type = "pitcher" if is_pitching else "batter"
            
            # ì—°ë„ ì¶”ì¶œ
            year_match = re.search(r'(\d{4})', filename)
            year = year_match.group(1) if year_match else "2025"
            
            # ë°°ì—´ ë‚´ ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
            if isinstance(file_content, list):
                for dataset in file_content:
                    dataset_name = dataset.get("name", "")
                    dataset_id = dataset.get("dataset_id", "")
                    
                    # data ë°°ì—´ ë‚´ ê° ë ˆì½”ë“œë¥¼ ë‚ ì§œ+íŒ€ìœ¼ë¡œ ê·¸ë£¹í™”
                    records = dataset.get("data", [])
                    print(f"   - {json_file.name}: {len(records)}ê°œ ë ˆì½”ë“œ ë°œê²¬")
                    
                    # ë‚ ì§œ+íŒ€ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
                    grouped = {}
                    for record in records:
                        date = record.get("Date", "Unknown")
                        team = record.get("Team", "Unknown")
                        key = (date, team)
                        
                        if key not in grouped:
                            grouped[key] = []
                        grouped[key].append(record)
                    
                    print(f"     â†’ {len(grouped)}ê°œ ê²½ê¸°(ë‚ ì§œ+íŒ€)ë¡œ ê·¸ë£¹í™”")
                    
                    # ê·¸ë£¹í™”ëœ ë°ì´í„°ë¥¼ ë¬¸ì„œë¡œ ë³€í™˜
                    for (date, team), player_records in grouped.items():
                        grouped_doc = {
                            "_source_file": json_file.name,
                            "_data_type": "match",
                            "_loaded_at": datetime.now().isoformat(),
                            "_dataset_name": dataset_name,
                            "_dataset_id": dataset_id,
                            "_season_type": season_type,
                            "_record_type": record_type,
                            "_year": year,
                            "Date": date,
                            "date": date,
                            "Team": team,
                            "teams": [team],
                            "players": player_records  # í•´ë‹¹ ê²½ê¸°ì˜ ëª¨ë“  ì„ ìˆ˜ ê¸°ë¡
                        }
                        match_data.append(grouped_doc)
            else:
                print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ JSON êµ¬ì¡°: {json_file.name}")
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
    
    return match_data


# =============================================================================
# ë¬¸ì„œ ë³€í™˜ í•¨ìˆ˜
# =============================================================================

def create_document_from_data(data: Dict, data_type: str) -> Document:
    """
    JSON ë°ì´í„°ë¥¼ LangChain Documentë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    í•µì‹¬ ì „ëµ (ë…¼ë¬¸ Section 4.2):
    - page_content: ì„¤ëª… ë¬¸ì¥ (ì„ë² ë”© ëŒ€ìƒ)
    - metadata: ì›ë³¸ JSON + í•„í„°ë§ìš© ë©”íƒ€ë°ì´í„°
    
    Args:
        data: JSON ë°ì´í„°
        data_type: "season" ë˜ëŠ” "match"
    
    Returns:
        Document: LangChain Document ê°ì²´
    """
    # 1. ì„¤ëª… ë¬¸ì¥ ìƒì„± (ì„ë² ë”© ëŒ€ìƒ)
    descriptive_sentence = generate_descriptive_sentence(data, data_type)
    
    # Instruct ëª¨ë¸ìš© ì¿¼ë¦¬ í¬ë§·
    # ë…¼ë¬¸ Section 4.4.1ì—ì„œ ê²€ì¦ëœ ë°©ì‹
    embedding_text = (
        f"Instruct: Find the baseball dataset covering the given team(s) and statistics. "
        f"Query: {descriptive_sentence}"
    )
    
    # 2. ë©”íƒ€ë°ì´í„° êµ¬ì„±
    metadata = {
        "type": data_type,
        "source_file": data.get("_source_file", ""),
        "raw_data": json.dumps(data, ensure_ascii=False),  # ì›ë³¸ JSON ì €ì¥
    }
    
    # ì‹œì¦Œ ë°ì´í„° ë©”íƒ€ë°ì´í„°
    if data_type == "season":
        metadata["season"] = data.get("season", "2025")
        metadata["team"] = data.get("Team", data.get("team", ""))
        metadata["stat_type"] = data.get("_stat_type", "")
        metadata["season_type"] = data.get("_season_type", "Regular")
        metadata["player_name"] = data.get("Name", "")
        # teams í•„ë“œ: ë©”íƒ€ë°ì´í„° í•„í„°ë§ìš© (ChromaDBëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‰¼í‘œ êµ¬ë¶„ ë¬¸ìì—´ë¡œ ì €ì¥)
        if data.get("teams"):
            metadata["teams"] = ",".join(data.get("teams"))
        elif data.get("Team"):
            metadata["teams"] = data.get("Team")
    
    # ê²½ê¸° ë°ì´í„° ë©”íƒ€ë°ì´í„° (ë‚ ì§œ+íŒ€ ê·¸ë£¹í™”ëœ íˆ¬ìˆ˜/íƒ€ì ê¸°ë¡)
    elif data_type == "match":
        metadata["date"] = data.get("date", data.get("Date", ""))
        # teams í•„ë“œ: ChromaDBëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‰¼í‘œ êµ¬ë¶„ ë¬¸ìì—´ë¡œ ì €ì¥
        teams_list = data.get("teams", [])
        metadata["teams"] = ",".join(teams_list) if teams_list else ""
        metadata["team"] = data.get("Team", "")
        metadata["season_type"] = data.get("_season_type", "Regular")
        metadata["year"] = data.get("_year", "2025")
        metadata["record_type"] = data.get("_record_type", "pitcher")
        
        # ê·¸ë£¹í™”ëœ ì„ ìˆ˜ ëª©ë¡ (ì´ë¦„ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥)
        players = data.get("players", [])
        player_names = [p.get("Name", "") for p in players if p.get("Name")]
        metadata["player_names"] = ",".join(player_names) if player_names else ""
        metadata["player_count"] = len(players)
    
    return Document(
        page_content=embedding_text,
        metadata=metadata
    )

# === ì¶”ê°€í•œ í•¨ìˆ˜ ===

def load_schedule_mapping(csv_path: str) -> Dict[str, str]:
    """
    ì¼ì • CSV(games_2025.csv)ë¥¼ ì½ì–´ (ë‚ ì§œ_íŒ€ëª…) -> ìƒëŒ€íŒ€ëª… ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    CSVì˜ í•œê¸€ íŒ€ëª…ì„ TEAM_MAPì„ í†µí•´ ì˜ë¬¸(ê³µì‹ëª…ì¹­)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë§¤í•‘í•©ë‹ˆë‹¤.
    """
    mapping = {}
    path = Path(csv_path)
    
    if not path.exists():
        print(f"âš ï¸ ê²½ê³ : ì¼ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ({csv_path}) ìƒëŒ€ íŒ€ ì •ë³´ê°€ ëˆ„ë½ë©ë‹ˆë‹¤.")
        return mapping

    print(f"ğŸ“… ì¼ì • ë°ì´í„° ë¡œë“œ ì¤‘: {csv_path}")
    
    try:
        with open(path, 'r', encoding='utf-8-sig') as f: # utf-8-sigë¡œ BOM ì²˜ë¦¬
            reader = csv.DictReader(f)
            for row in reader:
                # CSV ì»¬ëŸ¼ëª…: date, season, home_team, away_team ...
                date = row.get('date', '').strip()
                home_raw = row.get('home_team', '').strip()
                away_raw = row.get('away_team', '').strip()
                
                if not date or not home_raw or not away_raw:
                    continue

                # í•œê¸€ íŒ€ëª…ì„ ì˜ë¬¸ ì½”ë“œë¡œ ë³€í™˜ (ì˜ˆ: ë‘ì‚° -> Doosan)
                # TEAM_MAPì— ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                home = TEAM_MAP.get(home_raw, home_raw)
                away = TEAM_MAP.get(away_raw, away_raw)
                
                # ë§¤í•‘ ìƒì„± (ì–‘ë°©í–¥)
                # Key: "2025-05-05_Doosan" -> Value: "LG"
                mapping[f"{date}_{home}"] = away
                mapping[f"{date}_{away}"] = home
                
    except Exception as e:
        print(f"âš ï¸ ì¼ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
    return mapping

# prepare_document ìˆ˜ì •

def prepare_documents(match_data: List[Dict], schedule_map: Dict[str, str]) -> List[Document]:
    """
    Match Dataë¥¼ (ë‚ ì§œ, íŒ€) ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”í•˜ê³ , ìƒëŒ€ íŒ€ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ Documentë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    documents = []
    
    # 1. ìœ íš¨í•œ ë ˆì½”ë“œ í•„í„°ë§ (í•„ìˆ˜ í‚¤ê°€ ìˆëŠ” ë°ì´í„°ë§Œ)
    valid_records = [
        r for r in match_data 
        if r.get("Date") and r.get("Team")
    ]
    
    # 2. ê° ë ˆì½”ë“œì— ìƒëŒ€ íŒ€(Opponent) ì •ë³´ ë§¤í•‘
    for record in valid_records:
        date = record['Date']     # ì˜ˆ: "2025-05-05"
        team = record['Team']     # ì˜ˆ: "Doosan"
        
        # í‚¤ ìƒì„± (csv ë¡œë“œ ë•Œì™€ ë™ì¼í•œ ê·œì¹™)
        key = f"{date}_{team}"
        
        # ë§¤í•‘ì—ì„œ ìƒëŒ€íŒ€ ì°¾ê¸° (ì—†ìœ¼ë©´ 'Unknown')
        record['Opponent'] = schedule_map.get(key, "Unknown")

    # 3. ë°ì´í„° ì •ë ¬ (groupbyë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í•„ìˆ˜ ì „ì œ ì¡°ê±´)
    # ì •ë ¬ ê¸°ì¤€: ë‚ ì§œ -> íŒ€ -> ìƒëŒ€íŒ€
    def key_func(x):
        return (x['Date'], x['Team'], x['Opponent'])
    
    valid_records.sort(key=key_func)
    
    print(f"ğŸ”„ ì´ {len(valid_records)}ê°œì˜ ë ˆì½”ë“œë¥¼ ê²½ê¸° ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤...")
    
    # 4. ê·¸ë£¹í™” ë° Document ìƒì„±
    for (date, team, opponent), group in groupby(valid_records, key=key_func):
        # ì œë„ˆë ˆì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í•´ë‹¹ ê²½ê¸°ì˜ ëª¨ë“  íˆ¬ìˆ˜ ê¸°ë¡)
        game_records = list(group)
        
        # 4-1. ìì—°ì–´ ì„¤ëª… ìƒì„± (ì„ë² ë”©ë  í…ìŠ¤íŠ¸)
        description = generate_game_description(date, team, opponent, game_records)
        
        # 4-2. ë©”íƒ€ë°ì´í„° êµ¬ì„±
        # ì›ë³¸ ë°ì´í„°(game_records)ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë©”íƒ€ë°ì´í„°ì— ì €ì¥
        # ì´ë ‡ê²Œ í•˜ë©´ RAG ê²€ìƒ‰ í›„ ì›ë³¸ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë³µì›í•´ì„œ ì“¸ ìˆ˜ ìˆìŒ
        metadata = {
            "date": date,
            "team": team,
            "opponent": opponent,
            "season": str(game_records[0].get("Season", "2025")),
            "record_count": len(game_records),
            "original_data": json.dumps(game_records, ensure_ascii=False)
        }
        
        doc = Document(page_content=description, metadata=metadata)
        documents.append(doc)
        
    return documents


# =============================================================================
# ChromaDB ì ì¬ í•¨ìˆ˜
# =============================================================================

def initialize_vector_store(
    documents: List[Document] = None,
    persist_directory: str = None,
    collection_name: str = None,
    batch_size: int = 100
) -> Chroma:
    """
    ChromaDB ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    ì„±ëŠ¥ ìµœì í™”:
    - ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    - tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    
    Args:
        documents: ì ì¬í•  Document ëª©ë¡ (Noneì´ë©´ ê¸°ì¡´ ìŠ¤í† ì–´ ë¡œë“œ)
        persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 100)
    
    Returns:
        Chroma: ì´ˆê¸°í™”ëœ ë²¡í„° ìŠ¤í† ì–´
    """
    if persist_directory is None:
        persist_directory = str(CHROMA_DB_DIR)
    
    if collection_name is None:
        collection_name = COLLECTION_NAME
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = get_embedding_model()
    
    if documents:
        # ìƒˆë¡œìš´ ë¬¸ì„œë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print(f"\nğŸ“¦ ChromaDB ì´ˆê¸°í™” ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)})")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œì”© ì²˜ë¦¬")
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        first_batch = documents[:batch_size]
        vector_store = Chroma.from_documents(
            documents=first_batch,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=persist_directory
        )
        
        # ë‚˜ë¨¸ì§€ ë°°ì¹˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€
        remaining_docs = documents[batch_size:]
        if remaining_docs:
            total_batches = (len(remaining_docs) + batch_size - 1) // batch_size
            print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ë° ì ì¬ ì¤‘... ({total_batches}ê°œ ë°°ì¹˜)")
            
            for i in tqdm(range(0, len(remaining_docs), batch_size), desc="Batches"):
                batch = remaining_docs[i:i + batch_size]
                vector_store.add_documents(batch)
        
        print(f"âœ… ChromaDB ì ì¬ ì™„ë£Œ: {persist_directory}")
        
    else:
        # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        print(f"\nğŸ“‚ ê¸°ì¡´ ChromaDB ë¡œë“œ ì¤‘: {persist_directory}")
        
        vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=embeddings,
            persist_directory=persist_directory
        )
    
    return vector_store


def clear_vector_store(persist_directory: str = None, collection_name: str = None):
    """
    ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
    """
    if persist_directory is None:
        persist_directory = str(CHROMA_DB_DIR)
    
    if collection_name is None:
        collection_name = COLLECTION_NAME
    
    import shutil
    persist_path = Path(persist_directory)
    
    if persist_path.exists():
        shutil.rmtree(persist_path)
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ: {persist_directory}")


# =============================================================================
# ë©”ì¸ ì ì¬ íŒŒì´í”„ë¼ì¸
# =============================================================================

def ingest_all_data(
    season_dir: Path = None,
    match_dir: Path = None,
    clear_existing: bool = True
) -> Chroma:
    """
    ëª¨ë“  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBì— ì ì¬í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    
    Args:
        season_dir: ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ (Noneì¼ ê²½ìš° config ê¸°ë³¸ê°’ ì‚¬ìš©)
        match_dir: ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬ (Noneì¼ ê²½ìš° config ê¸°ë³¸ê°’ ì‚¬ìš©)
        clear_existing: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì—¬ë¶€ (ê¸°ë³¸ê°’ True)
    
    Returns:
        Chroma: ì´ˆê¸°í™”ëœ ë²¡í„° ìŠ¤í† ì–´
    """
    print("=" * 60)
    print("ğŸš€ KBO ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì„ íƒì )
    if clear_existing:
        clear_vector_store()
    
    # 2. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # (ì°¸ê³ ) ì´ë²ˆ ë¡œì§ì€ 'ê²½ê¸° ë‹¨ìœ„' ê¸°ë¡ì— ì§‘ì¤‘í•˜ë¯€ë¡œ ì‹œì¦Œ ë°ì´í„° ë¡œë”©ì€ ì ì‹œ ìƒëµí•˜ê±°ë‚˜
    # í•„ìš”í•˜ë‹¤ë©´ ë³„ë„ ë¡œì§ìœ¼ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # season_data = load_season_data(season_dir) 
    
    match_data = load_match_data(match_dir)
    
    # [NEW] ì¼ì • ë°ì´í„° ë¡œë“œ (ìƒëŒ€ íŒ€ ë§¤í•‘ìš©)
    # íŒŒì¼ ìœ„ì¹˜ê°€ ë°”ë€Œë©´ ì´ ê²½ë¡œë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.
    SCHEDULE_CSV_PATH = "games_2025.csv" 
    schedule_map = load_schedule_mapping(SCHEDULE_CSV_PATH)
    
    if not match_data:
        print("âš ï¸ ë¡œë“œëœ ê²½ê¸° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None
    
    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ìš”ì•½:")
    print(f"   - ê²½ê¸° ë°ì´í„°(Match): {len(match_data)}ê±´ (ë ˆì½”ë“œ ê¸°ì¤€)")
    print(f"   - ì¼ì • ë°ì´í„°(Schedule): {len(schedule_map)//2}ê²½ê¸° ì •ë³´ í™•ë³´") # ì–‘ë°©í–¥ ë§¤í•‘ì´ë¯€ë¡œ ë‚˜ëˆ„ê¸° 2
    
    # 3. Document ë³€í™˜
    # ê¸°ì¡´ ì½”ë“œì™€ ë‹¬ë¦¬, prepare_documentsì— 'match_data'ì™€ 'schedule_map'ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
    documents = prepare_documents(match_data, schedule_map)
    
    if not documents:
        print("âš ï¸ ìƒì„±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ë¬´ê²°ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

    # 4. ChromaDB ì ì¬
    vector_store = initialize_vector_store(documents)
    
    # 5. ê²°ê³¼ í™•ì¸
    doc_count = vector_store._collection.count()
    print(f"\nâœ… ì ì¬ ì™„ë£Œ! ì´ {doc_count}ê°œì˜ 'ê²½ê¸° ë‹¨ìœ„' ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 60)
    
    return vector_store


# =============================================================================
# CLI ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    """
    ëª…ë ¹ì¤„ì—ì„œ ì§ì ‘ ì‹¤í–‰:
    python -m src.ingest
    """
    vector_store = ingest_all_data()
    
    if vector_store:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰...")
        results = vector_store.similarity_search("í•œí™” ì´ê¸€ìŠ¤ ì‹œì¦Œ ì„±ì ", k=3)
        
        for i, doc in enumerate(results, 1):
            print(f"\nê²°ê³¼ {i}:")
            print(f"  ë‚´ìš©: {doc.page_content[:100]}...")
            print(f"  íƒ€ì…: {doc.metadata.get('type')}")
