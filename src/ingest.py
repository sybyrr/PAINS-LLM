"""
Ingest module - ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸

JSON ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBì— ì ì¬í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
ë…¼ë¬¸ Section 4.2 (Embedding the data) ì „ëµì„ êµ¬í˜„í•©ë‹ˆë‹¤.

í•µì‹¬ ì „ëµ:
1. JSON ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì„ë² ë”©í•˜ì§€ ì•ŠìŒ
2. ê° íŒŒì¼ì— ëŒ€í•´ "ì„¤ëª… ë¬¸ì¥(Descriptive Sentence)"ì„ ìƒì„±í•˜ì—¬ page_contentë¡œ ì €ì¥
3. ì›ë³¸ JSON ë°ì´í„°ëŠ” metadata í•„ë“œì— ì €ì¥
"""

import json
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import re

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document
from tqdm import tqdm

from .config import (
    CHROMA_DB_DIR, 
    COLLECTION_NAME, 
    EMBEDDING_MODEL,
    SEASON_DATA_DIR,
    MATCH_DATA_DIR
)
from .utils import generate_descriptive_sentence, TEAM_EN_TO_KO


# =============================================================================
# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
# =============================================================================

def get_embedding_model() -> HuggingFaceEmbeddings:
    """
    ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    
    ë…¼ë¬¸ì—ì„œ ê²€ì¦ëœ multilingual-e5-large-instruct ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    L2 ì •ê·œí™”ë¥¼ ì ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì— ìµœì í™”í•©ë‹ˆë‹¤.
    
    Returns:
        HuggingFaceEmbeddings: ì´ˆê¸°í™”ëœ ì„ë² ë”© ëª¨ë¸
    """
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},  # GPU ì—†ëŠ” í™˜ê²½ ì§€ì›
        encode_kwargs={
            "normalize_embeddings": True  # L2 ì •ê·œí™” - ë…¼ë¬¸ ê¶Œì¥
        }
    )


# =============================================================================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# =============================================================================

def load_season_data(data_dir: Path = None) -> List[Dict]:
    """
    ì‹œì¦Œ ëˆ„ì  ë°ì´í„°(Global Dataset)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    íŒŒì¼ëª… ì˜ˆì‹œ: KBO_2025_Season_Total.json, KBO_2025_Hanwha.json
    
    Args:
        data_dir: ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        List[Dict]: ë¡œë“œëœ ì‹œì¦Œ ë°ì´í„° ëª©ë¡
    """
    if data_dir is None:
        data_dir = SEASON_DATA_DIR
    
    season_data = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âš ï¸ ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        return season_data
    
    json_files = list(data_path.glob("*.json"))
    print(f"ğŸ“‚ ì‹œì¦Œ ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            data["_source_file"] = json_file.name
            data["_data_type"] = "season"
            data["_loaded_at"] = datetime.now().isoformat()
            
            # íŒŒì¼ëª…ì—ì„œ íŒ€/ì‹œì¦Œ ì •ë³´ ì¶”ì¶œ ì‹œë„
            filename = json_file.stem  # í™•ì¥ì ì œì™¸
            
            # ì‹œì¦Œ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: KBO_2025_Hanwha)
            season_match = re.search(r'(\d{4})', filename)
            if season_match:
                data["season"] = season_match.group(1)
            
            season_data.append(data)
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
    
    return season_data


def load_match_data(data_dir: Path = None) -> List[Dict]:
    """
    ê°œë³„ ê²½ê¸° ë°ì´í„°(Match Dataset)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    íŒŒì¼ëª… ì˜ˆì‹œ: 20250501_Hanwha_vs_LG.json
    
    Args:
        data_dir: ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        List[Dict]: ë¡œë“œëœ ê²½ê¸° ë°ì´í„° ëª©ë¡
    """
    if data_dir is None:
        data_dir = MATCH_DATA_DIR
    
    match_data = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âš ï¸ ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        return match_data
    
    json_files = list(data_path.glob("*.json"))
    print(f"ğŸ“‚ ê²½ê¸° ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            data["_source_file"] = json_file.name
            data["_data_type"] = "match"
            data["_loaded_at"] = datetime.now().isoformat()
            
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ/íŒ€ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: 20250501_Hanwha_vs_LG)
            filename = json_file.stem
            
            # ë‚ ì§œ ì¶”ì¶œ
            date_match = re.search(r'(\d{8})', filename)
            if date_match:
                date_str = date_match.group(1)
                formatted_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
                data["date"] = formatted_date
            
            # íŒ€ ì¶”ì¶œ (vs ë˜ëŠ” _ êµ¬ë¶„)
            team_pattern = r'([A-Za-z]+)(?:_vs_|vs|_)([A-Za-z]+)'
            team_match = re.search(team_pattern, filename, re.IGNORECASE)
            if team_match:
                data["teams"] = [team_match.group(1), team_match.group(2)]
            
            match_data.append(data)
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
    
    return match_data


# =============================================================================
# ë¬¸ì„œ ë³€í™˜ í•¨ìˆ˜
# =============================================================================

def create_document_from_data(data: Dict, data_type: str) -> Document:
    """
    JSON ë°ì´í„°ë¥¼ LangChain Documentë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    í•µì‹¬ ì „ëµ (ë…¼ë¬¸ Section 4.2):
    - page_content: ì„¤ëª… ë¬¸ì¥ (ì„ë² ë”© ëŒ€ìƒ)
    - metadata: ì›ë³¸ JSON + í•„í„°ë§ìš© ë©”íƒ€ë°ì´í„°
    
    Args:
        data: JSON ë°ì´í„°
        data_type: "season" ë˜ëŠ” "match"
    
    Returns:
        Document: LangChain Document ê°ì²´
    """
    # 1. ì„¤ëª… ë¬¸ì¥ ìƒì„± (ì„ë² ë”© ëŒ€ìƒ)
    descriptive_sentence = generate_descriptive_sentence(data, data_type)
    
    # Instruct ëª¨ë¸ìš© ì¿¼ë¦¬ í¬ë§·
    # ë…¼ë¬¸ Section 4.4.1ì—ì„œ ê²€ì¦ëœ ë°©ì‹
    embedding_text = (
        f"Instruct: Find the baseball dataset covering the given team(s) and statistics. "
        f"Query: {descriptive_sentence}"
    )
    
    # 2. ë©”íƒ€ë°ì´í„° êµ¬ì„±
    metadata = {
        "type": data_type,
        "source_file": data.get("_source_file", ""),
        "raw_data": json.dumps(data, ensure_ascii=False),  # ì›ë³¸ JSON ì €ì¥
    }
    
    # ì‹œì¦Œ ë°ì´í„° ë©”íƒ€ë°ì´í„°
    if data_type == "season":
        metadata["season"] = data.get("season", "2025")
        metadata["team"] = data.get("team", "")
        metadata["stat_type"] = data.get("stat_type", "")
        # teams í•„ë“œ: ë©”íƒ€ë°ì´í„° í•„í„°ë§ìš©
        if data.get("team"):
            metadata["teams"] = [data.get("team")]
    
    # ê²½ê¸° ë°ì´í„° ë©”íƒ€ë°ì´í„°
    elif data_type == "match":
        metadata["date"] = data.get("date", "")
        metadata["teams"] = data.get("teams", [])
        if metadata["teams"]:
            metadata["home_team"] = metadata["teams"][0] if len(metadata["teams"]) > 0 else ""
            metadata["away_team"] = metadata["teams"][1] if len(metadata["teams"]) > 1 else ""
    
    return Document(
        page_content=embedding_text,
        metadata=metadata
    )


def prepare_documents(season_data: List[Dict], match_data: List[Dict]) -> List[Document]:
    """
    ëª¨ë“  ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        season_data: ì‹œì¦Œ ë°ì´í„° ëª©ë¡
        match_data: ê²½ê¸° ë°ì´í„° ëª©ë¡
    
    Returns:
        List[Document]: ë³€í™˜ëœ Document ëª©ë¡
    """
    documents = []
    
    print("\nğŸ”„ ì‹œì¦Œ ë°ì´í„° ë³€í™˜ ì¤‘...")
    for data in tqdm(season_data, desc="Season"):
        doc = create_document_from_data(data, "season")
        documents.append(doc)
    
    print("\nğŸ”„ ê²½ê¸° ë°ì´í„° ë³€í™˜ ì¤‘...")
    for data in tqdm(match_data, desc="Match"):
        doc = create_document_from_data(data, "match")
        documents.append(doc)
    
    return documents


# =============================================================================
# ChromaDB ì ì¬ í•¨ìˆ˜
# =============================================================================

def initialize_vector_store(
    documents: List[Document] = None,
    persist_directory: str = None,
    collection_name: str = None
) -> Chroma:
    """
    ChromaDB ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        documents: ì ì¬í•  Document ëª©ë¡ (Noneì´ë©´ ê¸°ì¡´ ìŠ¤í† ì–´ ë¡œë“œ)
        persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
    
    Returns:
        Chroma: ì´ˆê¸°í™”ëœ ë²¡í„° ìŠ¤í† ì–´
    """
    if persist_directory is None:
        persist_directory = str(CHROMA_DB_DIR)
    
    if collection_name is None:
        collection_name = COLLECTION_NAME
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = get_embedding_model()
    
    if documents:
        # ìƒˆë¡œìš´ ë¬¸ì„œë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print(f"\nğŸ“¦ ChromaDB ì´ˆê¸°í™” ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)})")
        
        vector_store = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=persist_directory
        )
        
        print(f"âœ… ChromaDB ì ì¬ ì™„ë£Œ: {persist_directory}")
        
    else:
        # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        print(f"\nğŸ“‚ ê¸°ì¡´ ChromaDB ë¡œë“œ ì¤‘: {persist_directory}")
        
        vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=embeddings,
            persist_directory=persist_directory
        )
    
    return vector_store


def clear_vector_store(persist_directory: str = None, collection_name: str = None):
    """
    ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
    """
    if persist_directory is None:
        persist_directory = str(CHROMA_DB_DIR)
    
    if collection_name is None:
        collection_name = COLLECTION_NAME
    
    import shutil
    persist_path = Path(persist_directory)
    
    if persist_path.exists():
        shutil.rmtree(persist_path)
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ: {persist_directory}")


# =============================================================================
# ë©”ì¸ ì ì¬ íŒŒì´í”„ë¼ì¸
# =============================================================================

def ingest_all_data(
    season_dir: Path = None,
    match_dir: Path = None,
    clear_existing: bool = True
) -> Chroma:
    """
    ëª¨ë“  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBì— ì ì¬í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    
    Args:
        season_dir: ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬
        match_dir: ê²½ê¸° ë°ì´í„° ë””ë ‰í† ë¦¬
        clear_existing: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì—¬ë¶€
    
    Returns:
        Chroma: ì´ˆê¸°í™”ëœ ë²¡í„° ìŠ¤í† ì–´
    
    Example:
        >>> from src.ingest import ingest_all_data
        >>> vector_store = ingest_all_data()
        >>> print(f"ì ì¬ëœ ë¬¸ì„œ ìˆ˜: {vector_store._collection.count()}")
    """
    print("=" * 60)
    print("ğŸš€ KBO ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì„ íƒì )
    if clear_existing:
        clear_vector_store()
    
    # 2. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
    season_data = load_season_data(season_dir)
    match_data = load_match_data(match_dir)
    
    if not season_data and not match_data:
        print("âš ï¸ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None
    
    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ìš”ì•½:")
    print(f"   - ì‹œì¦Œ ë°ì´í„°: {len(season_data)}ê±´")
    print(f"   - ê²½ê¸° ë°ì´í„°: {len(match_data)}ê±´")
    
    # 3. Document ë³€í™˜
    documents = prepare_documents(season_data, match_data)
    
    # 4. ChromaDB ì ì¬
    vector_store = initialize_vector_store(documents)
    
    # 5. ê²°ê³¼ í™•ì¸
    doc_count = vector_store._collection.count()
    print(f"\nâœ… ì ì¬ ì™„ë£Œ! ì´ {doc_count}ê°œì˜ ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 60)
    
    return vector_store


# =============================================================================
# CLI ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    """
    ëª…ë ¹ì¤„ì—ì„œ ì§ì ‘ ì‹¤í–‰:
    python -m src.ingest
    """
    vector_store = ingest_all_data()
    
    if vector_store:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰...")
        results = vector_store.similarity_search("í•œí™” ì´ê¸€ìŠ¤ ì‹œì¦Œ ì„±ì ", k=3)
        
        for i, doc in enumerate(results, 1):
            print(f"\nê²°ê³¼ {i}:")
            print(f"  ë‚´ìš©: {doc.page_content[:100]}...")
            print(f"  íƒ€ì…: {doc.metadata.get('type')}")
