=== 페이지 1 ===

Degree Project in Technology  Second cycle, 30 credits  A Chatbot for Football Analytics: A deep dive into RAG, LLM Orchestration and Function Calling  NILS BLOMGREN

=== 페이지 2 ===



=== 페이지 3 ===

A Chatbot for Football Analytics: A deep dive into RAG, LLM Orchestration and Function Calling  NILS BLOMGREN  Master’s Programme, Software Engineering of Distributed Systems, 120 credits Date: June 25, 2025 Supervisor: Fabian Schmidt Examiner: Vladimir Vlassov School of Electrical Engineering and Computer Science Host company: PlaymakerAI Swedish title: Chatbot för fotbollsanalys: En djupdykning inom RAG, LLM-orkestrering och function calling

=== 페이지 4 ===

© 2025   Nils Blomgren

=== 페이지 5 ===

Abstract | i  Abstract  This thesis presents a small, modular chatbot that lets novice users talk directly to structured football data, lowering the entry barrier for analytics work. Its goal is to replace a 45-minute, multi-step workflow with a one-minute conversation and thereby make advanced insights available to journalists, coaches and football-focused professionals and enthusiasts.   The assistant supports three key tasks:   (i) create an opponent-analysis dashboard for a team, (ii) create a match-analysis for a game, and (iii) explain complex Key Performance Indicators in plain language. The pipeline follows a multilayer Retrieval-Augmented Generation design. A multilingual-E5 embedder stores data on football matches in a Chroma vector database.   The user query gets cleaned, classified and corrected if needed and a hybrid retriever ranks candidates based on the cleaned user query. Lastly an OpenAI assistant validates the fetched data (LLM-as-a-judge) and either answers directly with the analysis directly or calls backend functions that return JSON dashboards, this keeps token usage and latency low. Benchmarking shows the retriever ranks the correct dataset in 92 % of synthetic queries (MRR = 0.92), and an eight-person user study cut dashboard- creation time from 45 min to 17 s on average, with a BUS-15 usability score of 76/100. Building a domain-specific RAG stack that works on nearly identical foot- ball tables and runs on consumer hardware combines hard research questions (retrieval disambiguation, function-calling orchestration, latency–cost trade- offs) with industrial constraints (proprietary data, zero GPU budget). Key limitations are CPU-only hosting, absence of fine-tuning data and a bias toward men’s competitions, which constrain latency and domain coverage. Even so, the work demonstrates that a lightweight RAG + LLM stack can democratise football analytics on commodity hardware.   Future steps are adding GPU hosting, richer metadata filters and a fine-tuned analysis model, will most likely lift accuracy, reduce variance and turn the prototype into a production-ready assistant for both novice and expert users.  Keywords  Large   Language   Models,   Semantic   Search,   Chatbot,   Football,   Vector Databases, Embedding Models, RAG, LLM-as-a-Judge

=== 페이지 6 ===

ii | Sammanfattning  Sammanfattning  Denna masteruppsats presenterar en liten, modulär chattbot som låter nybörjar användare prata direkt med strukturerad fotbollsdata, vilket sänker tröskeln för   analysarbete.   Målet   är   att   ersätta   ett   45   minuter   långt   arbetsflöde innehållande flera manuella steg med en konversation på cirka en minut. Vilket resulterar i att avancerade insikter blir tillgängliga för journalister, tränare   samt   andra   fotbollsyrkesverksamma   och   entusiaster.   Assistenten stödjer tre huvuduppgifter: (i) skapa en motståndaranalys-dashboard (ii) skapa en matchanalys-dashboard och (iii) förklara komplexa Key Performance Indicators på svenska eller engelska. Pipelinen följer en flerlagersdesign för Retrieval-Augmented Generation. En flerspråkig E5-embedder lagrar data från fotbollsmatcher i en Chroma- vektordatabas. Andvändarens fråga tvättas, klassificeras och rättas, vid behov. En hybridsökning rankar kandidater baserad på användarens fråga. Slutligen validerar en OpenAI assistent den hämtade datan och besvarar antingen frågan direkt eller anropar backend-funktioner som returnerar JSON-dashboards. Detta håller både tokenanvändning och fördröjningen nere. Benchmarktester visar att RAG-komponenten hittar rätt data med ett MRR poäng på 0,92. En användarstudie med åtta deltagare genomfördes och visade att tiden för att bygga dashboards minskade från 45 minuter till i genomsnitt 17 sekunder, samt gav ett BUS-15-användbarhetsbetyg på 76 av 100. Att bygga en domänspecifik RAG-stack som fungerar på nästan identiska data och körs lokalt kombinerar svåra forskningsfrågor, som orkestrering och avvägningar mellan fördröjning och kostnad, med industriella begränsningar som proprietär data. Viktiga begränsningar är hårdvaran, bristen på data för finjustering av modeller samt en övervikt av data från herrfotboll. Arbetet visar ändå att en lättviktig RAG och LLM-arkitektur kan demokratisera fotbollsanalys på standardhårdvara. Framtida steg inkluderar GPU-hosting, rikare metadatafilter och en finjusterad analysmodell, vilket förväntas öka noggrannheten, minska variansen och förvandla prototypen till en produktionsklar assistent för både oerfarna och erfarna användare.  Nyckelord  Large Language Models, Semantisk sökning, Chattbot, Fotboll, Vektordata- baser, Inbäddningsmodeller, RAG, LLM-as-a-Judge

=== 페이지 7 ===

Acknowledgments | iii  Acknowledgments  I would like to thank my examiner, Professor Vladimir Vlassov, for his guidance and feedback and my supervisor, Fabian Schmidt, for his insights and continuous support. I am also very grateful to Alexander Jorge and Ola Eriksson at PlaymakerAI for hosting me, providing access to their platform and chatbot and sharing their domain expertise in football analytics and making this thesis possible. Stockholm, June 2025 Nils Blomgren

=== 페이지 8 ===

iv | Acknowledgments

=== 페이지 9 ===

Contents | v  Contents  1   Introduction   1  1.1   Background . . . . . . . . . . . . . . . . . . . . . . . . . . .   2 1.2   Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   2 1.2.1   Original problem and definition   . . . . . . . . . . . .   3 1.2.2   Scientific and engineering issues . . . . . . . . . . . .   3 1.3   Purpose   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   4 1.4   Goals   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   4 1.5   Contributions   . . . . . . . . . . . . . . . . . . . . . . . . . .   5 1.6   Delimitations   . . . . . . . . . . . . . . . . . . . . . . . . . .   6 1.7   Structure of the thesis . . . . . . . . . . . . . . . . . . . . . .   6 1.8   Ethics and Sustainability   . . . . . . . . . . . . . . . . . . . .   6  2   Background   8  2.1   Core-Techniques   . . . . . . . . . . . . . . . . . . . . . . . .   8 2.1.1   Function Calling   . . . . . . . . . . . . . . . . . . . .   8 2.1.2   Retrieval-Augmented Generation (RAG) . . . . . . . .   9 2.1.3   Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . .   11 2.1.4   Embeddings, Semantic Search and Vector Databases   .   12 2.2   Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   14 2.2.1   LangChain   . . . . . . . . . . . . . . . . . . . . . . .   14 2.2.2   Vector Databases . . . . . . . . . . . . . . . . . . . .   14 2.3   Evaluation and Use Cases . . . . . . . . . . . . . . . . . . . .   15 2.3.1   LLM-as-a-judge   . . . . . . . . . . . . . . . . . . . .   15 2.3.2   Applications in Football Analytics . . . . . . . . . . .   16 2.4   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   16  3   Methods   18  3.1   Research Process   . . . . . . . . . . . . . . . . . . . . . . . .   18 3.2   Research Paradigm   . . . . . . . . . . . . . . . . . . . . . . .   19

=== 페이지 10 ===

vi | Contents  3.3   Data Inspection, Selection and Preprocessing   . . . . . . . . .   20 3.3.1   Data Provided . . . . . . . . . . . . . . . . . . . . . .   21 3.3.2   Synthetically Generated Queries . . . . . . . . . . . .   23 3.4   Goal of the system   . . . . . . . . . . . . . . . . . . . . . . .   24 3.4.1   Opponent Analysis   . . . . . . . . . . . . . . . . . . .   24 3.4.2   Match Analysis . . . . . . . . . . . . . . . . . . . . .   25 3.4.3   General Question . . . . . . . . . . . . . . . . . . . .   25 3.5   System Architecture . . . . . . . . . . . . . . . . . . . . . . .   25 3.6   Assessing reliability and validity of the data collected . . . . .   27 3.7   System documentation   . . . . . . . . . . . . . . . . . . . . .   28  4   Implementation   29  4.1   Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .   29 4.1.1   Hardware & Software details . . . . . . . . . . . . . .   29 4.2   Embedding the data . . . . . . . . . . . . . . . . . . . . . . .   30 4.3   User Query Processing   . . . . . . . . . . . . . . . . . . . . .   30 4.3.1   Query Classification   . . . . . . . . . . . . . . . . . .   31 4.3.2   Query Cleaning and Normalization   . . . . . . . . . .   32 4.4   RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   34 4.4.1   Vector Database and Embedding Strategy . . . . . . .   34 4.4.2   Hybrid Retrieval Strategy   . . . . . . . . . . . . . . .   35 4.4.3   Post-Retrieval Data Preparation   . . . . . . . . . . . .   36 4.5   LLM Orchestration and Analysis Generation . . . . . . . . . .   37 4.5.1   OpenAI Assistant Integration   . . . . . . . . . . . . .   38 4.5.2   Data Validation within Assistant (LLM-as-a-judge) . .   38 4.5.3   Prompt for Opponent/Match Analysis   . . . . . . . . .   39 4.5.4   Function Calling for Dashboard Generation   . . . . . .   40 4.5.5   Handling General Questions   . . . . . . . . . . . . . .   41 4.6   Implementation Summary   . . . . . . . . . . . . . . . . . . .   42  5   Results and Analysis   43  5.1   Evaluation Setup and Metrics . . . . . . . . . . . . . . . . . .   43 5.1.1   Component vs. System Testing . . . . . . . . . . . . .   43 5.1.2   Evaluation Metrics Overview   . . . . . . . . . . . . .   44 5.1.3   F1, MRR, NDCG, Recall@K   . . . . . . . . . . . . .   44 5.2   Component-Level Evaluation . . . . . . . . . . . . . . . . . .   45 5.2.1   RAG Benchmarks   . . . . . . . . . . . . . . . . . . .   46 5.2.2   RapidFuzz Benchmarks   . . . . . . . . . . . . . . . .   49 5.3   Pipeline and Integration Evaluation . . . . . . . . . . . . . . .   51

=== 페이지 11 ===

Contents | vii  5.3.1   Initial Integration Tests . . . . . . . . . . . . . . . . .   51 5.3.2   Full Integration Test   . . . . . . . . . . . . . . . . . .   52 5.3.2.1   System Metrics   . . . . . . . . . . . . . . .   53 5.3.2.2   Error Types and System Behavior . . . . . .   53 5.3.3   Summary of Results   . . . . . . . . . . . . . . . . . .   55 5.4   User Evaluation and Acceptance   . . . . . . . . . . . . . . . .   55 5.4.1   User Acceptance Test Design . . . . . . . . . . . . . .   55 5.4.2   BUS-13 Survey Results . . . . . . . . . . . . . . . . .   58 5.4.3   Qualitative Feedback and Improvements . . . . . . . .   60 5.4.4   Insights from UAT Answers   . . . . . . . . . . . . . .   60 5.5   System Efficiency and Latency . . . . . . . . . . . . . . . . .   61 5.6   Comparison with Non-Domain LLM . . . . . . . . . . . . . .   61 5.7   Reliability Analysis . . . . . . . . . . . . . . . . . . . . . . .   62 5.8   Summary of Results . . . . . . . . . . . . . . . . . . . . . . .   63  6   Discussion   64 7   Conclusions and Future work   70  7.1   Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . .   70 7.2   Limitations   . . . . . . . . . . . . . . . . . . . . . . . . . . .   70 7.3   Future work . . . . . . . . . . . . . . . . . . . . . . . . . . .   71  References   73 A   Data Coverage   81  A.1   Competitions   . . . . . . . . . . . . . . . . . . . . . . . . . .   81 A.1.1   Leagues and Cups   . . . . . . . . . . . . . . . . . . .   81 A.1.2   Individual Games . . . . . . . . . . . . . . . . . . . .   83  B   JSON Dashboards   84  B.1   Opponent Analysis   . . . . . . . . . . . . . . . . . . . . . . .   84 B.2   Match Analysis   . . . . . . . . . . . . . . . . . . . . . . . . .   88  C   Benchmarking Queries and Their Target Competitions   92 D   System Instructions   96 E   Data Provided   97 F   UAT Form   101

=== 페이지 12 ===

viii | Contents  G   Full comparison of responses   105  G.1   "What is xG?" . . . . . . . . . . . . . . . . . . . . . . . . . . 105 G.1.1   PlaymakerAI   . . . . . . . . . . . . . . . . . . . . . . 105 G.1.2   GPT-4-Turbo   . . . . . . . . . . . . . . . . . . . . . . 106 G.2   "Create an opponent analysis of Real Madrid in the champions league"   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 G.2.1   PlaymakerAI   . . . . . . . . . . . . . . . . . . . . . . 106 G.2.2   GPT-4-Turbo   . . . . . . . . . . . . . . . . . . . . . . 107 G.3   "can you give me a match analysis for the latest game between Rennes-PSG." . . . . . . . . . . . . . . . . . . . . . . . . . . 108 G.3.1   PlaymakerAI   . . . . . . . . . . . . . . . . . . . . . . 108 G.3.2   GPT-4-Turbo   . . . . . . . . . . . . . . . . . . . . . . 109

=== 페이지 13 ===

List of Figures | ix  List of Figures  3.1   Chatbot Architecture   . . . . . . . . . . . . . . . . . . . . . .   26 5.1   Max Team and League F1 Scores Across Different String Matching Scorers   . . . . . . . . . . . . . . . . . . . . . . . .   50 5.2   Team F1 Scores Across QRatio Threshold Settings   . . . . . .   51

=== 페이지 14 ===

x | List of Tables  List of Tables  3.1   Top-level JSON keys: match vs. league dataset . . . . . . . . .   23 5.1   Embedding Model Benchmark Results for Dataset Retrieval (K=5)   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   47 5.2   Retrieval metrics for raw (uncleaned) queries   . . . . . . . . .   48 5.3   Retrieval metrics for pre-processed queries . . . . . . . . . . .   49 5.4   Integration tests before using the correcting dictionary   . . . .   52 5.5   Overall System Performance Metrics (N=44 Queries) . . . . .   53 5.6   Latency overview across UAT conversations.   . . . . . . . . .   58 5.7   Item-level BUS-13 ratings for the football chatbot ( n   = 8 )   . .   59 5.8   BUS-13 usability by self-rated football knowledge . . . . . . .   59

=== 페이지 15 ===

Listings | xi  Listings  4.1   Prompt for Query Classification LLM   . . . . . . . . . . . . .   31 B.1   Opponent Analysis Dashboard   . . . . . . . . . . . . . . . . .   84 B.2   Match Analysis Dashboard   . . . . . . . . . . . . . . . . . . .   88 D.1   System Instruction for OpenAI Assistant . . . . . . . . . . . .   96 E.1   Anonymised match-level data . . . . . . . . . . . . . . . . . .   97 E.2   Anonymised league-level data   . . . . . . . . . . . . . . . . .   98

=== 페이지 16 ===

xii | List of acronyms and abbreviations  List of acronyms and abbreviations  AI   Artificial Intelligence ANN   Approximate Nearest Neighbor CoT   Chain-of-Thought HNSW   Hierarchical Navigable Small World KPI   Key Performance Indicators LLM   Large Language Model LoRA   Low-Rank Adaptation ML   Machine Learning MRR   Mean Reciprocal Rank NDCG   Normalized Discounted Cumulative Gain NLP   Natural Language Processing PEFT   Parameter-Efficient Fine-Tuning RAG   Retrieval-Augmented Generation RR   Reciprocal Rank UAT   User Acceptance Test

=== 페이지 17 ===

Introduction | 1  Chapter 1 Introduction  Building a context-aware chatbot has become increasingly valuable for companies and organizations. With the rapid innovation in Machine Learning (ML) and Artificial Intelligence (AI), businesses now have the tools and computational power to automate processes and enhance their products by tailoring these technologies to their specific needs.   For example this could be including a chatbot with knowledge of company data or Frequently Asked Questions for customers, allowing for easier use of products, platforms or websites. The chatbot itself can also be the main product. This field is both relevant and essential, involving the understanding of large-scale intelligent systems, improving their efficiency and configuring them to meet domain- specific requirements. This thesis focuses on making football analytics data more accessible to a broader audience by simplifying data interaction for novice users. This thesis is conducted in collaboration with PlaymakerAI, a Swedish company   specializing   in   providing   data-driven   insights   about   football, including matches, players and tournaments.   These insights are delivered through their platform to customers such as professional football analysts, journalists and betting companies. The broad spectrum of users, each with varying levels of expertise in football, statistics and data interpretation, creates a significant knowledge gap.   The goal of this project is to bridge that gap, enabling novice users to generate datasets, analyze them and extract insights that would otherwise require assistance from a data scientist. The objective is to develop a chatbot that is context-aware, allowing users to create datasets, analyze data and extract insights effortlessly.   The chatbot must accurately interpret user queries, retrieve or generate relevant datasets and present the results through visualizations such as dashboards.

=== 페이지 18 ===

2 | Introduction  By integrating this chatbot into PlaymakerAI’s platform, the system will serve as a valuable tool for users unfamiliar with data analysis, making football analytics more accessible and actionable. While PlaymakerAI already supports experienced data scientists working with major football clubs, this project focuses on helping less experienced users by providing an intuitive, AI-driven solution for data interaction as a stepping stone on the platform.  1.1   Background  The increasing adoption of AI-driven systems has created a growing need for domain-specific, fine-tuned chatbots that can assist users in structured tasks. General-purpose Large Language Model (LLM)s often struggle with specialized applications where domain knowledge and precise interpretations are essential. Football analytics is one such field, where professionals rely on structured data to generate insights, yet many users lack the technical expertise required to work efficiently with such data. From a software engineering perspective, this challenge involves designing distributed,   scalable   AI   solutions   that   integrate   LLMs   with   structured databases. The ability to retrieve relevant data efficiently and execute domain- specific functions is important for creating an effective system.   This thesis aims to explore and implement a full pipeline for such a chatbot. Making sure it meets both industry needs and academic engineering challenges. PlaymakerAI ∗   provides a rich set of structured football data, but its user base spans from experienced data scientists to journalists and coaches with limited technical backgrounds. At the same time, data is becoming a central part of modern football, with increasing interest in metrics that explain team performance and player impact.   While platforms like PlaymakerAI offer advanced analytical tools, they are often complex and built with expert users in mind. This creates a need for a more accessible interface that allows non- experts to interact with the data through natural language, extract insights and generate dashboards without prior experience in analytics or data engineering.  1.2   Problem  Users in football analytics often lack the necessary knowledge to effectively interpret and work with data.   This challenge is further problematic when new users interact with a complex platform, making it difficult to extract  ∗ https://www.playmaker.ai/

=== 페이지 19 ===

Introduction | 3  meaningful insights without prior expertise. While PlaymakerAI provides a powerful data platform, its accessibility remains a barrier for non-expert users. This thesis seeks to develop a chatbot equipped with domain-specific knowledge that can assist new users in tasks such as analyzing football matches, creating dashboards and explaining Key Performance Indicators (KPI)s.   The chatbot must be context-aware and capable of retrieving structured   football   data   based   on   user   queries.   The   core   challenges revolve around designing an effective pipeline for the Retrieval-Augmented Generation (RAG) component, ensuring the chatbot can efficiently retrieve relevant datasets while minimizing token usage and balancing accuracy, usability and computational efficiency in a real-world deployment.   By addressing this problem,   the project contributes both to PlaymakerAI’s business objectives and to the broader field of domain-adapted AI and intelligent assistant systems.  1.2.1   Original problem and definition  Given the problem above the following research questions has been defined. 1. How to design and implement a domain specific chatbot? 2. How accurately can a two-layer LLM architecture retrieve the correct football dataset given a user query? 3. Does integrating function-calling improve novice users’ task-completion time and perceived usability when generating dashboards? 4. How does the chatbot perform against success criteria of accuracy and user satisfaction?  1.2.2   Scientific and engineering issues  The engineering challenges in this project revolve around designing a robust pipeline for building a football-specific chatbot and understanding the requirements from PlaymakerAI and their users.   The key aspects involve optimizing the system to reduce computational costs while maximizing accuracy (right data is found to analyses) and usability (correct analysis are performed).   This includes investigating the best methods for embedding data, for efficient data retrieval and integrating backend functions to execute domain-specific operations seamlessly. Understanding how to balance these factors is essential for creating a scalable and effective chatbot solution.

=== 페이지 20 ===

4 | Introduction  1.3   Purpose  The purpose of this thesis is two-folded:   to provide PlaymakerAI with a chatbot that enhances the accessibility of its platform for non-expert users and to contribute to the academic field of domain-specific AI by exploring the challenges and methodologies involved in implementing LLMs utilizing RAG for structured data analysis. For PlaymakerAI, this chatbot will serve as an assistant that lowers the knowledge barrier for new users, allowing them to interact with football analytics data more intuitively.   While the chatbot will not be directly integrated into PlaymakerAI’s source code, it will be designed to work seamlessly with their API, ensuring compatibility with their backend systems. This means that if PlaymakerAI decides to integrate the chatbot fully in the future, migration will be straightforward. The reason for not integrating with PlaymakerAI’s source code is because this is intended to be a proof of concept, PlaymakerAI would also need to give access to proprietary code which is not necessary with this set up. From an academic perspective, the thesis aims to explore the technical aspects required to complete domain-specific tasks, developing a scalable and efficient pipeline and leveraging state-of-the-art distributed computing techniques. In order to successfully complete the domain-specific tasks the chatbot needs to have access to data.   By favoring modern frameworks and engineering best practices, the project will contribute valuable insights into the design and implementation of AI-driven chatbots.  1.4   Goals  The goal of this thesis is to design the architecture and develop a chatbot that enables novice users to interact with PlaymakerAI’s platform through natural language queries to create, analyze and visualize football datasets. This goal is divided into two sub-goals: 1. For PlaymakerAI – Improve accessibility for users without prior data analysis experience by enabling intuitive dataset retrieval and interpretation, making football analytics more user-friendly. 2. From an academic perspective – Contribute to AI-driven data interaction by designing an efficient pipeline for LLMs using RAG in structured domains, with a focus on quality dimensions such as retrieval accuracy, latency, token efficiency and usability in a real-world setting.

=== 페이지 21 ===

Introduction | 5  Furthermore, the chatbot must be able to perform the following three core tasks:  •   Answer general questions regarding football and specific KPIs which PlaymakerAI have developed or uses on their platform.  •   Opponent Analysis – The chatbot should analyze match data between two teams,   ensuring that it retrieves relevant datasets from their respective leagues and provides insights in both text and dashboard formats. The output should be available as a concise text summary, a structured dashboard or both.  •   Match Analysis – The chatbot should generate insights for a specific match, similar to Opponent Analysis but with fewer data tabs.   The output should be available as a concise text summary, a structured dashboard or both. The final outcome will be a fully developed chatbot pipeline and prototype, evaluated for its effectiveness in assisting users with football analytics while providing insights into domain-specific chatbot development.  1.5   Contributions  This thesis contributes a modular chatbot system tailored to football analytics, designed to run efficiently on CPU-only hardware. A full pipeline has been built for PlaymakerAI, from design to implementation and tailored specifically to their platform, data, and user needs.   The system implements a RAG pipeline optimised for near-identical football datasets, combining multilingual embeddings, query normalization with RapidFuzz, and a hybrid retriever setup.   It integrates OpenAI’s assistant API through structured function calling, enabling automatic generation of dashboards in response to natural language queries. The pipeline incorporates several state-of-the-art techniques and models, including the use of multilingual instruct embeddings, hybrid semantic retrieval, and LLM-as-a-judge for in-context data validation.   The architecture is kept modular using LangChain, allowing for future model upgrades,   fine-tuning,   or integration into production environments with minimal changes.

=== 페이지 22 ===

6 | Introduction  1.6   Delimitations  This project includes a full pipeline from collecting the data from Playmak- erAI’s API to returning an answer based on that data to the user. It will not focus on data maintenance, UX or integrated into PlaymakerAI’s source code. Fine-tuning will not be done because there is not enough data to train new models on and the prototype will run on a CPU-only laptop.  1.7   Structure of the thesis  Chapter 2 presents relevant background information. Chapter 3 presents the methodology and method used to create the chatbot and answers the research questions. Chapter 4 presents the details of architecture and goes into further detail of the different main components. Chapter 5 presents the benchmarks of the system and its components. Chapter 6 presents insights from the project. Chapter 7 presents the final conclusion of the project.  1.8   Ethics and Sustainability  This system relies heavily on LLMs and embedding models.   In order to train these a lot of computational power and therefore electricity is required. This project does not include any sensitive personal data, but data from football matches.   However, users’ requests can be visible for PlaymakerAI and OpenAI ∗   (OpenAI saves the contents of the chats for 30 days for abuse monitoring [1], and PlaymakerAI has access to the logs). Lastly, one ethical point with this project is that whenever a user requests the UEFA Champions League, the system implies that it is for the men’s cup, this is because the women’s Champions League is referred to as UEFA Women’s Champions League. In order to mitigate this potential issue one straight forward solution would be to clearly label data as either men or women data.   Furthermore PlaymakerAI must choose a policy either default to men’s data, default to women’s, or never assume.   The safest path would be to never assume, but that would also include a follow up message needed which is not desirable. It also sheds light on issue with biases in training models and other software development where minorities are sometimes forgotten or overlooked leading to skewed models and LLMs. Since PlaymakerAI offers data for more men’s  ∗ https://openai.com/

=== 페이지 23 ===

Introduction | 7  leagues than women’s, this project has chosen to focus on men’s football as the primary target.

=== 페이지 24 ===

8 | Background  Chapter 2 Background  To provide the context for the work represented in this thesis, this chapter outlines the core concepts, methods, and prior research that underpin the proposed chatbot system. The background is divided into three main parts: 1.   Core   Techniques :   This   includes   Function   Calling   for   enabling structured responses and automation; RAG and how it is used; and the use of Embeddings, Semantic Search, and Vector Databases for retrieving domain-specific datasets efficiently. 2.   Tools :   Describes   the   frameworks   used   in   the   system,   including  LangChain ∗   for managing interactions and workflows, and the vector databases that support fast and scalable retrieval. 3.   Evaluation and Use Cases : Covers evaluation methods such as LLM- as-a-judge for validating output, and highlights practical applications of the system in football analytics.  2.1   Core-Techniques  2.1.1   Function Calling  Function calling enables LLMs to execute structured API calls, retrieve real- time data and generate outputs such as JSON objects that integrate directly into applications [2]. This is particularly relevant for generating football match analysis dashboards and opponent analysis dashboards for a user query. Hugging Face †   [3] provides tools for implementing function calls in data  ∗ https://www.langchain.com  † https://huggingface.co/

=== 페이지 25 ===

Background | 9  processing pipelines, where multiple functions are chained together [4]. This is relevant to this thesis, as the chatbot must retrieve data, analyze it and format outputs in JSON for dashboard generation. Recent research has introduced models explicitly trained for function calling, such as Granite-20B-FunctionCalling [5].   These models specialize in structured data retrieval, making them a potential benchmark for evaluating function-calling performance in this thesis. One critical consideration is token cost.   Function calling introduces additional overhead, as defining and executing functions requires extra tokens. OpenAI’s documentation suggests that fine-tuning can reduce function-calling costs [6].  2.1.2   Retrieval-Augmented Generation (RAG)  RAG is a framework that enhances LLMs by integrating external knowledge retrieval with text generation. Unlike traditional LLMs, which rely solely on pre-trained parameters and can struggle with outdated or missing information, RAG retrieves relevant documents from external databases before generating responses.   This improves accuracy,   reduces hallucinations and allows models to dynamically adapt to new information without requiring expensive retraining. The primary advantage of RAG is its ability to ground language model outputs in external knowledge, making it particularly effective for knowledge-intensive tasks such as question answering, information retrieval and summarization [7]. RAG has evolved through several architectural advancements, categorized into Naïve RAG, Advanced RAG and Modular RAG. Naïve RAG follows a straightforward "Retrieve-Read" process where a retrieval component fetches the most relevant documents and the language model incorporates them into its response generation. While this approach improves factual grounding, it suffers from retrieval inefficiencies, often leading to irrelevant or not optimal results due to query misalignment [7]. To address these issues, Advanced RAG introduced optimizations such as query expansion, re-ranking retrieved results and hierarchical indexing. This stage of development refined how models interact with external knowledge, incorporating metadata attachments and structured retrieval techniques to improve relevance.   Furthermore,   context compression techniques were introduced to filter out less relevant retrieved data, ensuring that only the most critical information is passed to the model. The most sophisticated evolution is Modular RAG, which incorporates

=== 페이지 26 ===

10 | Background  adaptive retrieval, iterative refinement and hybrid retrieval methods. Unlike previous approaches, Modular RAG dynamically updates retrieval strategies based on query complexity and document importance.   It can integrate specialized retrieval components, such as memory modules or multi-stage retrieval pipelines, allowing greater flexibility in handling complex reasoning tasks. This modular approach enables higher precision, reduced redundancy and improved robustness against noise and misinformation [7]. RAG significantly enhances LLMs by addressing key limitations inherent in purely generative models.   First, it enables real-time knowledge updates, allowing models to pull the latest information from curated databases rather than relying on static training data. This makes it ideal for rapidly changing domains where new information continuously emerges.   Second,   RAG improves response accuracy and interpretability by grounding answers in retrievable sources, reducing the risk of fabricated or unverifiable content. Third, it enhances efficiency by leveraging external retrieval rather than requiring full model retraining whenever new data needs to be incorporated [7]. Additionally,   RAG has proven particularly useful in domain-specific applications,   where   general-purpose   LLMs   struggle   due   to   a   lack   of specialized knowledge.   By integrating retrieval mechanisms that focus on structured and pre-processed domain datasets, RAG enables LLMs to provide more precise and contextually relevant answers, outperforming fine-tuned models in factual consistency [8]. Despite its advantages, RAG is not without challenges. Retrieval quality is a major limiting factor, as poor document selection can introduce irrelevant or misleading information into the generation process. If retrieval fails to capture the most relevant documents, the model’s response quality deteriorates. Noisy or contradictory data sources pose another risk, as LLMs may generate outputs influenced by incorrect or conflicting retrieved content. Balancing retrieval efficiency and cost is another challenge, particularly in systems that require frequent querying of large-scale databases [8]. RAG represents a critical advancement in Natural Language Processing (NLP) by enabling LLMs to dynamically incorporate external knowledge, improving factual accuracy and adaptability.   Its development from naïve to   modular   architectures   reflects   continuous   efforts   to   refine   retrieval mechanisms and optimize integration with language generation.   While RAG significantly reduces hallucinations and enhances domain specificity, challenges related to retrieval quality,   noise robustness and integration complexity remain active areas of research. As models continue to scale and

=== 페이지 27 ===

Background | 11  real-world applications demand greater accuracy, advancements in retrieval techniques and hybrid fine-tuning approaches will be crucial to maximizing the effectiveness of RAG [8]. JayBot is an example where a university chatbot was built with GPT-3.5 Turbo, an embedding model and a vector database in order to give accurate answers about courses, fees and admissions without hallucination.   Instead of just trusting the LLM’s memory (which gets outdated), it stores school- specific info in a vector database and uses embeddings to find the right details when someone asks. The backend uses Python with Flask to handle requests, LangChain to manage searches and Pinecone to store the vectors, so JayBot can pull real data instead of guessing. In tests, JayBot beat a human-run live chat by replying in under five seconds (vs. 8.5 hours) and was judged accurate by over 90% of users (compared to 28% for live chat). This shows that using a retriever-and-generator setup not only cuts down on wrong answers but also makes student support faster [9]. The combination of LLMs and vector databases represents a major shift in AI system design, enhancing retrieval efficiency while reducing the reliance on costly model fine-tuning [10].  2.1.3   Fine-Tuning  Fine-tuning is an essential technique for adapting LLMs to domain-specific applications.   It enables a model to specialize in a particular area without the need for training from scratch, making it significantly more efficient [11]. Fine-tuning is especially valuable when using datasets that were not included in the model’s original training data, such as domain-specific information. A study by Yoshida et al.   [12] highlights the non-deterministic nature of LLMs, such as GPT-3.5 Turbo and discusses fine-tuning techniques that mitigate inconsistencies in responses. Their findings suggest that data quality is more critical than quantity. Similarly, Carvalho et al. [13] emphasize the need for dual evaluation methods: expert assessments and user-based surveys. Research by Oliver et al. [14] demonstrates that fine-tuning with as few as 200 high-quality samples can significantly boost accuracy (from 70% to 88%). Additionally, studies comparing fine-tuning, RAG and prompt engineering indicate that fine-tuning achieves the highest accuracy [15].   While RAG reduces hallucination risks by incorporating external knowledge, fine-tuning enhances the model’s contextual consistency, which is critical for a structured analytics chatbot. Another popular technique is Parameter-Efficient Fine-Tuning (PEFT),

=== 페이지 28 ===

12 | Background  where methods like Low-Rank Adaptation (LoRA), QLoRA and DoRA are used to customize LLMs.   These methods reduce computational costs by freezing most of the model’s parameters and training only task-specific layers [16]. Given the computational constraints of fine-tuning large models, PEFT techniques will be explored to enhance the feasibility of this project.  2.1.4   Embeddings, Semantic Search and Vector Databases  Embeddings are numerical representations that capture the semantic meaning of data, such as text, images, or audio.   By converting information into high-dimensional vectors, embeddings enable the measurement of similarity between different pieces of data based on their contextual meaning.   For instance, sentences with similar meanings are represented by vectors that are close together in the embedding space, facilitating tasks like semantic search and recommendation systems [17]. Semantic search leverages embeddings to improve information retrieval by focusing on the meaning of queries rather than relying solely on keyword matching.   Traditional search methods often fall short when users employ varied phrasing or synonyms.   Semantic search addresses this limitation by mapping both queries and documents into the same vector space using embeddings.   This approach allows for the identification of semantically related content, even when exact keyword matches are absent. For example, a query about "benefits of voting" would retrieve documents discussing "advantages of participating in elections," as their embeddings would be proximate in the vector space [17]. Vector databases are specialized systems designed to store and manage these high-dimensional embeddings efficiently. Unlike traditional relational databases that rely on exact matches, vector databases facilitate Approximate Nearest Neighbor (ANN) searches, enabling rapid retrieval of semantically similar data.   This capability is crucial for applications like semantic search, where the goal is to find information that closely matches the intent behind a user’s query. By organizing embeddings in a manner that preserves their semantic relationships, vector databases enhance the performance and scalability of AI applications [17]. The integration of embeddings, semantic search and vector databases represents a significant advancement in information retrieval and natural language processing.   This combination allows systems to understand and process meaning-based data,   leading to more accurate and contextually relevant results. As AI continues to evolve, the adoption of these technologies is expected to expand, offering more intelligent and efficient solutions across

=== 페이지 29 ===

Background | 13  various domains [17].  Embeddings   Embeddings are numerical representations of data that capture their   semantic   meaning   in   a   high-dimensional   vector   space.   These representations enable machines to measure similarity between different data points, such as text, images or audio allowing for powerful applications in search engines, recommendation systems, code retrieval and AI-driven assistants   [17].   By   encoding data   into   vectors,   embeddings   facilitate semantic search, clustering, classification and other ML applications where understanding contextual relationships is essential. These   numbers   encode   the   meaning   of   the   sentence,   allowing   for comparisons with other embeddings to determine similarity.   Embeddings that are numerically close in the vector space correspond to semantically similar concepts. OpenAI, for example, has demonstrated how embeddings can capture meaning at a fine-grained level, where a phrase like "canine companions say" produces an embedding closer to "woof" than "meow" due to its contextual meaning [18]. Embeddings extend beyond natural language applications.   Images can also be embedded in a vector space, enabling comparison between textual descriptions and visual content.   This capability powers image search, classification and automatic caption generation,   allowing AI models to associate different modalities in a semantically meaningful way [18]. The strength of embeddings lies in their ability to preserve context and meaning in ways traditional keyword-based search methods cannot. Standard search approaches often fail when queries use different wording,   while embedding-based search considers semantic relationships, making retrieval more intuitive and effective [17]. Despite their advantages, embeddings require specialized infrastructure for large-scale use.   Working with millions or even billions of embeddings demands efficient storage and retrieval mechanisms, which is where vector databases play a crucial role.   These databases optimize searches in high- dimensional spaces, enabling real-time similarity comparisons and scalable AI applications [18].

=== 페이지 30 ===

14 | Background  2.2   Tools  2.2.1   LangChain  LangChain ∗   is an open-source framework that provides standardized ab- stractions and components for building applications powered by LLMs. LangChain is an orchestration layer, which lets developers link LLM calls with external data sources,   APIs or tools,   so tasks like conversational question-answering, retrieval-augmented generation or analytics chatbots can be assembled together.   Because it decouples code from any single model or service provider, software teams can swap providers, compare prompts and inject domain data without retraining models, speeding experimentation and avoiding lock-in.   In short, developers can choose LangChain to build LLM-powered chatbots and analytics assistants fast, flexibly and with seamless access to their own data. This allows for great modularity [19, 20].  2.2.2   Vector Databases  Vector databases have become essential components in modern AI applica- tions, enabling efficient storage, retrieval and similarity searches for high- dimensional data.   Several vector database implementations exist,   each optimized for different use cases.   For instance, Pinecone is tailored for large-scale enterprise applications, offering a fully managed, cloud-based vector database that simplifies high-performance similarity search and AI applications through automated infrastructure scaling and real-time indexing [21]. Other solutions prioritize ease of implementation and research. FAISS, an open-source library from Facebook AI Research, is optimized for large- scale, high-dimensional vector search and clustering, particularly with GPU acceleration. It functions as an embedded library, requiring external storage for full database management [22, 21].   Weaviate is another open-source vector database that uniquely combines semantic search with graph-based relationships, supporting both structured and unstructured data with native ML model integrations for vectorization, and is available in both cloud and self-hosted options [21, 23]. The following overview highlights their distinct capabilities. Milvus is another open-source, scalable vector database which is optimized for managing large-scale, high-dimensional embeddings.   Milvus supports a variety of indexing techniques and search metrics, allowing for efficient  ∗ https://python.langchain.com/docs/introduction/

=== 페이지 31 ===

Background | 15  retrieval in diverse AI applications. Milvus is designed to be highly adaptable, supporting both cloud-based and on-premises deployments.   Its ability to handle billions of vector records efficiently makes it a strong choice for AI- powered search, recommendation systems and real-time analytics [21, 23]. Qdrant is an open-source vector search engine optimized for high- dimensional similarity search. It offers features such as payload filtering and full-text search, making it a versatile choice for applications requiring more than just vector search.   With support for both cloud-based and self-hosted deployments, Qdrant is well-suited for scenarios requiring efficient vector search capabilities combined with additional search filters [21, 23]. Chroma, also open-source, is a vector database that focuses on ease of use and rapid development of embedding-based applications. It provides seamless integration with Python and ML frameworks, making it particularly useful for AI researchers and developers working on projects requiring fast retrieval and metadata filtering. Chroma is well-suited for applications involving RAG and semantic search, offering a streamlined approach to embedding management [21, 23]. Each of these vector databases has distinct strengths and the choice of which to use depends on factors such as scalability, ease of integration and   specific   application   requirements.   For   enterprise-scale   AI   search and recommendation systems, managed services like Pinecone provide a streamlined deployment experience, while open-source solutions like Chroma and FAISS cater to research-oriented and customizable implementations [23]. Finally, its important to keep an eye on the costs when using the full services. Vector indexes (e.g., Hierarchical Navigable Small World (HNSW)) can balloon beyond raw embedding size.   The paper stresses tuning index-type and hardware to hit your recall/latency targets and notes that self-hosting open- source engines (Chroma, Qdrant, etc.) on commodity storage can cut ongoing fees compared with fully-managed services [23].  2.3   Evaluation and Use Cases  2.3.1   LLM-as-a-judge  In recent years, using LLM for judging data has become popular. The main idea is simple, eliminate the human involvement and have an LLM judge, score or compare data.   On paper, LLM-as-a-judge [24] divides into three categories: pairwise comparison of data, single answer grading and reference- guided grading [24]. Pairwise comparison is when the LLM is presented with

=== 페이지 32 ===

16 | Background  a question and two answers with the goal of responding which of the answers is best. Single answer grading is when an LLM is aksed to score one answer. Lastly, the reference-guided grading is when the LLM receives a reference solutions in the prompt. Their experiments showed that an LLM-as-a-judge agreed with human experts about 80% of the time [24].  2.3.2   Applications in Football Analytics  Conversational AI has been explored in football analytics, primarily in live commentary and question-answering systems. Andrews et al. [25] developed AiCommentator, an AI-driven football commentary system. While their focus was on live match commentary, this thesis differs by emphasizing data-driven analysis rather than real-time broadcasting. A similar project, Chatbol, was developed to provide information on Spain’s La Liga [26]. Chatbol combined a question-answering system (using Wikidata) with a retrieval-based chatbot, but evaluations revealed limitations in handling intent variations. This thesis aims to overcome this limitation by fine-tuning an LLM for structured football analytics, ensuring improved query understanding. The key differentiation of this thesis is its focus on structured dataset retrieval   and   function   execution,   rather   than   general   knowledge-based responses. While prior works have focused on live match commentary and retrieval-based systems, this project integrates function calling and semantic search to develop a chatbot capable of executing analytical tasks on football data that is not publicly available using the RAG component.  2.4   Summary  This chapter addresses the challenge of making specialized football analytics data accessible and usable through a conversational interface by detailing the key components and their problem solving capacities.   To overcome the limitations of general-purpose LLMs and tailor them specifically to the football domain, fine-tuning techniques are presented as essential for achieving domain-specific understanding and performance. The chatbot’s requirement to perform practical tasks beyond mere text generation, such as creating dashboards, is met by function calling, which enables the LLM to execute structured API calls and generate outputs like JSON. To address the significant challenge introduced by LLMs is the potential for outdated knowledge and hallucination, RAG is utilized.   RAG enhances

=== 페이지 33 ===

Background | 17  the LLM by dynamically integrating external, up-to-date knowledge, therefore grounding responses in factual data and improving accuracy. The operational mechanics of RAG are constructed by several interconnected technologies: embeddings,   which numerically capture the semantic meaning of data; semantic search, which leverages these embeddings for nuanced, meaning- based information retrieval rather than simple keyword matching; and vector databases, which provide the specialized infrastructure for efficiently storing, managing and querying these embeddings.   The review of related work, including systems like JayBot that successfully employed RAG with a vector database for domain-specific university information and broader surveys on the integration of LLMs with vector databases, further validates this integrated technological approach. These components collectively creates a robust architecture designed to provide accurate, context-aware and interactive analytical capabilities for football data.

=== 페이지 34 ===

18 | Methods  Chapter 3 Methods  The purpose of this chapter is to provide an overview of the research methodology applied in this thesis. Section 3.1 explains the overall research process   followed   by   Section   3.2   which   details   the   research   paradigm. Section 3.3 focuses on the data inspection, selection and preprocessing needed for this project.   Section 3.4 describes the goal of the system and the functionality.   Section 3.5 describes the intended system architecture. Section 3.6 describes the methods used to evaluate the reliability and validity of the collected data and finally Section 3.7 which includes information on the system documentation.  3.1   Research Process  The overarching process for this thesis follows an iterative design science approach [27],   used together with experimental validations.   The work proceeds in multiple steps or stages, as follows. The first step included obtaining domain knowledge (football analytics metrics and understanding of PlaymakerAI’s data and platform). Interviews were held with employees at PlaymakerAI in order to understand their users, their platform and their data.   Furthermore, these interviews were used to determine the requirements for the system. Then,   a small language model (GPT-tiny) was fine-tuned using the knowledge of the domain and the requirements in order to create synthetic prompts.   The quick pilot study verified that the technical pipeline (LoRA + Hugging Face) is functional and also provides the researcher with a better understanding of the tools, frameworks, and techniques. The fine-tuning was initially intended to be used in the project, but was later deemed hard to

=== 페이지 35 ===

Methods | 19  complete because of the lack of data to train the model on. Next step after the requirements from PlaymakerAI were compiled, with the knowledge from the literature review and pilot study, in order to create the chatbot for PlaymakerAI. Tools, frameworks, and technologies were chosen based on both the literature and available resources at the company. This was an iterative process, where the final product evolves through repeated testing and refinement [28]. The next step was to integrate the OpenAI Assistant into a pipeline. The pipeline includes a vector database, a retrieval component and function- call endpoints to simulate the creation of dashboards on PlaymakerAI’s platform. Retrieval performance (Recall@k, Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG)@ k ) was benchmarked. Token usage and system instructions were refined to reduce cost and errors. Then the performance measurements were recorded, focusing on both correctness (using F1 score) and cost metrics (token usage). Findings were used to further improve the pipeline until the final prototype is stable. This design science cycle was grounded in the principle of building an artifact, in this case a domain-specific football analytics chatbot and rigorously evaluating it to ensure it meets stakeholder requirements [29].  3.2   Research Paradigm  This thesis primarily adopts a design science research paradigm [27], which emphasizes the creation and evaluation of an artifact. Design science aims to produce knowledge that is both relevant and applicable in real-world contexts, aligning with PlaymakerAI’s objectives for the chatbot. Additionally, the thesis takes a positivist stance in measuring the chatbot’s performance, focusing on the objective metrics mentioned above. According to Easterbrook et al.[28], a positivist approach uses quantitative methods to assess whether the created artifact meets predefined success criteria, e.g., improved retrieval accuracy. However, the project also uses elements of interpretivism or exploratory inquiry when examining why the system might fail certain queries and how domain-specific language influences results. This iterative reflection is characteristic of design science in software engineering, where insights from failed or partial prototypes feedback into redesign and refinement [29]. The research combines:  •   Deductive Reasoning:   Formulating hypotheses on model performance

=== 페이지 36 ===

20 | Methods  (e.g., that implementing RAG would increase accuracy) and testing them using controlled experiments.  •   Inductive Reasoning:   Adjusting system instructions,   data collection methods or embedding strategies based on observed faults, generating new hypotheses for subsequent cycles. Together, these aspects form a pragmatic, design-oriented paradigm that both builds and measures the artifact, striving to balance academic rigor with industrial relevance.  3.3   Data   Inspection,   Selection   and   Prepro- cessing  The   necessary   data   for   creating   a   football   analysis   was   provided   by PlaymakerAI. There are two significantly different types of data that the system uses. Either data for a specific game, for example Liverpool vs Manchester United, or data from a specific competition.   A specific competition, in the context of football, can be a league or a cup, such as the Premier League season 2024/25 or the Europa League 2023/24. This data is gathered by PlaymakerAI and then converted to different KPIs. The data can be structured on two levels: team or player. Team-level data focuses on the performance of entire teams in a match or competition, with KPIs such as goals scored, number of throw-ins, or corners taken by each team. Player-level data, on the other hand, tracks the individual performance of every player involved in a match or competition. These KPIs include metrics like the number of passes completed, minutes played, or goals scored by a specific player. In this project, all benchmarks and evaluations have been performed using player-level data.   This means that each player is included in the dataset and has the same set of data fields, such as goals scored even if the value is zero.   Player-level data is particularly useful for opponent analysis, as it helps identify which players are most influential.   It also supports scouting by highlighting individual performance, and allows for deeper insights into how specific players impacted a match. While player-level data offers more granular analysis, team-level data may be more suitable for use cases like journalism, where the goal is often to summarize the outcome of a game rather than analyze individual contributions. Importantly, all games used in this project rely on a preset called "Match Analysis," developed by Playmaker. This preset includes a defined set

=== 페이지 37 ===

Methods | 21  of KPIs and uses player-level data by default. Both team and player-level data can be applied to matches or entire competitions. For the benchmarks and other tests of the system, user queries to the chatbot have been synthetically generated, with input from PlaymakerAI on how users use their platform and chatbot. The history of the previous chatbot has been analyzed, but the chatbot has not been available to all users, only to beta users and therefore there are not sufficient data for the usage of this chatbot.   Therefore, synthetic queries were deemed the better option. Furthermore, because the chatbot will include drastically new functionality not available prior to this, there is no user history that can be utilized.  3.3.1   Data Provided  PlaymakerAI creates structured data from football matches played around the world and organizes this information into KPIs, categorized by either competitions   (leagues   and   cups)   or   individual   matches.   This   data   is accessible to developers through the PlaymakerAI API. A key distinction within the data ecosystem is between global datasets and user-generated datasets. Global datasets refer to pre-configured, platform-maintained data for major competitions such as the Premier League 2024/25, Champions League 2024/25, Allsvenskan 2025, and Svenska Cupen 2025.   These datasets are available to all users by default, are updated nightly and use a KPI structure defined by PlaymakerAI. They include data on a player level. This makes the global datasets convenient for most users, as they are ready to use without any manual configuration. In contrast, user-generated datasets offer flexibility and customization. Users can create their own datasets for any competition, even for those already covered globally.   If they wish to modify the KPI set or change the level of data (player or team) to a specific use case. This is especially useful for more targeted analyses or for competitions not included in the global dataset. While competitions can exist in both global and user-defined forms, individual matches are only available through user-generated datasets.   PlaymakerAI does not provide global match-level datasets by default. Instead, users must manually create match datasets by selecting the game, choosing relevant KPIs, and defining whether the data should be on a player or team level. All user-generated datasets are private and unique to the individual user.   Even if two users generate data for the same match or league, their datasets may differ depending on the chosen KPIs and structure. To support this, the platform offers a variety of presets, such as “Opponent Analysis,”

=== 페이지 38 ===

22 | Methods  “Attacking Player Scouting,” or “Match Analysis,” which provide predefined KPI structures to streamline the dataset creation process. These presets help users align their data with common analysis tasks while maintaining the flexibility to adjust the details.   The chatbot will have access to all global datasets but also the user-generated data. One of the constraints on the system is that the chatbot only have access to data which have been created or updated within a month, this is to reduce the storage load on the vector database and to exclude irrelevant old seasons. PlaymakerAI has data to all games that have been played in the big leagues from years back, the chatbot will therefore only focus on games, leagues and cups that has been updated, meaning a game has been played in that competition in the past month.   Meaning the chatbot wont have access to previous season (e.g., Premier League 2020/21). For this project 14 games were randomly selected and created on PlaymakerAI’s platform in order to be used for testing purposes. The data coverage can be found in appendix A. The structure of the data is very similar, regardless of whether it’s a match or a league/cup data. It always consists of the name of the data, such as "AIK - Hammarby" or "Champions League 2024/25". If it’s a league/cup, it has a season name, if it’s a game, it does not. The data structure has a type, which could be player or team. It has timestamps for the last updated and an ID for the data. Furthermore, the data has a list of headers, which is a short placeholder for the KPIs used, these could be "PassProgDist90", "Game Units", or "xG90". Lastly, the most important part, it has an object of data for every player that is part of the dataset. The object contains all the KPIs mentioned in the headers but also the values for all of those, such as: "’PassProgDist90’: 9.39", "’Game Units’: 2.3" or "’xG90’: 0.81". The global datasets cannot be changed, but the datasets which has been created by the users need to have different KPIs selected (or pre-sets, provided by PlaymakerAI). Below a comparison of data for a match and a league. Full example of a data provided can be found in the appendix E.

=== 페이지 39 ===

Methods | 23  Table 3.1: Top-level JSON keys: match vs. league dataset  Key   Match-level value   League-level value  aggregate   true   true data   [ {} ]   [ {} ] headers   [ ]   [ ] name   "Team 1 vs. Team 2"   "League 2025" seasons   [ ]   [ "League, 2025" ] status   ready   ready type   player   player updated   DateTime   DateTime dataset_id   XXX   XXX Data holds the actual player-level KPIs, while headers is the list of column names for those KPIs.   Out of 25 columns in the match file and 23 in the league file, 9 headers are identical.   That means the overlap is 36% of the match columns and 39% of the league columns. Although each dataset adds a few KPIs specific to its context, the core player information is largely shared as can be seen in (Table 3.1).  3.3.2   Synthetically Generated Queries  In order to effectively create a larger number of queries while keeping the creation random and not fully defined by the author, it was decided to use LLMs for synthetically generating queries for testing different parts of the systems.   Based on the three main use cases of the system, all scenarios could be well covered:   general questions, opponent analysis, and match analysis. This also reduced the biases for the benchmarks. The query set was intentionally kept small, with a focus on ensuring reliable handling of Swedish competitions and major European leagues and cups (e.g., La Liga, Premier League, Serie A, Bundesliga, Europa League, Champions League). To test edge cases, additional queries targeting leagues like Eredivisie and Ligue 1 were included.   Given PlaymakerAI’s customer base, extra emphasis was placed on the Swedish football market, particularly Allsvenskan and Svenska Cupen. The synthetically created datasets were used to perform the integration test as well as for the benchmarking of the fuzzy mapping (rapidFuzz ∗ ) where user queries was generated based on the rules that the set was structured with 50% of queries targeting teams known to the core   TEAM_LEAGUE_MAP   and  ∗ https://rapidfuzz.github.io/RapidFuzz/

=== 페이지 40 ===

24 | Methods  50% targeting unknown (out-of-scope) teams. For the known teams, roughly half of the queries involved valid team-competition combinations as per the  TEAM_LEAGUE_MAP , while the other half intentionally paired known teams with competitions not listed for them in the map.   The   TEAM_LEAGUE_MAP   is a predefined dictionary that maps teams in Allsvenskan and Premier League to a list of competitions they have participated in, such as leagues or cups. It serves as a reference for validating whether a given team-competition pair is valid. Furthermore, the RAG system was also tested using synthetically generated queries in order to benchmark MRR and Recall@K.  3.4   Goal of the system  As stated previously, the system is built to support three main use cases: general question answering, opponent analysis, and match analysis.   For both opponent and match analysis, the system can return results either as a text-based analysis in the chat, a dashboard created on the platform, or a combination of both. The text response is generated by the OpenAI assistant and is returned directly in the chat interface. It often includes markdown tables to structure the information clearly, but these tables are not to be confused with the actual dashboards. The   dashboard   is   a   separate   visual   component   generated   on   the PlaymakerAI platform.   Unlike the text response, which is conversational and shown in the chat, the dashboard provides a more interactive and visual summary of the data. While the chatbot is designed to create these dashboards directly on the platform, it is important to note that, as this is a proof of concept, the system is currently not connected to PlaymakerAI’s production backend.   Instead, the system replicates the exact JSON structure required by PlaymakerAI to generate dashboards. This design choice ensures that the chatbot can be integrated seamlessly with the production environment in the future.  3.4.1   Opponent Analysis  When a user has requested an opponent analysis, one will be built using PlaymakerAI’s data, and then a URL link to that dashboard will be returned to the user. In order to create the dashboard for the opponent analysis, the most influential players of the team need to be extracted. The system can utilize one of PlaymakerAI’s KPIs called PlaymakerAI ranking, based on this ranking, the assistant extracts the most influential players. The dashboard consists of

=== 페이지 41 ===

Methods | 25  10 widgets, where every widget focuses on a certain analytical part of the opponent. The parameters needed to dynamically create the dashboards are: Team name, season id, name of the competition, label and list of influential players. The team name is used to filter out only the opponent the user has requested, needed because the data for a league, e.g., Bundesliga 2024/25, consists of players from multiple teams, the widget then needs to filter on the requested team. The season id is used to find the correct data in PlaymakerAI’s backend and the naming of the data is done with the season name. Lastly, if the user wants to add a specific name to call the dashboard, this will be done through the Label parameter. Lastly, the list of influential players is used to highlight the most influential players of the opponent team in the dashboard and analyze them more in-depth. The full creation of the dashboard can be found in the Appendix B.1.  3.4.2   Match Analysis  The match analysis is also created by the system and returned as a URL to the user in the chat.   It consists of seven widgets.   For this to be properly created and work together with PlaymakerAI’s system, there are the following dynamic parameters that the chatbot needs to determine and set: home team, away team, season id and label. The home team and away team are used to determine which teams should be analyzed together with season id, all of them are necessary for correctly creating the dashboard. Lastly, a label is used to create the URL link to the dataset. The full JSON object can be found in the Appendix B.2.  3.4.3   General Question  For handling general questions, the output can be whatever the user requests, still within the context of football.   PlaymakerAI expects most of the total requests will be classified as general.   This does not require the system to perform RAG and therefore can send the request straight to the OpenAI assistant.  3.5   System Architecture  This system design will not be directly built into the system of PlaymakerAI. Therefore the creation of the dashboards on PlaymakerAI’s platform needs to be simulated. The system architecture will look as follows:

=== 페이지 42 ===

26 | Methods  Figure 3.1: Chatbot Architecture

=== 페이지 43 ===

Methods | 27  The system has three main components.   First is the intent component (Query Classification), which identifies whether the user is asking for a match analysis, opponent analysis, or a general question. This module is necessary to handle diverse input types, as simple keyword matching is unreliable. It also extracts and normalizes the mentioned teams and leagues, and provides feedback if the user input refers to an invalid or unsupported combination. Once the intent is identified, the RAG component is triggered (if the request involves analysis) to fetch the corresponding data based on the user query. This step includes preprocessing the query, retrieving the relevant dataset, and cleaning the data to remove redundancy and prepare it for downstream processing.   The third component handles the interaction with the OpenAI Assistant, which includes validating the data (LLM-as-a-judge), generating natural-language analysis, and optionally creating dashboards.   Function calling enables dynamic conversations, allowing the user to choose whether or not a dashboard should be returned.   It was also used to align with PlaymakerAI’s existing infrastructure and project requirements.  Why RAG   The RAG system serves as the central component enabling the chatbot to interact with PlaymakerAI’s private data.   This allows the chatbot to perform context-aware actions, such as generating textual analyses or configuring dashboards, based on up-to-date and specific football data. Which would be impossible otherwise because the data needed cannot be found publicly, PlaymakerAI creates its own data.   This capability is also essential as the underlying LLM (in the assistant) operates with a knowledge cutoff (October 2023 for the models used [30]) and therefore lacks information regarding recent games, leagues, or specific datasets requested by the user, which is a must for a chatbot in this context. The RAG architecture ensures the system can access up-to-date information, facilitating PlaymakerAI’s ability to regularly update the chatbot’s knowledge base, potentially on a nightly basis, using the scripts for fetching data from their database and updating the data in the vector database.  3.6   Assessing   reliability   and   validity   of   the data collected  This project used an ongoing quality checking process that looked at individual component tests and how the whole system worked together.   Before users got involved, core parts like the RAG system were benchmarked and the

=== 페이지 44 ===

28 | Methods  RapidFuzz settings were tuned.   Choices about RAG settings, embedding models, and how to write the LLM prompts/system instructions were made based on tests and related work. Integration tests were performed where the chatbots answers a wide range of synthetically generated queries, which were then manually checked and graded. After the integration test, the final User Acceptance Test (UAT) was held, the quantitative feedback from the adapted BUS-15 (Bot Usability Scale [31]), along with the qualitative insights from think-aloud, open-ended question and observations, were gathered from users interacting with a system whose main parts had already been improved and checked.   This thorough approach, which reflects common practices in the software industry and academia, makes the user data and the final conclusions more trustworthy.  3.7   System documentation  PlaymakerAI received an introduction, walkthrough, and full documentation of the system.   The code is owned by PlaymakerAI and isn’t public.   All material that can be shared is in the appendix, the rest remains proprietary.

=== 페이지 45 ===

Implementation | 29  Chapter 4 Implementation  4.1   Introduction  This chapter provides an in-depth explanation of the implemented system architecture and its components.   The system includes multiple subsystems (components) to function as a football analytics chatbot. It handles three main user scenarios:   requests for opponent analysis, match analysis, or general questions.   The description below focuses primarily on the two analysis pathways (opponent and match), as these need data retrieval for specific contexts.   General questions bypass the data retrieval stage.   Both analysis pathways (opponent and match) follow a similar process: initial cleaning and normalization of the user’s query, followed by a RAG stage responsible for fetching the relevant dataset from a vector database and finally, integration with the OpenAI Assistant API to analyze the retrieved dataset and generate insights or dashboards. This system was primarily developed using Python, leveraging frameworks such as LangChain, the OpenAI API for large language model capabilities, Chroma database for vector storage and Hugging Face libraries for embedding models. In order to get correct data from PlaymakerAI to the local vector storage, a pipeline was built to fetch data using their API and upload it to the Chroma database. The process of fetching data from their API is straightforward and will only be mentioned here, without further discussion in this thesis.  4.1.1   Hardware & Software details  A personal M1 ARM architecture MacBook Pro with 8GB of RAM was utilized for this work. While it is expected that a cloud-based system, such

=== 페이지 46 ===

30 | Implementation  as Google Cloud Platform with greater virtual RAM, would yield enhanced results, PlaymakerAI has specified a requirement for local execution for this project.   This decision aims to avoid dependencies on external software, maintain modularity and minimize operational costs. The code was developed in Python (version 3.9.18), aligning with the backend of PlaymakerAI’s current system.  4.2   Embedding the data  Once the datasets have been retrieved from the PlaymakerAI API, they must be embedded and stored in the vector database. To enable efficient semantic search, each dataset is transformed into a short descriptive sentence written in plain English. This sentence includes the dataset name (e.g., "Premier League, 2024/25"), season, involved teams, and an inferred type based on team count. If more than two teams are present, the dataset is assumed to be from a league or cup; otherwise, it is treated as a specific game. This distinction helps reduce retrieval errors. For example, if a user asks for an opponent analysis of Real Madrid, returning a single match would technically match the team, but fail to capture the user’s intent, which is likely to analyze trends across a full competition.   By embedding this structured sentence instead of the full JSON, the retriever is better able to differentiate between game-level and competition-level data. The descriptive sentence becomes the   page_content   used for embedding, while the full dataset, ID, team names, seasons, and raw JSON are stored as metadata. These are wrapped using LangChain’s Document object and added to ChromaDB via the   add_documents()   method. This design emerged from early issues with mismatched query results and now forms the core of the system’s semantic retrieval strategy.  4.3   User Query Processing  In order to correctly process the query, the system needs to determine the user’s intent and then clean the query as much as possible in order to remove redundant information from the query that might reduce the performance of the RAG component.

=== 페이지 47 ===

Implementation | 31  4.3.1   Query Classification  The system must classify whether a user request corresponds to one of three main scenarios: a general question, an opponent analysis, or a match analysis. Identifying the type of request accurately is crucial because it decides the subsequent workflow.   To address this classification problem, several approaches were evaluated during development, including keyword matching, fuzzy matching and zero-shot classification techniques. While these methods offered varying degrees of success, the final implementation utilizes an LLM for robust intent recognition and entity extraction.   Specifically, the GPT- 4o-mini model is employed via the   classify_query   function. The LLM is invoked with a prompt that outlines the task, defines the desired output format (JSON), provides illustrative examples and includes specific instructions for handling ambiguous queries, such as interpreting questions about a team’s playing style as opponent analysis requests. Listing 4.1: Prompt for Query Classification LLM  1   """Analyze the football query below. First, determine the query ' s primary intent and identify key entities. Then, classify it and extract relevant information into a JSON structure.  2 3   Query: "{query}"  4 5   Return a JSON object with the following fields:  6   1. "reasoning_steps": A very brief step-by-step thought process explaining the classification and entity identification.  7   2. "query_type": one of "match_analysis", "opponent_analysis", or "general_question".  8   3. For "match_analysis": include "team1" and "team2".  9   4. For "opponent_analysis": include "team" and "league".  10 11   Examples:  12   - Query: "Analyze Liverpool vs Man Utd"  13   Output: {  14   "reasoning_steps": "1. Query contains   ' vs '   indicating a match. 2. Two teams identified:  ' Liverpool '   and   ' Man Utd ' .",  15   "query_type": "match_analysis",  16   "team1": "Liverpool",  17   "team2": "Manchester United"  18   }  19 20   - Query: "How does Barcelona play in La Liga"  21   Output: {  22   "reasoning_steps": "1. Query asks   ' How does X play in Y ' , indicating opponent/team performance analysis. 2. Team   ' Barcelona '   and league   ' La Liga '   identified.",  23   "query_type": "opponent_analysis",  24   "team": "Barcelona",  25   "league": "La Liga"  26   }  27 28   - Query: "How is Liverpool playing in champions league?"  29   Output: {

=== 페이지 48 ===

32 | Implementation  30   "reasoning_steps": "1. Query structure   ' How is X playing in Y '   matches opponent analysis pattern. 2. Team   ' Liverpool '   and league   ' champions league '   (normalized to   ' UEFA Champions League ' ) identified.",  31   "query_type": "opponent_analysis",  32   "team": "Liverpool",  33   "league": "UEFA Champions League"  34   }  35 36   - Query: "What is xG?"  37   Output: {  38   "reasoning_steps": "1. Query is a definition question ( ' What is... ' ). 2. No specific teams or leagues for analysis context. Classified as general.",  39   "query_type": "general_question"  40   }  41 42   - Query: "Whos is the best player from the data?"  43   Output: {  44   "reasoning_steps": "1. Query is a definition question ( ' Who is... ' ). 2. No specific teams or leagues for analysis context. Classified as general. 3. User seems to reference a previously given answer",  45   "query_type": "general_question"  46   }  47 48   - Query: "Hur har AIK spelat i Allsvenskan?"  49   Output: {  50   "reasoning_steps": "1. Query   ' Hur har X spelat i Y '   fits the opponent analysis pattern. 2. Team   ' AIK '   and league   ' Allsvenskan '   identified.",  51   "query_type": "opponent_analysis",  52   "team": "AIK",  53   "league": "Allsvenskan"  54   }  55 56   Guideline for opponent_analysis:  57   If a user requests something regarding how a specific team is playing, their performance, or similar patterns (e.g., "How does X play in Y", "hur har X spelat i Y", "hur brukar X spelat i Y"), it should be considered an "opponent_analysis" of team X in league/cup Y.  58 59   Return only the valid JSON object.  60   """  The LLM is instructed to return its classification and extracted entities in JSON format. The system then parses this JSON output to determine the query_type and extract relevant details like team names or league, which guide the next processing steps. The prompt is written in accordance with Chain-of- Thought (CoT) (Section 4.5.3) for optimal results.  4.3.2   Query Cleaning and Normalization  Following the initial classification of the user’s intent (Section 4.3.1), the system cleans the query and normalization step if an opponent analysis is requested.   This step aims to map the user’s input to known entities (teams and competitions) and construct an optimized search query for the RAG component. The reason this isn’t used for match opponent is because the user

=== 페이지 49 ===

Implementation | 33  selects the name of that dataset, and therefore, we do not have a ground truth (which we have for the global datasets used in opponent analysis, where we know the names of the datasets). The core of this process utilizes the   rapidfuzz ∗   library. Selected for its computational speed, similarity scoring capability, ease of integration and MIT license.   Furthermore, this is a library already used by PlaymakerAI, which makes for easier integration with their existing system.   The library is used to match the team and league entities extracted during classification against a predefined Python dictionary,   TEAM_LEAGUE_MAP . This dictionary, provided by PlaymakerAI, contains teams and the specific competitions they participate in for which PlaymakerAI has data coverage and reflects common user requests. The setup uses QRatio currently as its scorer [32, 33]. The objective of this normalization step is to mitigate errors arising from user requests for non-existent team/league combinations (e.g., "analyze Tottenham in Allsvenskan").   Based on consultation with PlaymakerAI, a design decision was made to prioritize finding the correct team over the exact competition requested by the user. The system attempts to identify the most likely team intended by the user using fuzzy matching with a confidence threshold ( TEAM_THRESH ). If a high-confidence team match is found, it then attempts to match the requested league against the known valid competitions for that team using a similar threshold ( LEAGUE_THRESH ). Based on the outcome of these fuzzy matching steps, the final search query for the RAG component is constructed according to the following logic: 1.   Team and League Match:   If both the team and a valid competition for that team are matched with sufficient confidence, the query is simplified and structured for optimal retrieval:   "Find dataset called for the league/cup:   {canonical_league}, that includes data on the team {canonical_team}" . 2.   Team Match Only:   If a confident team match is found, but the requested competition does not match any known, valid competition for that team (or no competition was specified clearly), then the system populates the query with a league corresponding to the main league for that identified team:   "Find dataset called for the league/cup: {canonical_league}, that includes data on the team {canonical_team}" . Where canonical_team would be Allsvenskan for Swedish teams and the Premier League for English teams by default.  ∗ https://rapidfuzz.github.io/RapidFuzz/

=== 페이지 50 ===

34 | Implementation  3.   No Confident Team Match:   If the system cannot confidently match the requested team to an entry in the   TEAM_LEAGUE_MAP , it defaults to using the user’s original extracted entities, acknowledging the higher potential for ambiguity:   "{user_team}, {user_league}" . This structured approach, found through testing different approaches, aims to maximize the chances of retrieving a relevant dataset even when user input is not precise enough or requests data is outside the available scope, while prioritizing team accuracy as per requirements.  4.4   RAG  Implementation details on the RAG component are given below. Firstly, the choice of vector database and embedding strategy will be discussed, followed by the retrieval strategy, and lastly, the post-retrieval data preparation.  4.4.1   Vector Database and Embedding Strategy  A vector database provides the foundation for efficient semantic retrieval within the RAG system.   Chroma is an example of a vector database and was selected because of key project constraints: it’s open-source, available at no cost and supports local deployment. This local operation is essential for maintaining data privacy and provides architectural modularity, simplifying potential future migrations compared to cloud-based services. Furthermore, Chroma’s use of efficient indexing methods like HNSW is well-suited to our chatbot application, where low-latency query responses are vital for a positive user experience [34, 35]. The selection of an optimal text embedding model and the strategy for representing datasets as embeddings were identified as critical for retrieval performance.   A systematic evaluation process was performed, utilizing Hugging Face’s Massive Text Embedding Benchmark (MTEB) leaderboard [36] to identify promising candidates based on retrieval scores,   while considering model size and computational requirements. Several models were benchmarked locally using a purpose-built test suite comprising 86 queries targeting various leagues and match types (Appendix C). The benchmark results, summarized in (Table 5.1), provided quantitative data comparing Recall@5, MRR and average query time across different models and query preprocessing conditions ("Not Cleaned",   "Explicit League/Cup",   "Fully Cleaned").

=== 페이지 51 ===

Implementation | 35  The dataset embedding strategy was refined by selecting the   intfloat /multilingual-e5-large-instruct ∗   Hugging Face model.   Experiments showed that embedding a concise, "humanized" natural language sentence summarizing   key   metadata   (name,   type,   seasons,   teams)   significantly improved MRR over embedding full JSON or structured metadata. This "metadata sentence" became the content for each vector embedding. Crucially, embeddings are generated with   normalize_embeddings=True . L2 normalization is important because it scales all vectors to the same length, which makes the dot product behave like cosine similarity, allowing for faster and more meaningful similarity comparisons. It also ensures fair similarity comparisons by eliminating vector magnitude biases, leading to more stable and accurate retrieval, aligning with the E5 model’s design. The full JSON document is stored alongside each normalized vector in the metadata for complete data retrieval.   This approach uses the descriptive sentence as an implicit instruction for the embedding model, with the full data accessible post-retrieval. Lastly, because of the nature of an instruct model, the query needs to be written in the specified way for instruct models [37].   Meaning the actual string that gets embedded has the format of   "Instruct: Find the football dataset covering the given team(s) and competition in the query. Query: {query}" . First, the context/task is stated, and then the actual query from the user. These instruct models have been specifically fine-tuned for a specific use case, in this case, finding a correct dataset [38]. This structured approach aligns with the model’s training, where it learned to associate specific instructions with desired outcomes, leading to more accurate and consistent results.  4.4.2   Hybrid Retrieval Strategy  The   system   employs   a   hybrid   retrieval   strategy,   implemented   in   the  find_dataset_hybrid   function, to optimize for retrieving the single most relevant dataset for a given query. This design followed experimentation with various methods, including standalone semantic search, standalone BM25 keyword search, custom semantic+keyword boosting and cross-encoder re- ranking. Benchmarking revealed that while standalone semantic search using the selected embedding model often yielded the highest MRR (best top-1 relevance), its performance could degrade on ambiguous queries. Standalone BM25 showed higher recall but lower MRR and cross-encoders introduced significant latency. The chosen hybrid strategy aims to leverage the high MRR  ∗ https://huggingface.co/intfloat/multilingual-e5-large-instruct

=== 페이지 52 ===

36 | Implementation  of semantic search when confidence is high, while incorporating keyword relevance via an ensemble fallback mechanism when semantic similarity is low: 1.   Semantic Search:   Retrieves the top   K   =   3   candidate datasets via cosine similarity ( K_VAL=3 ). 2.   Threshold Check:   Convert Chroma’s distance into cosine similarity and compare the top hit against   THRESHOLD   = 0 . 75 . 3.   Fallback Logic:   If similarity is below the threshold, an   Ensemble Retriever   (from LangChain) combines the semantic results with BM25 scores derived from the K=3 candidates, using weights favoring the semantic score ( weights=[0.8,   0.2] ).   Otherwise,   the top semantic result is used directly. These settings were tuned over multiple runs to balance peak top-1 relevance with robust fallback on ambiguous queries.  4.4.3   Post-Retrieval Data Preparation  Following the successful retrieval of a target dataset’s raw JSON document by the RAG component (Section 4.4), a critical data preparation phase is executed by functions within the   prepare_data.py   module.   This phase is essential before the dataset content is passed as context to the LLM assistant (Section 4.5), serving to refine the data for relevance, enhance focus and minimize token consumption, thereby reducing API costs.   Two primary functions handle this preparation:  •   The   clean_opponent   function is invoked for data intended for opponent analysis.   Its main objective is to isolate and keep only the data entries corresponding to the   canonical_team   identified in the query normalization phase (Section 4.3.2).   To achieve this, the function iterates through player or sub-team entries within the retrieved data. For each entry, it calculates the similarity between the   canonical_team  and the entry’s listed team using   RapidFuzz   fuzz.QRatio   with  utils.default_process   for string preprocessing. The   QRatio   scorer was selected for its balance of computational efficiency and effective similarity assessment, based on initial tests of RapidFuzz parameters. Entries with a   QRatio   score below a threshold of 60 are discarded. This threshold was determined through testing and iterative refinement, with

=== 페이지 53 ===

Implementation | 37  the aim of aggressively removing entries that are definitively not a match for the target team. If the similarity score is uncertain ( ≥   60 ), the entry is retained in the dataset passed to the LLM, allowing the assistant to make the final contextual determination by being the judge. The LLM is supported by its data validation instructions.  •   The   clean_game   function is applied to datasets used for match analysis. This   function   performs   a   more   straightforward   cleaning   process, primarily focused on removing redundant header information that is embedded within the raw JSON structure provided by PlaymakerAI. In addition to team-specific filtering in   clean_opponent , both functions systematically remove the aforementioned ’headers’ property from the JSON data.   The information in headers is part of the raw dataset structure.   This header information includes column names or metadata already understood by the system or irrelevant to the LLM’s analytical task, its removal does not reduce the informational value for the LLM. The comprehensive post-retrieval preparation significantly enhances the quality and focus of the contextual data provided to the LLM assistant. Furthermore, it has benefits in reducing the token count of the final prompt; for example, header removal alone is estimated to save approximately 600 tokens per request, contributing to lower operational costs.  4.5   LLM Orchestration and Analysis Genera- tion  Following the initial query processing, the retrieval and preparation of relevant data via the RAG system (Section 4.4), the core logic execution and response generation are orchestrated by an LLM, specifically, the OpenAI assistant API. The OpenAI assistant was primarily selected because PlaymakerAI is currently part of their system and it was part of the requirements to continue using that service. The assistant framework provides several key advantages for this application: persistent conversation threads for context management, robust instruction-following capabilities guided by system prompts and native support for function calling, which is essential for integrating the chatbot’s analytical capabilities with PlaymakerAI’s backend systems for dashboard generation.   This section details the integration with the assistant,   its configuration, the implementation of function calling and the distinct handling pathways for different query types [1, 39].

=== 페이지 54 ===

38 | Implementation  4.5.1   OpenAI Assistant Integration  The primary interaction with the LLM occurs through the   ask_assistant  function within   updated_langchain.py , which interfaces with a specific OpenAI assistant instance. This function manages the creation of conversation threads, sends the user’s query along with the prepared context (which includes system instructions, conversation history and potentially the cleaned data fetched from the vector database), initiates the assistant run and processes the final response, including handling any intermediate function call requests. The assistant’s operational behavior was refined through iterative adjust- ments to its system prompt based on testing and evolving requirements. Initial comprehensive instructions were adapted to better suit novice users by allowing more flexibility, such as inferring unspecified parameters like the relevant league for analysis.   The final system prompt (Appendix D) concentrates on core principles: maintaining a neutral analytical tone, strict reliance on provided data sources, careful handling of opinions, clear response formatting and crucially performing data validation before analysis. Furthermore, to enhance the assistant’s ability to understand and discuss football-specific terminology, relevant documentation explaining KPIs used by PlaymakerAI was made available to the assistant.   This was achieved by uploading KPI definition documents as files accessible via the assistant’s built-in file search/retrieval capabilities [1], rather than relying on a separate vector database for the assistant itself.   This allows the assistant to answer user questions like "What is xG?" and provide richer context when analyzing datasets containing these KPIs. The reason for not uploading all files (all the datasets) directly to OpenAI is that it would be too costly due to token lengths and vector-based maintenance storage costs.   Another reason for using the OpenAI vector database was that PlaymakerAI already had them uploaded, and they had other assistants dependent on that data. This allows for simpler maintenance of the system if KPIs only need to be maintained at one place.  4.5.2   Data Validation within Assistant (LLM-as-a-judge)  A significant challenge in RAG systems is ensuring the LLM does not hallucinate or misuse retrieved data that may not perfectly match the user’s original request.   To mitigate this, a data validation step is performed by the assistant itself.   Through the use of specific instructions embedded within the context prompt generated before calling   ask_assistant .   This approach avoids the complexity of implementing a separate "judge" model while utilizing the reference-guided grading. The authors recorded that using

=== 페이지 55 ===

Implementation | 39  LLM-as-a-judge can have an agreement rate of 80% with experts, showing the versatility and efficiency [24]. When an analysis query is processed, the context prompt includes not only the retrieved dataset content but also explicit metadata: the user’s originally requested team(s) and league/cup, alongside the name and team information associated with the retrieved dataset. The system instructions then direct the assistant to perform a verification step before proceeding with any analysis or function call:  •   Compare Request vs. Data:   The assistant must compare the requested entities (team, league/cup) with the metadata of the provided dataset.  •   Handle Mismatches:   Based on the comparison, the assistant follows specific logic:  – Full Match:   If the retrieved dataset corresponds well to the request (correct team and league/cup), proceed with analysis and function calling, stating the dataset used.  – Partial Match (League/Cup Mismatch):   If the correct team is found but the dataset pertains to a different league/cup than requested, the assistant must inform the user of the discrepancy (e.g., "Found data for Team X in League Y, not the requested League Z. Analyzing available data...") before proceeding with the analysis using the available data.  – Team Mismatch / No Match:   If the retrieved dataset’s team does not match the requested team, the assistant is instructed not to perform the analysis or call any function.   Instead, it should inform the user that the correct data could not be found and suggest clarifying the request or checking data availability on the PlaymakerAI platform. This instruction-guided validation by the assistant serves as a fence, reducing the risk of generating misleading analyses based on irrelevant data and improving the overall trustworthiness of the chatbot’s responses.  4.5.3   Prompt for Opponent/Match Analysis  Both prompts lean on CoT principles to get more advanced reasoning from an LLM, which is key when it’s judging data found by the RAG component. The opponent analysis prompt is a few-shot CoT type, as Wei et al. (2022)

=== 페이지 56 ===

40 | Implementation  show, providing just a few chain-of-thought demonstrations as examples can significantly improve performance on complex reasoning tasks. This is evident in the examples for spotting team name differences [40]. For example, in the case where a user writes a team abbreviation, but the database only has full names.   This helps the LLM handle those mismatches better.   The prompt also decomposes the task into smaller steps, like checking the team, then the league, a known CoT strategy for tackling complex problems [41, 40]. This step-by-step flow, guided by textual instructions within the prompt, also aims for clearer, more interpretable decision-making [41, 40]. Yu et al. (2023) talk about different task types. When the LLM is judging if the right team and league data was found, especially if it has to leverage its pre- trained knowledge to generalize beyond the provided exemplars for obscure or highly variable team names, that specific judgment step leans towards Open Domain Reasoning and QA as described by Yu et al. (2023) [40]. When the assistant is told to analyze the data snippet attached, then it’s a clear Closed Domain Reasoning and QA task because all the info it needs for that specific analysis is right there in the prompt [41, 40]. The match analysis prompt, while less complex in its outcomes than the opponent analysis, still relies on CoT principles.   It employs few-shot exemplars to define the logic for team presence verification and uses step-by- step textual instructions to guide the LLM to a binary outcome (match found or mismatch). This structured approach, even for a simpler task, ensures the reasoning is methodical and aligned with CoT best practices [41, 40].  4.5.4   Function Calling for Dashboard Generation  To fulfill user requests for specific analyses, the system leverages the OpenAI assistant’s function calling.   The function for creating an opponent analysis (Appendix 3.4.1) and match analysis (Appendix 3.4.2) can be found in the appendix. The interaction flow is as follows: 1. The assistant, based on the user’s query, the provided context (including validated RAG data, if applicable) and its instructions, determines if an analysis dashboard should be generated. 2. If a function call is deemed necessary, the assistant generates the function   name   (e.g.,   create_match_analysis )   and   the   required arguments (e.g., team names, dataset ID) in JSON format. 3. The OpenAI API returns a ‘requires_action‘ status to the backend application.

=== 페이지 57 ===

Implementation | 41  4. The   execute_function_call   function in   updated_langchain.py  parses the function name and arguments provided by the assistant. 5. It   then   invokes   the   corresponding   Python   function ( create_match_analysis   or   create_opponent_analysis ). 6. These Python functions execute their logic (generating the dashboard JSON)   and   return   a   result   (e.g.,   a   success   message)   back   to  execute_function_call . 7. The backend submits this result back to the OpenAI assistant API against the specific tool call ID. 8. The assistant incorporates the function’s result into its context and generates   the   final   natural   language   response   to   the   user,   often confirming the dashboard creation and providing any requested textual analysis based on the RAG data and the function outcome. This mechanism effectively connects the LLM’s reasoning and decision- making capabilities with the specific backend functionalities required to deliver the core value proposition of the chatbot.  4.5.5   Handling General Questions  Queries classified as   "general_question"   by the initial processing step (Section 4.3.1) follow a simplified pathway within the orchestration logic. Recognizing that these queries do not require specific dataset retrieval or dashboard generation, the system bypasses the RAG component, data preparation, data validation and function calling mechanisms entirely. This route is taken when a user is asking a follow-up question on a previous answer. The user’s original query is sent directly to the OpenAI assistant via the   ask_assistant   function, typically accompanied only by the standard system instructions.   The assistant then leverages its general knowledge base (or the uploaded KPI documentation files, if relevant to the question) to formulate an answer.   If the query falls outside its capabilities or the scope defined by the system prompt (e.g., asking about non-football topics), the assistant is expected to state that it cannot fulfill the request, as observed during testing.   This distinct handling streamlines processing for non-analytical queries, conserving computational resources and potentially reducing response latency.

=== 페이지 58 ===

42 | Implementation  4.6   Implementation Summary  The   Implementation   chapter   describes   a   privacy-preserving,   end-to-end pipeline that turns PlaymakerAI’s raw football data into actionable answers and dashboards.   First, a Python pipeline downloads each dataset, rewrites its metadata into a single English sentence, and stores both that sentence’s embedding (E5-large-instruct, L2-normalised) and the full JSON in a local Chroma vector store. When a user submits a query, GPT-4o-mini classifies it as match, opponent or general, then uses RapidFuzz to normalise team- league names against a curated map, prioritising the team if the league is uncertain. For analytical queries, a hybrid retriever performs semantic search (K = 3) and falls back to weighted BM25 when top-hit similarity drops below 0.75; the winning JSON is pruned, headers are removed and non-matching teams filtered with QRatio   ≥   60 to save tokens while keeping ambiguous rows for the LLM to judge.   The cleaned slice, the user’s intent and KPI glossaries are handed to an OpenAI Assistant whose system prompt enforces neutrality, source fidelity and an LLM-as-judge step: it must verify that the retrieved dataset’s team and competition align with the request, warn the user if only partly correct, and abort if the team is wrong. On a full or acceptable match, it calls backend functions (e.g.,   create_match_analysis ) that build the dashboard and return the insights. General questions bypass retrieval and are answered from the assistant’s own knowledge plus the uploaded KPI files. Key design choices: local storage for modularity, benchmark-driven model selection, CoT prompts and strict validation, which aim to balance speed, accuracy, trustworthiness and token usage.

=== 페이지 59 ===

Results and Analysis | 43  Chapter 5 Results and Analysis  Initial tests were performed to test the RAG component. Followed by tests to compare different embedding models to see the difference in RAG component. Afterwards, tests of different embedding techniques were done to see the impact of preprocessing the data. Then, a more in-depth integration test was conducted to test the full pipeline. Tests on the rapidFuzz component used to correct invalid user requests were performed. Lastly, UATs were held with experts and novice users to get their feedback on the system.   The system is evaluated using several quality dimensions: retrieval accuracy, response latency, usability, system precision, and handling of ambiguous inputs.  5.1   Evaluation Setup and Metrics  A challenging aspect of developing Chatbot systems is effectively measuring their performance.   For this project, a strategy involving both independent component testing, larger integration tests was adopted and lastly UATs. To analyze the performance of key components, specific benchmarks were developed. As mentioned previously, the RAG component’s ability to fetch the correct dataset in response to a user query was rigorously tested. This involved benchmarks for embedding model selection and the efficiency of the query cleaning and normalization pipeline, utilizing the synthetically generated and pre-processed query sets.  5.1.1   Component vs. System Testing  The intended plan was to first ensure that each component functioned reliably and as expected. Once this was confirmed, the complete system was tested as

=== 페이지 60 ===

44 | Results and Analysis  a whole.  5.1.2   Evaluation Metrics Overview  To measure retrieval performance, several metrics were employed, including average computation time, Recall@k, NDCG and MRR. Both MRR and Recall@k were selected due to their suitability for evaluating ranked retrieval systems, where the order of results and overall accuracy are primary. In order to perform the integration test, a set of queries was synthetically generated with the input and knowledge of the people from PlaymakerAI. A few different models were used to generate the queries, such as GPT o3, GPT 4.5 Mini-High and Gemini 2.5 Advanced. This was done to combat the bias of one model. This becomes a qualitative approach for testing the system, though time-consuming, it generates good and clear answers on what is working and what is not working as expected. The answers will be checked manually by the author. Once acceptable results have been achieved, a final integration test, UAT, will be held with experts at PlaymakerAI and users, where they will try to complete seven tasks, and the results will be measured.  5.1.3   F1, MRR, NDCG, Recall@K  F1 Score   represents the harmonic between precision and recall, and it measures how well a model balances these two. Precision is the number of true positives divided by all predicted positives. The F1 score is calculated as:  F 1   = 2   ×   Precision   ×   Recall Precision   +   Recall This score ranges from 0 to 1, where higher values indicate better balance between precision and recall [42].  MRR   offers a nuanced assessment of how highly the first correct or most relevant dataset is ranked. For each query in the test set, the Reciprocal Rank (RR) is the multiplicative inverse of the rank of the first correctly identified dataset. If the target dataset is the first item retrieved, the RR is   1 / 1 = 1 ; if it is the second, the RR is   1 / 2 = 0 . 5 ; if it is the third one, the RR is   1 / 3 = 0 . 33 . If the correct dataset is not found within the top K retrieved items, the RR for that query is 0. The MRR is then calculated as the average of these individual

=== 페이지 61 ===

Results and Analysis | 45  RRs across all queries in the evaluation set. The mathematical formulation is:  M RR   =   1  | N   |  | N   |  X  i =1  1  rank i  where:  •   | N   |   represents the total number of queries in the test set.  •   rank i   denotes the rank of the first relevant dataset retrieved for the   i -th query. A higher MRR, approaching 1, means that the retrieval system consistently ranks the correct dataset at or near the top of its output. This is particularly important for the RAG system, where only the first document will be used and therefore directly impacts the quality and relevance of the final generated response.   The MRR scores presented in Table 5.1, for example, provided quantitative evidence for selecting the   intfloat/multilingual-e5-large -instruct   embedding model and informed the configuration of the hybrid retrieval strategy.   The average time for computation was also recorded to ensure the selected methods were efficient enough for an interactive chatbot application [43].  NDCG@ k   tells not only whether the correct datasets appear in the top- k  results but also how well they are ordered. Each hit is given a relevance grade (for example, “perfect,” “useful,” or “irrelevant”), and hits near the top count more than those further down. The score is then normalised so that 1.0 means a perfect, most-relevant-first ranking, while 0 means the ranking is no better than random. In practice, NDCG@5 lets us see whether the retriever keeps “almost right” alternatives directly under the best match [44].  Recall@k   measures the proportion of queries for which the correct dataset is found within the top   k   retrieved results.   High recall is critical in RAG systems, as failing to retrieve relevant information prevents the LLM from utilizing it, potentially leading to less accurate or incomplete responses [43].  5.2   Component-Level Evaluation  Initial tests were conducted to optimize and benchmark two key components of the system, the RAG component and the RapidFuzz component.

=== 페이지 62 ===

46 | Results and Analysis  5.2.1   RAG Benchmarks  Firstly tests were performed to find the most suited embedding model as discussed (Section 4.4.1).   The benchmark results presented in Ta- ble   5.1   clearly   illustrate   the   performance   trade-offs.   While   the   large  infly/inf-retriever-v1-1.5b ∗   model achieved the highest MRR (0.89 on cleaned queries), its average query time exceeded 30 seconds, rendering it unsuitable for an interactive chatbot application on the target hardware. The small   all-MiniLM-L6-v2   model was fast but resulted in significantly lower MRR scores, indicating worse relevance ranking.   The   intfloat /multilingual-e5-large-instruct   model emerged as the optimal choice, delivering excellent MRR (0.886 on cleaned queries) comparable to the much larger model, but with an average query time under 0.1 seconds, thus meeting both accuracy and performance requirements within the hardware constraints. The table also confirms the significant positive impact of query cleaning (Section 4.3.2) on retrieval performance across all tested models.  ∗ https://huggingface.co/infly/inf-retriever-v1-1.5b

=== 페이지 63 ===

Results and Analysis | 47  Table 5.1: Embedding Model Benchmark Results for Dataset Retrieval (K=5) Model   Query Condition   Recall@5   MRR   Avg Time (s)   Std MRR   Std Time (s)  Snowflake/snowflake-arctic-embed-l-v2.0 (568M)  Not Cleaned   0 . 837   0 . 567   0 . 157 9   0 . 367   0 . 212 9  Explicit League/Cup   0 . 965   0 . 686   0 . 153 4   0 . 306   0 . 192 4  Fully Cleaned   0 . 988   0 . 857   0 . 103 3   0 . 246   0 . 103 8  infly/inf-retriever-v1-1.5b (7.07B)  Not Cleaned   0 . 872   0 . 673   33 . 121 3   0 . 380   5 . 681 2  Explicit League/Cup   0 . 988   0 . 843   32 . 490 8   0 . 262   5 . 653 3  Fully Cleaned   1 . 000   0 . 890   32 . 109 3   0 . 209   4 . 663 2  intfloat/multilingual-e5-large-instruct (560M)  Not Cleaned   0 . 791   0 . 632   0 . 087 4   0 . 422   0 . 119 1  Explicit League/Cup   1 . 000   0 . 883   0 . 074 8   0 . 231   0 . 061 6  Fully Cleaned   1 . 000   0 . 886   0 . 052 9   0 . 218   0 . 054 3  sentence-transformers/all-MiniLM-L6-v2 (22.7M)  Not Cleaned   0 . 523   0 . 348   0 . 048 9   0 . 409   0 . 043 2  Explicit League/Cup   0 . 709   0 . 465   0 . 052 4   0 . 403   0 . 052 8  Fully Cleaned   1 . 000   0 . 784   0 . 040 9   0 . 282   0 . 086 9  sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (118M)  Not Cleaned   0 . 802   0 . 533   0 . 055 8   0 . 389   0 . 042 2  Explicit League/Cup   0 . 919   0 . 690   0 . 052 5   0 . 357   0 . 040 6  Fully Cleaned   0 . 930   0 . 750   0 . 049 1   0 . 339   0 . 043 7  sentence-transformers/all-mpnet-base-v2 (109M)  Not Cleaned   0 . 628   0 . 429   0 . 070 3   0 . 411   0 . 064 1  Explicit League/Cup   0 . 767   0 . 556   0 . 067 2   0 . 402   0 . 056 5  Fully Cleaned   0 . 965   0 . 697   0 . 056 0   0 . 322   0 . 045 0  ’Not Cleaned’: original queries. ’Explicit League/Cup’: added tournament. ’Fully Cleaned’: only team + tournament. Std MRR: standard deviation of MRR. Std Time: standard deviation of query time. K=5 used for all benchmarks.  After   intfloat/multilingual-e5-large-instruct   was selected more tests were performed to further optimize the RAG component.   With the new added functionality since the initial tests (Section 5.3.1), the component was tested in isolation using 534 queries drawn from the leagues, cups and individual fixtures already stored in the vector database. In this first “raw” run no spell-checking or rewriting was applied to the user query before sending it to the embedding model. To reflect realistic usage, most questions targeted season-long datasets from the seven main competitions we cover in depth are Allsvenskan, Premier League, Serie A, La Liga, Bundesliga, the UEFA Champions League and the Europa League. About six percent of the set was for matches (e.g., Fiorentina

=== 페이지 64 ===

48 | Results and Analysis  – Napoli or Wolves vs Manchester City), while another ten percent was from other big leagues such as Ligue 1, Eredivisie, Primeira Liga and the English Championship. Roughly half of the queries were written in English and the rest in Swedish. For the benchmark, the top   k   = 5   retrieved items were kept for each of the 534 queries, resulting in a total of   534 × 5 = 2 670   rows to evaluate. Relevance was judged on a three-point scale: (1)   irrelevant   – wrong team or competition; (2)   partially correct   – right team but wrong competition (e.g. “Brentford in FA Cup” returns a Premier League dataset); (3)   perfect   – correct competition or both requested match-teams present. The first hundred rows were graded by hand.   Those decisions were converted into explicit rules, passed to an LLM and the model’s output was then spot-checked. The only systematic fix required was to downgrade women’s league hits that the model had marked as “partially correct” but should have been marked as "irrelevant". Two ranking-sensitive metrics were used. MRR counts a query as correct if any of its five hits scores   ≥   2 .   For NDCG@ k   the grades are remapped (1 becomes 0, 2 becomes 1, 3 becomes 2) so that irrelevant results add no gain. English queries reached   MRR 0.82   versus   0.77   for Swedish, showing a difference in handling the languages. For the raw (uncleaned) queries, there were 1534 Grade 1 rows, 577 Grade 2 rows, and 559 Grade 3 rows, giving 2670 total queries (534   ×   5).   The retrieval metrics for these queries are shown in Table 5.2. Table 5.2: Retrieval metrics for raw (uncleaned) queries Metric   Value MRR   0.796 NDCG@5   0.817 NDCG@3   0.807 For the pre-processed queries (using the same prompt template as production), there were 1358 Grade 1 (irrelevant) rows, 723 Grade 2 (partly correct) rows,   and 589 Grade 3 (perfect) rows.   Table 5.3 reports the corresponding retrieval metrics.

=== 페이지 65 ===

Results and Analysis | 49  Table 5.3: Retrieval metrics for pre-processed queries Metric   Value MRR   0.919 NDCG@5   0.903 NDCG@3   0.913 Normalising each query to the exact template used by the production system lifts MRR by +0.12 and NDCG@5 by +0.09, confirming the benefit seen in earlier pilot tests.  5.2.2   RapidFuzz Benchmarks  Following the initial user query classification by the LLM (Section 4.3.1), the   tool_find_dataset   function employs fuzzy string matching via the RapidFuzz library to map extracted team and league entities to canonical entries within the   TEAM_LEAGUE_MAP . To optimize this critical matching step,   a   systematic   benchmark   was   conducted   to   determine   the   most effective RapidFuzz scorer and confidence thresholds ( team_threshold   and  league_threshold ). The main objective of this benchmark was to identify a configuration that maximizes the correct identification of known teams and their valid leagues (True Positives) while concurrently minimizing False Positives (FPT for teams, FPL for leagues), particularly the incorrect matching of out-of- scope entities or invalid team-league combinations. The evaluation utilized the pre-processed dataset of 203 queries, where each query’s LLM-extracted  user_team   and   user_league   served as input to the fuzzy matching logic. Six distinct RapidFuzz scorers were evaluated:   Ratio ,   PartialRatio ,  TokenSetRatio ,   TokenSortRatio ,   WRatio   and   QRatio .   For each scorer, performance was assessed across a grid of   team_threshold   values (ranging from 80% to 100% in increments of 2). The performance was quantified using standard classification metrics, focusing on Team F1 score, Team Specificity (to address the concern of FPT) and conditional League F1 score. Analysis of these results ( Table 5.1) indicated that the   QRatio, Ratio and TokenRatio   scorer demonstrated marginally superior or comparable League F1, particularly in handling variations typical of entity names. Moreover, the Team F1 score was the same for all entries, an explanation for this could be the fact that the user input is very scaled down only to a word (the team name), therefore, the different scorers perform similar results.   Overall, the results

=== 페이지 66 ===

50 | Results and Analysis  show minor differences between the scorers and the different thresholds. As seen in (Figure   5.2) which highlights that changing threshold values yields only marginal differences in Team F1 performance, typically around 0.005%, improving the robustness of the scorer slightly.  Figure 5.1:   Max Team and League F1 Scores Across Different String Matching Scorers Originally, the QRatio scorer was evaluated across a wider range of threshold combinations, with team and league thresholds ranging from 70 to 100. To improve clarity, the heatmap in Figure 5.2 has been simplified to show only a subset of these results.

=== 페이지 67 ===

Results and Analysis | 51  Figure 5.2: Team F1 Scores Across QRatio Threshold Settings  5.3   Pipeline and Integration Evaluation  Another approach to benchmarking was to evaluate the full pipeline. This was done twice. Initially, at the start of the development process, and again at the end, after parameters, components, and functionality had been optimized.  5.3.1   Initial Integration Tests  The first integration test was performed after the initial implementation of all components and assembly of the complete pipeline. The test was run to identify areas requiring improvements, specifically in the RAG system. The results of this test are presented below. Afterwards, an iterative process was undertaken to refine each component and the overall pipeline.

=== 페이지 68 ===

52 | Results and Analysis  Table 5.4: Integration tests before using the correcting dictionary  Outcome   Count   Percentage  Correct   39   70.9% Wrong (Total)   16   29.1%  — No data available, but wrong data used   (6)   (37.5%) — Right team, wrong dataset (league/cup)   ( 10)   (62.5%)  Total Queries   55   100.0% The breakdown of the 16 incorrect results is as follows:  •   No Coverage / Edge Cases (6 queries):   These involved requests for data from leagues/cups where the system lacks coverage. An example is querying Tottenham in the Champions League for a season they did not participate in. These reflect realistic user queries, but are outside the current data scope.  •   Wrong Dataset Retrieved (10 queries):   For these queries,   the system correctly identified the user’s need (e.g., team and competition) but failed to retrieve the precise dataset.   Instead,   a related but incorrect dataset was used (e.g., retrieving PSG’s Ligue 1 data for a Champions League query). This points towards challenges in dataset disambiguation. It was after this that it was decided to implement the rapidFuzz and query cleaning (Section 4.3.2) functionality to mitigate the faulty behavior and implement the LLM-as-a-judge (Section 4.5.2) for handling the cases where the wrong dataset had been fetched.  5.3.2   Full Integration Test  Once the mitigation was implemented based on the findings of the initial test,   it was decided to perform an integration test.   Compared to the initial tests (Section 5.3.1), the full pipeline was tested, including all the components and not only the RAG component. To evaluate the system, tests in   test_playmaker.py   generate detailed logs that were reviewed thoroughly. This involved processing 44 mixed queries designed to test performance on tasks where data was available in the vector database, as well as edge cases involving missing data, out-of-scope requests and future matches. The analysis classifies each outcome using the framework defined previously (Scenario 1-4

=== 페이지 69 ===

Results and Analysis | 53  mapping to True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN)) based on whether the query should be possible and whether the system executed it properly.  5.3.2.1   System Metrics  Based on the evaluation of the 44 test queries, the system’s end-to-end performance was measured.   Accuracy : (TP + TN) / Total = (28 + 6) / 44 = 77.2727%. This shows the system provided the correct handling for just over three-quarters of the test queries.   F1 Score : Calculated at 84.9%. This high score indicates a strong overall performance, effectively balancing the system’s ability to correctly execute possible queries and avoid errors.   Precision : TP / (TP + FP) = 28 / 34 = 82.3529%. When the system did attempt an analysis or provide a direct answer, it was the correct course of action about 82% of the time.   Recall   (Sensitivity): TP / (TP + FN) = 28 / 32 = 87.5%. The system successfully handled queries that were possible and within scope.   Specificity : TN / (TN + FP) = 6 / 12 = 50.0%. This metric highlights a key weakness: the system correctly identified and appropriately refused to execute impossible or invalid queries only half of the time. The other half resulted in False Positives. Table 5.5: Overall System Performance Metrics (N=44 Queries) Metric   Value True Positives   28 True Negatives   6 False Positives   6 False Negatives   4 Total Queries   44 Accuracy   77.3% Precision   82.4% Recall   87.5% F1 Score   84.9% Specificity   50.0%  5.3.2.2   Error Types and System Behavior  Examining the logs and notes for specific queries revealed the following patterns.

=== 페이지 70 ===

54 | Results and Analysis  Query Identification:   The initial classification step was mostly marked as correct in the results table.   However, the few cases marked false (like "Summarize AC Milan’s season" or "Real Madrid perform in the Champions League?") directly led to False Negatives (FN) because the system treated them as general questions instead of opponent analyses. An entity extraction issue was also noted for the Man City UCL query (FN).  Data   Retrieval   &   Usage:   For   most   valid   queries   (TPs),   the   system successfully located and utilized the appropriate dataset from the vector database, contributing to the high Recall. Issues arose when the wrong data was retrieved. Sometimes this involved related data (e.g., finding a specific match dataset when opponent analysis was requested for Real Madrid or Inter Milan - FN cases where the assistant noted the mismatch but could not fulfill the original intent). Other times, it involved finding data for the correct teams but the wrong competition (e.g., using Serie A data for the Coppa Italia query - FP). The note indicating competition data isn’t always present in the source data, which likely contributes here.   In several False Positive cases, completely unrelated data was retrieved when the correct data was missing (e.g., analyzing Wolves/Man City data for the impossible Brentford/Fulham 2025 query). Showing that the LLM-as-a-judge is not perfect.  Handling of Impossible/Invalid Queries:   This was inconsistent (Speci- ficity 50%).  Correct Handling (TN):   The system often correctly identified out-of-scope requests (Paris FC/Lyon Cup, Bodø/Glimt), queries with unavailable data where it found unrelated info but the LLM-as-a-judge functionality of the assistant "gracefully turned it down" (Östers/GAIS), or future matches where the year mismatch was caught (Bayer/BVB). Gibberish input was also handled well.  Incorrect Handling (FP):   In numerous cases, the system attempted analysis despite the impossibility.   This included analyzing a match from the wrong competition because the teams existed (Elfsborg/Degerfors), trying to analyze a team not in the relevant dataset (Luton Town), analyzing some game when the requested one did not exist (Barca/RM 2023, Barca/RM no year), or inappropriately confirming "successful creation" of an analysis based on clearly wrong data (Brentford/Fulham).   This indicates that the assistant’s ability to handle or detect mismatched instructions shows some limitations.

=== 페이지 71 ===

Results and Analysis | 55  Response Quality & Failures:   When the data was correct, instructions like providing a textual analysis followed.   Some simple queries failed unexpectedly (e.g., "Assistant was not able to answer" the Arsenal Manager question - FN). Vague queries ("Tell me about football.") were handled appropriately by asking for clarification.  5.3.3   Summary of Results  The chatbot performs well across multiple dimensions of quality. It retrieves the correct data with high precision (MRR = 0.92), responds quickly enough for real-time use (avg. 17s), and was rated highly for usability by both novice and expert users (76/100 BUS-13). Errors were mainly linked to ambiguous queries or dataset mismatches, which are expected in systems relying on retrieval and LLM judgment.  5.4   User Evaluation and Acceptance  The UAT (Appendix F) will be performed with distinct user groups, expert evaluation and the collection of specific performance metrics.   While methodologies like MRR, Recall@K and F1 are common in evaluating retrieval components of information systems, the primary focus for this user evaluation will be on usability and task completion from the end-user’s perspective. The tests were performed in order to tweak certain parts of the system before finalizing the system. But most importantly, these tests will be used to serve as data for the user satisfaction with the system.  5.4.1   User Acceptance Test Design  The test will target two distinct user groups:   expert users, sourced from PlaymakerAI, users with a little football knowledge and novice users with little to no football knowledge, representing the general audience anticipated for the chatbot. This distinction is crucial as it allows for testing the system with users possessing varied levels of knowledge in football analytics, thereby capturing a broader usage of user experiences and needs.   The objectives of using these experts are twofold: to rigorously test the chatbot’s functionality and accuracy and to evaluate its suitability and potential value for their customers or professional use cases. The participant sample for the UAT is 8 individuals, with a deliberate selection to ensure a majority of novice users (in terms of football analysis

=== 페이지 72 ===

56 | Results and Analysis  expertise) and a minority of experts in the field.   While Jakob Nielsen’s well-known heuristic suggests that five users are often sufficient to identify approximately 85% of usability problems in traditional interfaces [45], research specific to chatbots indicates that this number might vary.   For instance, Holmes et al. (2019) found that a larger number of users (26 in their case) might be needed to identify the majority of chatbot-specific usability errors [46].   This suggests that while an initial group can provide valuable insights, flexibility in participant numbers or iterative testing rounds might be beneficial if comprehensive error detection is paramount. For this project, the target of 8 users aims to gather richer feedback than the minimal heuristic, especially given the two distinct user groups, but not as many as Holmes et al suggest. Prior to the main task-based session, participants will complete a brief pre-session questionnaire.   Drawing inspiration from usability and chatbot research (e.g., questionnaire and BUS-15 survey found in [31] for assessing user context), will include 9 questions designed to capture demographic information, self-reported football knowledge (e.g., "How would you rate your football knowledge?") and prior experience with chatbots (e.g., "How often do you use chatbots?"). Moreover it this will also include a question where the users get to rate their knowledge of football. This initial data collection is important for contextualizing subsequent results and feedback, as prior experience and domain knowledge can significantly influence user perception and interaction patterns. This aligns with findings highlighted by Chen and Wang (2018) [47] and noted in the review by Ren et al. (2022) [48]. Similarly, Venkatesh et al. (2018) considered "Frequent Users" in their CUX evaluation, underscoring the importance of user background [49]. The   core   of   the   UAT   will   involve   participants   completing   seven representative tasks(Appendix F). These tasks are structured as scenarios, placing users in specific situations to interact with the chatbot. The number of tasks will be determined based on complexity and scope, with literature reviewed by Ren et al. (2022) suggesting a flexible range is common in chatbot evaluations [48].   During task execution, the think-aloud protocol will be employed. This method is widely used for capturing in-depth qualitative user insights during interaction, allowing for a deeper understanding of the user’s cognitive processes, expectations and any points of friction encountered [48]. The evaluation sessions will be conducted on a dedicated computer, with the evaluator present to observe and facilitate. Before commencing the tasks, a brief, standardized introduction to the chatbot system and the specific context or persona users should adopt will be provided. This ensures participants have

=== 페이지 73 ===

Results and Analysis | 57  a foundational understanding of the system’s intended use and helps simulate more realistic usage scenarios, a practice consistent with experimental setups where tutorials or familiarization periods are provided (Ren et al., 2022 [48]). Throughout the tasks, minimal guidance will be offered, with intervention only for instances where a user becomes significantly stuck, thereby maintaining the validity of the interaction. The tasks themselves will be provided to users in clear, written form to minimize the need for evaluator intervention and ensure consistency across sessions [48]. During these task-based sessions, several quantitative metrics will be recorded: the occurrence of errors, the time taken to complete each task and the number of messages exchanged to achieve the task goal.   Furthermore, task success (defined as binary: completed or not completed) will be logged. These metrics align with common measures for assessing effectiveness and efficiency, which are core concepts from ISO 9241-11 [50, 48]. Additionally, chatbot interaction logs will be reviewed post-session for supplementary data, a common practice for analyzing conversational patterns and errors.   The importance of analyzing conversation turns and error rates is also highlighted by Venkatesh et al. (2018) [49]. Following the interactive task session, participants will complete a concise post-session questionnaire based on an adapted version of the BUS-15. The BUS-15 was selected due to its specific design for evaluating user experience with chatbots, as detailed by Borsci et al. (2021) [31]. The first two questions from BUS-15 were removed because they were focusing on the UX of the chatbot, which is out of scope for this thesis. This survey aims to capture the user’s perceived experience through structured items, assessing aspects such as "conversational efficiency, quality of the functionality and accessibility" [31].   To complement the structured responses from the adapted BUS- 15, the questionnaire will conclude with an open-ended question, allowing users to provide free-text feedback.   This mixed-method approach to data collection ensures a mix of quantitative metrics from task performance and the questionnaire, alongside qualitative insights from the think-aloud protocol and open-ended feedback. The value of combining quantitative and qualitative data for a holistic view is well-supported, with Ren et al. (2022) noting that 92.9% of the experiments they reviewed adopted questionnaires and many also utilized interviews or analyzed qualitative content [48]. Finally, to refine the entire evaluation protocol, pilot tests will be conducted with three individuals. This step is important for fine-tuning task instructions, questionnaire wording and the overall session flow, as well as providing practice for the evaluator before the main data collection phase. This aligns

=== 페이지 74 ===

58 | Results and Analysis  with general good practice in empirical research to ensure the robustness and smooth execution of the study design.  Metric   Value  Total messages analysed   81 Average latency (User   →   Assistant Answer)   17.3 s Standard deviation   8.4 s Fastest assistant response   6.0 s Slowest assistant response   41.2 s Table 5.6: Latency overview across UAT conversations.  5.4.2   BUS-13 Survey Results  The BUS-15 was shortened because two of the questions regarded the UX of the chatbot, which is out of scope for this project. Herby BUS-15 is referenced as BUS-13.   The shortened BUS-13 produced an overall usability score of 76/100.   Each item was first recoded from the 1–5 Likert scale to 0–4, the 13 values were summed (range 0–52) and the total was linearly rescaled to a 0–100 index, mirroring the BUS-15 paper procedure. Two statements stood out positively: “Communicating with the chatbot was clear” and “The chatbot gives me the appropriate amount of information” (both M = 4.62). The lowest rating concerned “The chatbot informs me of any possible privacy issues” (M = 3.38), suggesting a concrete area for improvement. Overall, users perceive the chatbot as easy to talk to and informative, while transparency around privacy needs strengthening.

=== 페이지 75 ===

Results and Analysis | 59  Table 5.7: Item-level BUS-13 ratings for the football chatbot ( n   = 8 )  Statement   Mean   Std  Communicating with the chatbot was clear   4.62   0.52 I was immediately made aware of what informa- tion the chatbot can give me 3.50   0.53 The interaction with the chatbot felt like an ongoing conversation 4.00   0.00 The chatbot was able to keep track of context   4.25   0.71 The chatbot was able to make references to the website or service when appropriate 3.38   1.06 The chatbot could handle situations in which the line of conversation was not clear 3.75   1.04 The chatbot’s responses were easy to understand   4.50   0.76 I find that the chatbot understands what I want and helps me achieve my goal 4.50   0.53 The chatbot gives me the appropriate amount of information 4.62   0.52 The chatbot only gives me the information I need   3.88   0.64 I feel like the chatbot’s responses were accurate   4.00   0.76 I believe the chatbot informs me of any possible privacy issues 3.38   1.19 My waiting time for a response from the chatbot was short 4.38   1.06 Users who rated themselves as knowing football well score the chatbot about 4 points higher on the 0–100 BUS scale. But with only eight testers, the gap is not statistically conclusive, but the pattern hints that novice users score it lower. The difference also hints that users with lower football knowledge were more impressed (averaged 5) by the speed of the answer than the experts (averaged 4). Table 5.8: BUS-13 usability by self-rated football knowledge  Group   n   Mean (0–100)   Std   Take-away  Higher knowledge ( ≥   3 )   5   78.1   7.3   Demanding & satisfied Lower knowledge (1–2)   3   73.7   8.7   Good but less positive The two lowest-scoring items were:   "The chatbot was able to make references to the website or service when appropriate" and "I believe the

=== 페이지 76 ===

60 | Results and Analysis  chatbot informs me of any possible privacy issues." These features are not fully implemented yet, especially the ability to reference the platform. The privacy aspect is also less relevant in this case.   When these two items are excluded, the overall usability score increases from 76 to 80 out of 100. This shifts the system from a solid mid-70s rating to a better usability range, mainly due to the removal of the lowest-rated statements.  5.4.3   Qualitative Feedback and Improvements  One expert said that some answers were too long and had too much explanation. The expert also pointed out that opponent analysis should include both what the opponent is good at and where they are weak.   The expert liked how the KPIs were picked based on player positions, but suggested some changes for defenders. The expert gave specific KPIs for each position, which were added to the system. The answers were shortened by updating the prompt to keep them more concise. A second expert gave more overall positive feedback. The second expert said it’s a very useful feature and could be great for journalists to quickly summarize games. The second expert mentioned at least once per task that the answers were really good. The second expert also said the game analysis itself was solid and made sense. Twice, users asked for clarification, and multiple times, users got intrigued and asked follow-up questions to see the capabilities or get more insights. Overall, the novice testers liked the chatbot’s speed, saying it felt “fast,” “chatty,” and easy to follow.   They praised the detailed dashboards and tables.   One said they liked that position-specific KPIs helped them picture real scouting tasks.   Still, many struggled with basic concepts like xG and possession, and found terms like “mixed performance” too vague.  5.4.4   Insights from UAT Answers  The results from (Section 5.4.1) were carefully gone through in order to determine the quality of the answers. The UAT consisted of seven tasks that the users were asked to complete; all of the users were able to complete the tasks on the first try. But in a few cases, users asked for clarification, and multiple times, users got intrigued and asked follow-up questions to see the capabilities or get more insights. Moreover, it became clear that, even when the correct dataset was provided, the assistant sometimes failed to interpret key details, especially the final score. To fix this, the code was updated to extract the final

=== 페이지 77 ===

Results and Analysis | 61  score during the data-cleaning step and manually append it to the JSON sent to the assistant. This change had a noticeable impact on the quality of the output. In the earlier version, the assistant returned an incorrect result:  - Final Score: Napoli 1 - 0 Inter Milan - Key Goal Scorer: - Philip Billing scored the only goal for Napoli. ...  To the right conclusion, which was received after updating the script.  - Goals: The match ended in a draw with a score of Inter Milan: 1 - Napoli: 1. - Key Goal Scorers: - Inter Milan: Federico Dimarco scored 1 goal. - Napoli: Philip Billing scored 1 goal. ...  5.5   System Efficiency and Latency  PlaymakerAI estimates that, without using the chatbot, for a novice user, the completion of a dashboard takes around 30 minutes, and adding additional text takes about 15 minutes more.   After having the the UAT novice users can now reduce that number down to under a minute. The average response time during the UAT was approximately 17.3 seconds, with a standard deviation of approximately 8.4 seconds. Heavy requests like loading big match data stretch times to 30–41 seconds.   The slowest run seen was the Real- Madrid Champions-League breakdown at 41 seconds.   The faster, general questions finish around 9-12 seconds. Showing huge improvements in terms of latency for completing tasks like opponent analyses and match analyses. First messages in a new session take extra time, probably because the system needs to warm up, then replies speed up once the chat is rolling and context is cached. Longer, data-heavy outputs clearly increase latency, whereas short answers stay well under ten seconds.  5.6   Comparison with Non-Domain LLM  In (Appendix G), one can go through the responses in full detail.   Below follows a summary. While both PlaymakerAI and GPT-4-turbo offer football analysis, their responses show the difference in data access.   PlaymakerAI consistently delivers concrete, data-driven insights, such as detailing Adam Vikman’s 6.53 Interceptions (Int90) and Joakim Persson’s 0.92 xG90 for Sirius, or providing the 4-1 final score and specific goal scorers (Dembele,

=== 페이지 78 ===

62 | Results and Analysis  Barcola, Ramos for PSG; Brassier for Rennes) for the Rennes-PSG match. This precision, stemming from its RAG architecture, clearly demonstrates its ability to pull and analyze current, granular football data. In contrast, GPT-4-turbo offers general definitions and analytical frame- works, repeatedly acknowledging its lack of real-time data with phrases like, "As of my last update in December 2023, I don’t have real-time data..." or "I can guide you on how to conduct a comprehensive opponent analysis..." This highlights a core limitation of large language models without live data integration.   PlaymakerAI’s capacity to interpret up-to-the-minute football data, evident in its highly specific and quantitative responses, unquestionably makes it a superior tool for detailed sports analysis.  5.7   Reliability Analysis  To test the reliability of the RAG component, four separate Chroma vector databases were created with identical embeddings, and a fixed set of 100 benchmark queries was run on each of the databases.   First, the vector databases were built using the exact same insertion order and produced identical results across all runs (Jaccard similarity = 1 for all comparisons). Then, the insertion order was randomized before adding the datasets, and the process was repeated. The results remained identical, showing no impact of insertion order on retrieval at this scale, even though previous studies have reported differences of up to 12 percentage points depending on insertion order [51]. Attempts were made to set a seed for the HNSW index to explicitly test randomness, but this was not supported by the version of LangChain’s Chroma wrapper used in this project. Although measuring output stability across different temperature values was initially considered, it was decided not to pursue it due to the large amount of manual work required to evaluate it properly.   Instead, the temperature was set to 0 for both the user intent classification model and the assistant model. This makes the system as deterministic as possible in practice, further supporting the reliability of the pipeline. Moreover, most of the pipeline is deterministic:   the index creation is deterministic because Chroma freezes the seed internally and the embeddings are deterministic because the same embedding model is always used.   This reduces the risk of variation even further. The only part that is currently non- deterministic is the data analysis phase, which could be less deterministic by fine-tuning the user intent classifier and the assistant model.

=== 페이지 79 ===

Results and Analysis | 63  5.8   Summary of Results  The system achieved strong performance across both technical benchmarks and user testing. The RAG component consistently ranked the correct dataset first in 92% of test queries (MRR = 0.92), and the overall system reached an F1 score of 85%. A user study showed that dashboard creation time was reduced from 45 minutes to under 20 seconds on average, with a usability score of 76 out of 100. Most errors were caused by incorrect matches or ambiguous queries, but the LLM-as-a-judge component handled mismatches in a majority of cases.   Qualitative feedback confirmed that both novice and expert users found the chatbot fast, useful and easy to interact with. The results show that a modular RAG + LLM system can reliably support football analysis tasks on limited hardware.

=== 페이지 80 ===

64 | Discussion  Chapter 6 Discussion  The discussion chapter is centered around the three research questions and ends with a more general discussion. The main goal is to further explain the results and potential explanations for those observations.  RQ1.   How to design and implement a domain specific chatbot?   The design of the chatbot followed a clear and modular pipeline. The system begins with fast and accurate query understanding, where user inputs are cleaned, labeled, and mapped using RapidFuzz. Relevant data is then retrieved through a RAG component. To control prompt costs, only essential data is passed to the assistant. A final safeguard is applied using the LLM-as-a-judge to verify that the correct data has been retrieved before analysis begins. Once validated, the assistant triggers structured function calls to generate dashboards for match and opponent analysis.   This architecture (Figure 3.1) keeps each module small, makes the system flexible and modular which allows for individual models to be swapped as new ones emerge.  RQ2.   How accurately can a two-layer LLM architecture retrieve the correct football dataset given a user query?   The retrieval component reached an MRR of 0.92; simply put, the right document is almost always ranked first.   For being a ranking measurement for a RAG component, this is a score representing a well-functioning system. The reasons for the high score were that it was due to many reasons, including correctly identifying the user intent to a very high degree and cleaning out noise from the query. During the project, many different embedding models were tested to see which one was most suitable for this use case.   A multilingual embedding model was a requirement in order to handle Swedish well. An instruct model

=== 페이지 81 ===

Discussion | 65  ( intfloat/multilingual-e5-large-instruct ) was the best performing, it allows the developers to prompt the model, give it context on the task and what the goal is.   This is very similar to how the model has been trained, and therefore, it can perform better using the instruction prompts. In comparison with other models that scored well in the benchmarks, we can clearly see the fundamental differences in them and how they play a role in the benchmarks (Table 5.1). One of the main reasons for   all-MiniLM-L6-v2  is because of its size. The   all-MiniLM-L6-v2   is nearly 25x smaller than the  intfloat/multilingual-e5-large-instruct   model. The smaller model would be useful if the data weren’t as similar to each other as it was for this project.   Smaller models result in worse results when it comes to handling data which are very similar to each other, this is because of the lack of layers.   More layers allow for better differentiation between data, which is very important for this project, where the data is very similar to each other. Moreover, the   Snowflake/snowflake-arctic-embed-l-v2.0 ∗   is distilled from a chatbot, which could add more chatty noise and be the reason for the lower MRR score, even though it’s larger in its parameter size. The Snowflake- Arctic-embed-l is also mostly trained on English, whereas the   intfloat /multilingual-e5-large-instruct   was specifically trained on Swedish data as well.   If reducing latency had been critical, then the MRR trade- off using the   all-MiniLM-L6-v2   would have been viable.   When manually controlling the grading of the RAG system and the 534 queries, interesting patterns arose.   Whenever the user writes in Swedish, the queries usually return worse results (because it favors Swedish leagues unreasonably high, even though the user might request La Liga or the Premier League). The model often pulls up Svenska Cupen data first. It seems to latch onto language cues before league names, so a Swedish question about the Premier League can still return Swedish records. Preliminary analysis indicates a clear pattern, which can be observed in (Section 5.2.1).  RQ3.   Does integrating function-calling improve novice users’ task– completion time and perceived usability when generating dashboards?  According to PlaymakerAI, it takes a novice user anywhere from 30 to 45 minutes to create a dashboard for an opponent analysis or match analysis. Once again, this chatbot reduces that initial hurdle for new users and also allows for PlaymakerAI to reduce the support functions for new users, allowing new users to create dashboards and analyses in less than 18 seconds on average. During the UAT, we saw that many of the users specifically said that it was fast, easy,  ∗ https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0

=== 페이지 82 ===

66 | Discussion  clean and nice. The negative feedback was from one of the experts who said that answers were sometimes too long, and they also requested changes in which types of KPIs were gonna be displayed per position. The experts also had positive feedback whilst using the chatbot: "very nice fast information", "very good answer" and "good with simple explanations". From the beginner users, they were noted saying things like: "the chatbot is very easy to use", "very nice to use", and saying they were impressed (Section 5.4.1). Giving the Chatbot the possibility to connect to the platform allows it to be better integrated and less of a free-standing widget of the platform. It’s also from the dashboard view on the platform where the most advanced analysis can be made, but getting there has been the problem for new users; this chatbot allows for a smoother transition from beginner users to more advanced users. The creation of dashboards could have been scripted in the backend, but function calls keep the flow conversational, users decide for themselves easily if they want a dashboard or not, simply by saying so, and it future-proofs the pipeline for more plugins later.  RQ4.   How does the chatbot perform against the success criteria of accuracy, token efficiency and user satisfaction?   In the UAT, the users solved every task, so functional accuracy was 100% for the test set.   Users liked the experience, as said before, beginners called the bot “very easy to use” and “impressive”, while experts praised the speed but asked for shorter answers and a few extra KPIs. Because of the modularity of the architecture and design PlaymakerAI can easily scale up the system by using larger models if needed. The vector database would then need to be repopulated. Functionality for repopulating is already part of the script.   Furthermore, the use of a wrapper such as LangChain allows for quickly changing and/or updating the embedding model and LLMs whenever newer and more football-specific ones are published. Because of the targeted users (novice), the most difficult task during the project wasn’t creating the analysis of the data. Rather, it was optimizing the RAG system, which is of huge importance to a chatbot system like this one, where there is only one true answer (dataset) and a few more are acceptable (usually like three more per team, like other cups the teams participate in, which still would give useful information).   This also sheds light on an important part of the RAG system, which is the importance of correctly differentiating data from each other. If the data is too similar to each other, then according to the nature of how embedding works, they will be too similar to each other.   This means that by better differentiating data to begin with,

=== 페이지 83 ===

Discussion | 67  the RAG component can more accurately find the data the user is requesting. Moreover, there is a difference in the two types of data, for the leagues/cups, the data are global and available for everyone. But for match analysis, that is not the case as discussed earlier. The system has not been tested with a large number of games in the vector database.   If a user added all games played across Europe, naturally the embedding model would face a much larger search space, making it harder to find the most relevant matches. In that scenario, retrieval quality would likely decrease. However, it is not expected that the user will add that many games in practice. The system only adds games manually selected by users, not every game by default. Since users typically save just a few recent games, the database remains small enough to maintain strong retrieval performance. Furthermore, it needs to be mentioned that a system like this could be built solely using one platform like OpenAI, which provides all the tools to upload documents to vector databases and use their embedding model. But this also increases the costs (compared to running most locally) and it ties software to a specific company and platform, which PlaymakerAI specifically said they did not want to.   This system design is a middle ground, utilizing some of OpenAI’s capabilities but importantly handling the RAG component locally. This also provides less risk for data governance. This allows PlaymakerAI to have full control of their data, their usages and their models. Fine-tuning both the assistant model for accurate analyses and the embedding model to better interpret football-related queries would likely enhance the chatbot significantly.   However, this was one of the tasks that could not be completed, primarily due to a lack of data.   Given that the chatbot has been in beta, PlaymakerAI does not currently have sufficient user data available.   Without real user interactions, creating a high-quality fine- tuned model becomes very challenging. Moreover, PlaymakerAI wasn’t able to provide data on what the different analyses could look like and what to focus on other than which KPIs are important per position.   If the author were to generate synthetic data myself, it wouldn’t have been effective, as the author lack the deep football analysis expertise required to produce realistic and useful examples. Fine-tuning the embedding model was less critical since the current system already performs well. Therefore, prioritizing fine-tuning the assistant model would have yielded the best improvements for analysis. However, since this chatbot was specifically designed for beginner users, the existing setup already meets these needs effectively. If PlaymakerAI decides to target more advanced users in the future, investing in fine-tuning would definitely be beneficial.

=== 페이지 84 ===

68 | Discussion  Improving the vector database with better filtering options would be beneficial, but this would require more detailed annotation of the original data.   Chroma currently has limited filtering capabilities, so exploring paid alternatives could provide more functionality and boost performance metrics, like MRR, for the RAG component if data were annotated more thoroughly to reduce the risk of fetching a game when a user’s request data for a league or cup.   Improving filtering will boost retrieval quality, but it first requires richer metadata on each record. Migrating to a self-hosted Milvus or Qdrant cluster adds JSON/hybrid filtering and still keeps costs low. Both engines ship with sharding and replication, so the switch also future-proofs scale. Finally, deciding on a compact index such as IVF-PQ instead of full HNSW shrinks storage, allowing for adding those filters without inflating disk or budget. Semantic search consistently outperformed BM25 during tests. Semantic search   works   better   because   it   captures   the   contextual   meaning   and relationships within user queries, while BM25 relies solely on keyword matching,   making semantic search more effective for nuanced football analytics and user intent. The system prompts described in (Chapter 4) effectively handle out-of- domain queries, such as when users ask about unrelated topics like current political figures. For example, the chatbot responds clearly by stating its focus is strictly on football analytics. One downside is the relatively slow response time from the assistant compared to the rest of the system, which users find very fast according to UAT. The assistant alone averages 14.7 seconds, contributing significantly to the total system latency. If reducing this response time becomes a priority, using a smaller, fine-tuned model could decrease the round-trip latency. High variability in response time is mainly due to the OpenAI assistant, with a standard deviation of 6.1 seconds. A re-fetch mechanism was developed for the RAG component, allowing users to request alternative data when initially retrieved results are incorrect. Scripts for this feature have been written but remain untested and inactive. This functionality enables user feedback to enhance data accuracy, leveraging high NDCG@ k   scores indicating that secondary results typically meet user expectations. Some errors observed include occasionally losing context in follow-up questions, for instance, unexpectedly switching from football (soccer) to American football. Another rare but notable error is when the chatbot retrieves an incorrect dataset upon user request. Although the assistant appropriately refuses to analyze incorrect data, it still fails to deliver the correct information,

=== 페이지 85 ===

Discussion | 69  even when the query is within scope and data availability. The current implementation aligns well with the ideas behind the RAG framework, particularly when it comes to dealing with retrieval efficiency and fine-tuning limitations.   Since user data was limited, fine-tuning wasn’t really an option, making the retrieval of dynamic external knowledge even more important. Adopting a more advanced vector database closely matches what Advanced and Modular RAG methods suggest, likely improving the relevance and accuracy of results, which it did in this project. Additionally, the untested re-fetch feature embodies Modular RAG’s concept of iterative refinement, allowing users to correct retrieval mistakes through feedback. Lastly, the system’s latency issues and occasional inaccuracies reflect typical RAG challenges, showing clear areas for further improvement in retrieval methods and model optimization. An important insight from this project was the realization that LLMs perform best when they are not overloaded with too many responsibilities at once.   Rather than expecting a single general-purpose LLM to handle everything from understanding user intent to judging response quality and performing in-depth data analysis, it is more effective to split these tasks across specialized components. This mirrors how software is traditionally structured, with separate functions for distinct tasks. In the context of LLMs, this could mean using multiple smaller or fine-tuned models: one specifically trained for user classification, another for LLM-as-a-judge, and a third focused on interpreting and analyzing data.   This modular approach not only improves performance but also makes the system more interpretable and easier to maintain.

=== 페이지 86 ===

70 | Conclusions and Future work  Chapter 7 Conclusions and Future work  This work has resulted in a domain-specific chatbot working,   allowing PlaymakerAI’s users to chat with their complex data and giving easy access to more detailed and advanced analysis of matches and teams.  7.1   Conclusions  Initial tests with other chatbots showed hallucinations and not up-to-date facts, similar to what was seen in the JayBot study. This study shows that a small, modular RAG + LLM stack can drop PlaymakerAI’s dashboard build-time from 45 minutes to less than a minute while still ranking the right data with a MRR score of 0.92 out of 1 in cases and hitting an 0.85 F1 score. Users solved every test task and called the bot “fast” and “easy” proving the chat layer removes the steep learning curve that used to block new users. Because every component: vector store, embedder, or LLM, can be swapped with a single command, the system can grow when better or cheaper models appear. One problem is the over-confidence of the system, adding language- aware embeddings and fine-tuning the assistant on real chat logs should lift specificity past 50% and turn the prototype into a production-ready analyst for both novices and experts, but also lead to more accurate and insightful analyses that further improve overall quality.  7.2   Limitations  This study was run on a CPU-only laptop,   which set strict hardware boundaries on the system and its components. Fine-tuning was also deferred. PlaymakerAI’s platform still lacks a critical mass of real chat logs and

=== 페이지 87 ===

Conclusions and Future work | 71  synthetic data would not capture the information experts expect. Until more authentic queries have been gathered, both the assistant and the embedding model will stay in zero-shot mode. LangChain’s production add-ons, such as LangGraph for stateful flows and LangSmith for monitoring, were evaluated but not integrated. They make sense once the bot moves from prototype to live service.   To reduce the complexity level, it was decided not to use a separate model for the LLM as a judge.   Finally, development and testing centered on men’s competitions. The retrieval stack may misclassify or rank women’s league data until separate embeddings or metadata tags are added and evaluated.   If data were clearly annotated as women’s and men’s data, then filtering when using the vector database could easily be implemented as a possible solution.  7.3   Future work  Several steps can lift the prototype to the production level.   First, expand the RapidFuzz dictionary with team nicknames so terms like “Spurs” still resolve to Tottenham. Linking this list to an external API and refreshing it monthly would keep the league current without manual edits after promotions or relegation. Second, fine-tune the assistant once a few hundred genuine chat logs have been collected. This should shorten answers and allow them to be more concise and stick to what the users really want. Use LoRA to reduce carbon footprint, costs and time. Third, move the stack to a cloud instance with GPU support so larger embedding models and possibly a cross-encoder re-ranker can run at low latency. Fourth, when the dataset workload becomes larger, moving to a vector database manager with lower latency and better filtering options (like Milvius or Qdrant) would be a good fine-tuning of the system. Fifth, using a separate model for the LLM as a judge would allow for more thorough writing of system instructions and fine-tuning the model only for this use case. Finally, connect the chatbot to PlaymakerAI’s backend so it can write dashboards straight to the user’s workspace. The current script drops the entire vector store and rebuilds it from scratch whenever it needs to be updated.   That brute-force approach is fine for a prototype, but it is slow, wasteful and risks downtime in production. What is still missing is an updater that spots new, changed, or deleted data and touches only those vectors.   That logic should run as a nightly (or weekly) job will keep the index fresh without interrupting service. Because the thesis focused on retrieval quality and response quality rather than data-ops, this polishing step was left for future work.

=== 페이지 88 ===

72 | Conclusions and Future work  Although data privacy isn’t unique to PlaymakerAI, it remains a key concern.   PlaymakerAI’s data are proprietary, yet queries run through an external LLM. The current modular setup means mitigation can easily be performed to an on-prem or open-source model later, but a formal data handling policy should be created.

=== 페이지 89 ===

References | 73  References  [1] “OpenAI Platform - Your data.” [Online]. Available: https://platform.o penai.com/docs/guides/your-data [Pages 6, 37, and 38.] [2] A. Eleti, J. Harris, and L. Kilpatrick, “Function calling and other API updates.” [Online]. Available: https://openai.com/index/function-calli ng-and-other-api-updates/ [Page 8.] [3] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. Von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush, “Transformers:   State-of-the-Art Natural Language Processing,” in   Proceedings of the 2020 Conference on   Empirical   Methods   in   Natural   Language   Processing:   System Demonstrations .   Online: Association for Computational Linguistics, 2020.   doi:   10.18653/v1/2020.emnlp-demos.6   pp.   38–45.   [Online]. Available:   https://www.aclweb.org/anthology/2020.emnlp-demos.6 [Page 8.] [4] “Function Calling.” [Online]. Available: https://huggingface.co/docs/ hugs/en/guides/function-calling [Page 9.] [5] K. Basu, “Bridging Knowledge Gaps in LLMs via Function Calls,” in  Proceedings of the 33rd ACM International Conference on Information and Knowledge Management .   Boise ID USA: ACM, Oct. 2024. doi: 10.1145/3627673.3679070. ISBN 9798400704369 pp. 5556–5557. [Online]. Available: https://dl.acm.org/doi/10.1145/3627673.3679070 [Page 9.] [6] “Open   AI   Platform   -   Capabilities.”   [Online].   Available:   https: //platform.openai.com/docs/guides/function-calling [Page 9.] [7] Y.   Gao,   Y.   Xiong,   X.   Gao,   K.   Jia,   J.   Pan,   Y.   Bi,   Y.   Dai, J. Sun, M. Wang, and H. Wang, “Retrieval-Augmented Generation

=== 페이지 90 ===

74 | References  for Large Language Models:   A Survey,” 2023. [Online]. Available: https://arxiv.org/abs/2312.10997 [Pages 9 and 10.] [8] R. Lakatos,   P. Pollner,   A. Hajdu,   and T. Joó,   “Investigating the Performance of Retrieval-Augmented Generation and Domain-Specific Fine-Tuning   for   the   Development   of   AI-Driven   Knowledge-Based Systems,”   Machine Learning and Knowledge Extraction , vol. 7, no. 1, p. 15, Feb. 2025. doi:   10.3390/make7010015. [Online]. Available: https://www.mdpi.com/2504-4990/7/1/15 [Pages 10 and 11.] [9] J. Odede and I. Frommholz, “JayBot – Aiding University Students and   Admission   with   an   LLM-based   Chatbot,”   in   Proceedings   of the 2024 ACM SIGIR Conference on Human Information Interaction and   Retrieval .   Sheffield   United   Kingdom:   ACM,   Mar.   2024. doi:   10.1145/3627508.3638293. ISBN 9798400704345 pp. 391–395. [Online]. Available: https://dl.acm.org/doi/10.1145/3627508.3638293 [Page 11.] [10] Z.   Jing,   Y.   Su,   and   Y.   Han,   “When   Large   Language   Models Meet   Vector   Databases:   A   Survey,”   2024.   [Online].   Available: https://arxiv.org/abs/2402.01763 [Page 11.] [11] V. B. Parthasarathy, A. Zafar, A. Khan, and A. Shahid, “The Ultimate Guide   to   Fine-Tuning   LLMs   from   Basics   to   Breakthroughs:   An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities,” 2024. [Online]. Available: https://arxiv.org/abs/2408.13296 [Page 11.] [12] C. Yoshida, M. Matsushita, and Y. Higo, “Estimating the Difficulty of   Programming   Problems   Using   Fine-tuned   LLM,”   in   2024 IEEE/ACIS 22nd International Conference on Software Engineering Research,   Management and Applications (SERA) .   Honolulu,   HI, USA: IEEE, May 2024. doi:   10.1109/SERA61261.2024.10685596. ISBN   9798350391343   pp.   28–34.   [Online].   Available:   https: //ieeexplore.ieee.org/document/10685596/ [Page 11.] [13] L. V. d. Carvalho, P. H. D. Valle, B. R. Leifheit, L. E. R. Cabrejos, W.   Nakamura,   G.   C.   Guerino,   R.   D.   S.   Garcia,   and   W.   Silva, “What Do We Know About Usability Evaluation for Chatbots?:   A Systematic Mapping Study,”   in   Proceedings of the 20th Brazilian Symposium   on   Information   Systems ,   ser.   SBSI   ’24.   New   York,

=== 페이지 91 ===

References | 75  NY, USA: Association for Computing Machinery,   May 2024. doi: 10.1145/3658271.3658324. ISBN 9798400709968 pp. 1–11. [Online]. Available: https://dl.acm.org/doi/10.1145/3658271.3658324 [Page 11.] [14] M. Oliver and G. Wang, “Crafting Efficient Fine-Tuning Strategies for   Large   Language   Models,”   2024.   [Online].   Available:   https: //arxiv.org/abs/2407.13906 [Page 11.] [15] H. K. Chaubey, G. Tripathi, R. Ranjan, and S. K. Gopalaiyengar, “Comparative Analysis of RAG, Fine-Tuning, and Prompt Engineering in Chatbot Development,” in   2024 International Conference on Future Technologies for Smart Society (ICFTSS) .   Kuala Lumpur, Malaysia: IEEE, Aug. 2024. doi:   10.1109/ICFTSS61109.2024.10691338. ISBN 9798350373844 pp. 169–172. [Online]. Available: https://ieeexplore.i eee.org/document/10691338/ [Page 11.] [16] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “LoRA: Low-Rank Adaptation of Large Language Models,” 2021. [Online]. Available: https://arxiv.org/abs/2106.09685 [Page 12.] [17] “Getting   Started   With   Embeddings.”   [Online].   Available:   https: //huggingf ace.co/blog/getting-started-with-embeddings   [Pages 12 and 13.] [18] “Introducing text and code embeddings,” Mar. 2024. [Online]. Available: https://openai.com/index/introducing-text-and-code-embeddings/ [Page 13.] [19] “LangChain.” [Online]. Available: https://www.langchain.com/langch ain [Page 14.] [20] “Introduction | LangChain.” [Online]. Available: https://python.langcha in.com/docs/introduction/ [Page 14.] [21] M. Kumar, “The Ultimate Guide to Vector Databases — 2024 and Beyond,” Aug. 2024. [Online]. Available: https://medium.com/madhu karkumar/the-ultimate-guide-to-vector-databases-2024-and-beyond-1 6dfb15bef12 [Pages 14 and 15.] [22] J. Johnson, M. Douze, and H. Jegou, “Billion-Scale Similarity Search with GPUs,”   IEEE Transactions on Big Data , vol. 7, no. 3, pp. 535–547, Jul. 2021. doi: 10.1109/TBDATA.2019.2921572. [Online]. Available: https://ieeexplore.ieee.org/document/8733051/ [Page 14.]

=== 페이지 92 ===

76 | References  [23] S. Kukreja,   T. Kumar,   V. Bharate,   A. Purohit,   A. Dasgupta,   and D. Guha,   “Vector Databases and Vector Embeddings-Review,”   in  2023   International   Workshop   on   Artificial   Intelligence   and   Image Processing (IWAIIP) .   Yogyakarta,   Indonesia:   IEEE, Dec. 2023. doi: 10.1109/IWAIIP58158.2023.10462847. ISBN 9798350382914 pp. 231–236. [Online]. Available: https://ieeexplore.ieee.org/document/1 0462847/ [Pages 14 and 15.] [24] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica, “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,” 2023. [Online]. Available:   https://arxiv.org/abs/2306.05685 [Pages 15, 16, and 39.] [25] P.   Andrews,   O.   E.   Nordberg,   S.   Zubicueta   Portales,   N.   Borch, F. Guribye, K. Fujita, and M. Fjeld, “AiCommentator: A Multimodal Conversational Agent for Embedded Visualization in Football Viewing,” in   Proceedings of the 29th International Conference on Intelligent User   Interfaces .   Greenville   SC   USA:   ACM,   Mar.   2024.   doi: 10.1145/3640543.3645197. ISBN 9798400705083 pp. 14–34. [Online]. Available: https://dl.acm.org/doi/10.1145/3640543.3645197 [Page 16.] [26] C. Segura, \. Palau, J. Luque, M. R. Costa-Jussà, and R. E. Banchs, “Chatbol, a Chatbot for the Spanish “La Liga”,” in   9th International Workshop on Spoken Dialogue System Technology , L. F. D’Haro, R. E. Banchs, and H. Li, Eds.   Singapore: Springer Singapore, 2019, vol. 579, pp. 319–330. ISBN 9789811394423 9789811394430. [Online]. Available:   http://link.springer.com/10.1007/978-981-13-9443-0_28 [Page 16.] [27] Hevner,   March,   Park,   and Ram,   “Design Science in Information Systems Research,”   MIS Quarterly , vol. 28, no. 1, p. 75, 2004. doi: 10.2307/25148625. [Online]. Available: https://www.jstor.org/stable/1 0.2307/25148625 [Pages 18 and 19.] [28] S. Easterbrook, J. Singer, M.-A. Storey, and D. Damian, “Selecting Empirical Methods for Software Engineering Research,”   in   Guide to Advanced Empirical Software Engineering ,   F. Shull,   J. Singer, and D. I. K. Sjøberg, Eds.   London:   Springer London, 2008, pp. 285–311. ISBN 9781848000438 9781848000445. [Online]. Available: http://link.springer.com/10.1007/978-1-84800-044-5_11 [Page 19.]

=== 페이지 93 ===

References | 77  [29] K. Peffers,   T. Tuunanen,   M. A. Rothenberger,   and S. Chatterjee, “A Design Science Research Methodology for Information Systems Research,”   Journal of Management Information Systems , vol. 24, no. 3, pp. 45–77, Dec. 2007. doi: 10.2753/MIS0742-1222240302. [Online]. Available: https://www.tandfonline.com/doi/full/10.2753/MIS0742-1 222240302 [Page 19.] [30] “GPT-4o   mini:   advancing   cost-efficient   intelligence.”   [Online]. Available:   https://openai.com/index/gpt-4o-mini-advancing-cos t-efficient-intelligence/ [Page 27.] [31] S. Borsci, A. Malizia, M. Schmettow, F. Van Der Velde, G. Tariverdiyeva, D. Balaji, and A. Chamberlain, “The Chatbot Usability Scale:   the Design and Pilot of a Usability Scale for Interaction with AI-Based Conversational Agents,”   Personal and Ubiquitous Computing , vol. 26, no. 1,   pp. 95–119,   Feb. 2022. doi:   10.1007/s00779-021-01582-9. [Online]. Available: https://link.springer.com/10.1007/s00779-021-0 1582-9 [Pages 28, 56, and 57.] [32] P. Shah, “All about RapidFuzz — String Similarity and Matching,” Oct. 2021. [Online]. Available: https://shahparthvi22.medium.com/all-abo ut-rapidfuzz-string-similarity-and-matching-cd26fdc963d8 [Page 33.] [33] “rapidfuzz/RapidFuzz,”   May   2025,   original-date:   2020-02- 29T14:41:44Z.   [Online].   Available:   https://github.com/rapidfuzz /RapidFuzz [Page 33.] [34] X. Xie, H. Liu, W. Hou, and H. Huang, “A Brief Survey of Vector Databases,” in   2023 9th International Conference on Big Data and Information Analytics (BigDIA) .   Haikou, China:   IEEE, Dec. 2023. doi: 10.1109/BigDIA60676.2023.10429609. ISBN 9798350330076 pp. 364–371. [Online]. Available: https://ieeexplore.ieee.org/document/1 0429609/ [Page 34.] [35] “Chroma | LangChain.” [Online]. Available: https://python.langchain.c om/docs/integrations/vectorstores/chroma/ [Page 34.] [36] “MTEB Leaderboard - a Hugging Face Space by mteb.” [Online]. Available: https://huggingface.co/spaces/mteb/leaderboard [Page 34.] [37] L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei, “Multilingual   E5   Text   Embeddings:   A   Technical   Report,”   2024. [Online]. Available: https://arxiv.org/abs/2402.05672 [Page 35.]

=== 페이지 94 ===

78 | References  [38] “What is an instruct model?   - Instruction and Chat Fine-Tuning.” [Online]. Available: https://community.aws/content/2ZVa61RxToXU FzcuY8Hbut6L150/what-is-an-instruct-model [Page 35.] [39] “GPTs vs Assistants | OpenAI Help Center.” [Online]. Available: https: //help.openai.com/en/articles/8673914-gpts-vs-assistants [Page 37.] [40] Z. Yu, L. He, Z. Wu, X. Dai, and J. Chen, “Towards Better Chain-of- Thought Prompting Strategies: A Survey,” 2023. [Online]. Available: https://arxiv.org/abs/2310.04959 [Page 40.] [41] J. Wei,   X. Wang,   D. Schuurmans,   M. Bosma,   B. Ichter,   F. Xia, E. Chi, Q. V. Le, and D. Zhou, “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,”   Advances in Neural Information Processing Systems , vol. 35, pp. 24 824–24 837, Dec. 2022. [Online]. Available:   https://proceedings.neurips.cc/paper_files/paper/2022/ha sh/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html [Page 40.] [42] C. Goutte and E. Gaussier, “A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation,” in   Advances in Information Retrieval , D. Hutchison, T. Kanade, J. Kittler, J. M. Kleinberg,   F.   Mattern,   J.   C.   Mitchell,   M.   Naor,   O.   Nierstrasz, C. Pandu Rangan, B. Steffen, M. Sudan, D. Terzopoulos, D. Tygar, M. Y. Vardi, G. Weikum, D. E. Losada, and J. M. Fernández-Luna, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, vol. 3408, pp. 345–359. ISBN 9783540252955 9783540318651. [Online]. Available: http://link.springer.com/10.1007/978-3-540-31865-1_25 [Page 44.] [43] J.   Shen,   H.   He,   W.   Shen,   and   T.   Shen,   “Enhancing   the   RAG Retrieval   Engine   Through   Multi-Encoder   Fusion,”   in   2024   5th International Conference on Electronic Communication and Artificial Intelligence (ICECAI) .   Shenzhen,   China:   IEEE, May 2024. doi: 10.1109/ICECAI62591.2024.10674962.   ISBN   9798350386943   pp. 227–230. [Online]. Available:   https://ieeexplore.ieee.org/docume nt/10674962/ [Page 45.] [44] Y. Wang, L. Wang, Y. Li, D. He, T.-Y. Liu, and W. Chen, “A Theoretical Analysis of NDCG Type Ranking Measures,” 2013. [Online]. Available: https://arxiv.org/abs/1304.6480 [Page 45.]

=== 페이지 95 ===

References | 79  [45] “Why You Only Need to Test with 5 Users.” [Online]. Available: https: //www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/ [Page 56.] [46] S. Holmes, A. Moorhead, R. Bond, H. Zheng, V. Coates, and M. Mctear, “Usability testing of a healthcare chatbot:   Can we use conventional methods to assess conversational user interfaces?”   in   Proceedings of the 31st European Conference on Cognitive Ergonomics .   BELFAST United Kingdom:   ACM, Sep. 2019. doi:   10.1145/3335082.3335094. ISBN   9781450371667   pp.   207–214.   [Online].   Available:   https: //dl.acm.org/doi/10.1145/3335082.3335094 [Page 56.] [47] M.-L.   Chen   and   H.-C.   Wang,   “How   Personal   Experience   and Technical   Knowledge   Affect   Using   Conversational   Agents,”   in  Companion   Proceedings   of   the   23rd   International   Conference   on Intelligent User Interfaces .   Tokyo Japan:   ACM, Mar. 2018. doi: 10.1145/3180308.3180362. ISBN 9781450355711 pp. 1–2. [Online]. Available: https://dl.acm.org/doi/10.1145/3180308.3180362 [Page 56.] [48] R.   Ren,   M.   Zapata,   J.   W.   Castro,   O.   Dieste,   and   S.   T. Acuna,   “Experimentation   for   Chatbot   Usability   Evaluation:   A Secondary   Study,”   IEEE   Access ,   vol.   10,   pp.   12 430–12 464, 2022.   doi:   10.1109/ACCESS.2022.3145323.   [Online].   Available: https://ieeexplore.ieee.org/document/9687577/ [Pages 56 and 57.] [49] A. Venkatesh, C. Khatri, A. Ram, F. Guo, R. Gabriel, A. Nagar, R. Prasad, M. Cheng, B. Hedayatnia, A. Metallinou, R. Goel, and A. Raju, “On Evaluating and Comparing Conversational Agents,” 2017. [Online]. Available: https://www.amazon.science/publications/on-eva luating-and-comparing-conversational-agents [Pages 56 and 57.] [50] J.   Casas,   M.-O.   Tricot,   O.   Abou   Khaled,   E.   Mugellini,   and P. Cudré-Mauroux, “Trends & Methods in Chatbot Evaluation,” in  Companion   Publication   of   the   2020   International   Conference   on Multimodal Interaction .   Virtual Event Netherlands: ACM, Oct. 2020. doi:   10.1145/3395035.3425319. ISBN 9781450380027 pp. 280–286. [Online]. Available: https://dl.acm.org/doi/10.1145/3395035.3425319 [Page 57.] [51] O. P. Elliott and J. Clark, “The Impacts of Data, Ordering, and Intrinsic Dimensionality on Recall in Hierarchical Navigable Small Worlds,”

=== 페이지 96 ===

80 | References  in   Proceedings of the 2024 ACM SIGIR International Conference on Theory of Information Retrieval .   Washington DC USA: ACM, Aug. 2024. doi: 10.1145/3664190.3672512. ISBN 9798400706813 pp. 25–33. [Online]. Available: https://dl.acm.org/doi/10.1145/3664190.3 672512 [Page 62.]

=== 페이지 97 ===

Data Coverage | 81  Appendix A Data Coverage  A.1   Competitions  A.1.1   Leagues and Cups  •   AFC Champions League (2024)  •   Indian Super League (2024)  •   German 3. Liga (2024)  •   A-League (2024)  •   UEFA Europa League (2024)  •   Austrian 2. Liga (2024)  •   Bulgarian First Division (2024)  •   England League Cup (2024)  •   Frauen Bundesliga (2024)  •   Serie A Femminile (2024)  •   UEFA Conference League (2024)  •   UEFA Champions League (2024)  •   Swiss Challenge League (2024)  •   Romanian Super League (2024)  •   VBET League (2024)  •   Turkish 1. Lig (2024)  •   League Two (2024)  •   League One (2024)  •   1. SNL (2024)  •   Liga MX (2024)  •   Saudi Pro League (2024)  •   Israeli Premier League (2024)  •   Greece Super League (2024)  •   Slovakian Super League (2024)  •   Serie B (2024)  •   2. Bundesliga (2024)  •   Ligue 1 (2024)  •   Danish Super League (2024)  •   Austrian Bundesliga (2024)  •   Ekstraklasa (2024)  •   Eerste Divisie (2024)  •   Turkish Super League (2024)  •   Premiership (2024)  •   Swiss Super League (2024)  •   Eredivisie (2024)

=== 페이지 98 ===

82 | Data Coverage  •   Ligue 2 (2024)  •   Belgium First Division B (2024)  •   Portugal Segunda Liga (2024)  •   Segunda Division (2024)  •   NB1 (2024)  •   Serbian Super League (2024)  •   Primeira Liga (2024)  •   Belgium First Division (2024)  •   Fortuna Liga (2024)  •   Serie A (2024)  •   Croatia 1. HNL (2024)  •   Bundesliga (2024)  •   La Liga (2024)  •   Premier League (2024)  •   Championship (2024)  •   Danish 2. Division (2024)  •   Danish First Division (2024)  •   Women’s EC Qualification Gr A (2025)  •   Northern Super League (2025)  •   CONCACAF Champions Cup (2025)  •   CAF Confederation Cup (2024)  •   USL Super League (2024)  •   Brazil Serie B (2025)  •   Besta-deild karla (2025)  •   Canadian Premier League (2025)  •   Veikkausliga (2025)  •   Bolivia LFPB (2025)  •   Ireland Premier Division (2025)  •   Brazil Serie A (2025)  •   Saudi WPL (2024)  •   I-League 2 (2024)  •   China League Two (2025)  •   China League One (2025)  •   Qatari Second Division (2024)  •   EFL Trophy (2024)  •   Dutch Cup (2024)  •   CWSL (2025)  •   OBOS (2025)  •   Allsvenskan (2025)  •   Superettan (2025)  •   Eliteserien (2025)  •   Ettan (2025)  •   CAF Champions League (2024)  •   I-League (2024)  •   Liga Venezolana (2025)  •   Thai League 1 (2024)  •   Qatar Stars League (2024)  •   A-League Women (2024)  •   UAE Pro League (2024)  •   K League 2 (2025)  •   France National 1 (2024)  •   USL League One (2025)  •   Damallsvenskan (2025)  •   Coupe de France (2024)  •   DFB-Pokal (2024)  •   Copa del Rey (2024)  •   Coppa Italia (2024)  •   FA Cup (2024)  •   CSL (2025)  •   Colombia Liga Betplay (2025)  •   NWSL (2025)  •   Copa Sudamericana (2025)

=== 페이지 99 ===

Data Coverage | 83  •   USL Championship (2025)  •   Virsliga (2025)  •   MLS Next Pro (2025)  •   J3 League (2025)  •   Brazil Paulista 1 (2025)  •   MLS (2025)  •   Paraguay Division Profesional (2025)  •   K League 1 (2025)  •   Chile Primera Division (2025)  •   Svenska Cupen Herr (2025)  •   J2 League (2025)  •   Ecuador Liga Pro (2025)  •   J1 League (2025)  •   Peru Primera Division (2025)  •   Copa Libertadores (2025)  •   Argentina Liga Profesional (2025)  •   Eredivisie Women (2024)  •   Egypt Premier League (2024)  •   PSL (2024)  •   Feminine Division 1 (2024)  •   Liga F (2024)  •   UEFA Champions League Women (2024)  •   Ireland First Division (2025)  •   Segunda Liga (2024)  A.1.2   Individual Games  1.   Fiorentina – Napoli 2.   Villarreal – Valencia 3.   Monaco – Brest 4.   West Ham United – Newcastle 5.   Borussia Mönchengladbach – Stuttgart 6.   Göteborg –AIK 7.   Rennes – PSG 8.   Juventus – AC Milan 9.   Malmö FF – Hammarby 10.   Liverpool – Manchester City 11.   Real Madrid – Barcelona 12.   Bayern Munich – RB Leipzig 13.   Napoli – Inter Milan 14.   Wolves – Manchester City 15.   Newcastle – N. Forrest (team) 16.   Newcastle – N. Forrest

=== 페이지 100 ===

84 | JSON Dashboards  Appendix B JSON Dashboards  B.1   Opponent Analysis  Listing B.1: Opponent Analysis Dashboard  import   json def   create_opponent_analysis ( team ,   season_id ,   season_name ,   label , influential_players ): try : analysis   =   { " name ":   f "{ team }:   Opponent   Analysis ", " widgets ":   [ { " key ":   " sc_1740409340 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   season_name , " selectedRadarKeys ":   [ influential_players [0] , influential_players [1] ], " selectedKpis ":   [ " Goals90 ", " CarryProgDist90 ", " PassProgDist90 ", " ShotAssists90 ", " Int90Padj ", " Chall90Padj ", " xTPrevented90Padj ", " Aerials90Padj ", " XTReceived90 ", " xT90 ", " xG90 " ], " availableKpis ":   [] , " toggleMeta ":   False }, " tab_index ":   0,

=== 페이지 101 ===

JSON Dashboards | 85  " type ":   " Playmaker365SpiderWidget ", " wid ":   " sc_1733907403161 " }, { " key ":   " sc_1740409341 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   season_name , " selectedBars ":   [ " xG90 ", " xT90 ", " MeanKPIScore " ], " teamFilter ":   [ team ], " selectedPoints ":   [] , " alwaysShownPoints ":   [] }, " tab_index ":   0, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1733907404138 " }, { " key ":   " sc_1740409345 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   season_name , " selectedX ":   " PassProgDist90 ", " selectedY ":   " CarryProgDist90 ", " selectedZ ":   " PlaymakerRank ", " selectedPoints ":   [] , " teamFilter ":   team , " wide ":   "100%" , " toggleRegLine ":   False , " toggleCrosshair ":   False , " zAxisOptions ":   { " showAxis ":   True , " gradient ":   True , " dotSize ":   False }, " toggleRegLineOffset ":   50 }, " tab_index ":   1, " type ":   " Playmaker365ScatterWidget ", " wid ":   " scatter_1733907553564 " }, { " key ":   " sc_1740409348 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   season_name , " selectedBars ":   [ " xT90 ", " xG90 ", " Goals90 " ], " teamFilter ":   [ team

=== 페이지 102 ===

86 | JSON Dashboards  ], " selectedPoints ":   [] , " alwaysShownPoints ":   [] }, " tab_index ":   2, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1733907638898 " }, { " key ":   " sc_1740409349 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   season_name , " selectedBars ":   [ " ShotAssists90 " ], " teamFilter ":   [ team ], " selectedPoints ":   [] , " alwaysShownPoints ":   [] }, " tab_index ":   2, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1733907639047 " }, { " key ":   " sc_1740409355 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   season_name , " selectedRadarKeys ":   [ influential_players [0] , influential_players [1] ], " selectedKpis ":   [ " Goals90 ", " CarryProgDist90 ", " PassProgDist90 ", " ShotAssists90 ", " Int90Padj ", " Chall90Padj ", " xTPrevented90Padj ", " Aerials90Padj ", " XTReceived90 ", " xT90 ", " xG90 " ], " availableKpis ":   [] , " toggleMeta ":   False }, " tab_index ":   3, " type ":   " Playmaker365SpiderWidget ", " wid ":   " sc_1733926343763 " }, { " key ":   " sc_1740409356 ", " settings ":   { " datasetId ":   season_id ,

=== 페이지 103 ===

JSON Dashboards | 87  " datasetName ":   season_name , " selectedBars ":   [ " Minutes " ], " teamFilter ":   [ team ], " selectedPoints ":   [] , " alwaysShownPoints ":   [] }, " tab_index ":   3, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1733926365798 " }, { " key ":   " sc_1740409357 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   season_name , " selectedBars ":   [ " xTPrevented90Padj ", " Aerials90Padj " ], " teamFilter ":   [ team ], " selectedPoints ":   [] , " alwaysShownPoints ":   [] }, " tab_index ":   3, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1733926365929 " }, { " key ":   " sc_1740409358 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   season_name , " selectedBars ":   [ " PlaymakerRank " ], " teamFilter ":   [ team ], " selectedPoints ":   [] , " alwaysShownPoints ":   [] }, " tab_index ":   3, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1733926477879 " }, { " key ":   " sc_1740409359 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   season_name , " selectedBars ":   [ " MeanKPIScore " ],

=== 페이지 104 ===

88 | JSON Dashboards  " teamFilter ":   [ team ], " selectedPoints ":   [] , " alwaysShownPoints ":   [] }, " tab_index ":   3, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1733926478025 " } ], " tab_names ":   [ " Style   of   Play ", " Passing   style ", " Goal   threat ", " Players " ], " emails ":   [] , " labels ":   [] } #   print ("\ nOpponent   Analysis   Template :") print ( json . dumps ( analysis ,   indent =2) ) print (f" dashboard_url :   https :// playmaker . ai / dashboards /{ label . replace ( '   ' ,   ' - ' ). lower () }") return   True except   Exception   as   e: print (f" Error   creating   opponent   analysis :   {e }") return   False  B.2   Match Analysis  Listing B.2: Match Analysis Dashboard  import   json def   create_match_analysis ( home_team ,   away_team ,   season_id ,   label ): try : season_id_players   =   " Needs   to   be   created " analysis   =   { " name ":   f" Game   analysis :   { home_team }   vs   { away_team }" , " widgets ":   [ { " key ":   " sc_1740410873 ", " settings ":   { " availableKpis ":   [] , " datasetId ":   season_id , " datasetName ":   f" Match   analysis :   { home_team } -{ away_team }" , " selectedItems ":   [ home_team , away_team ], " selectedKpis ":   [ " ChallPct ", " Crosses ",

=== 페이지 105 ===

JSON Dashboards | 89  " FTE ", " LostBallsDH ", " Passes ", " PassesPerChain ", " PassPct ", " PIB ", " PlusMinusSubs ", " PlusMinusXGSubs ", " Shots ", " ShotsInGZ ", " ShotsOnTarget ", " WonBallsOH ", " xG ", " xT " ], " showTable ":   True }, " tab_index ":   0, " type ":   " Playmaker365Head2HeadWidget ", " wid ":   " head2head_1738849819528 " }, { " key ":   " sc_1740410874 ", " settings ":   { " datasetId ":   season_id , " datasetName ":   f "{ home_team }   vs   { away_team }   ( team )", " selectedKpi ":   " Shots ", " showCount ":   20 }, " tab_index ":   0, " type ":   " Playmaker365TopListWidget ", " wid ":   " toplist_1738849903311 " }, { " key ":   " sc_1740410877 ", " settings ":   { " alwaysShownPoints ":   [] , " datasetId ":   season_id , " datasetName ":   f "{ home_team }   vs   { away_team }   ( team )", " selectedBars ":   [ " xG ", " xT ", " ShotsInGZ " ], " selectedPoints ":   [] , " teamFilter ":   [] }, " tab_index ":   0, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1738850363490 " }, { " key ":   " sc_1740410878 ", " settings ":   { " availableKpis ":   [] , " datasetId ":   season_id_players , " datasetName ":   f "{ home_team }   vs   { away_team }   ( player ) ", " selectedKpis ":   [

=== 페이지 106 ===

90 | JSON Dashboards  " Aerials ", " Assists ", " CarryProgDist ", " ChallengePct ", " Dribbles ", " Goals ", " Interceptions ", " Passes ", " PassPct ", " PassProgDist ", " ShotAssists ", " Tackles ", " xG ", " xT ", " XTAssisted ", " XTReceived " ], " selectedRadarKeys ":   [ " Alexander   Isak ", " Chris   Wood " ], " toggleAverage ":   True , " toggleMeta ":   False }, " tab_index ":   1, " type ":   " Playmaker365SpiderWidget ", " wid ":   " sc_1738850912914 " }, { " key ":   " sc_1740410879 ", " settings ":   { " datasetId ":   season_id_players , " datasetName ":   f "{ home_team }   vs   { away_team }   ( player ) ", " selectedPoints ":   [] , " selectedX ":   " PassPct ", " selectedY ":   " Dribbles ", " selectedZ ":   " ChallengePct ", " teamFilter ":   "" , " toggleRegLineOffset ":   99999 }, " tab_index ":   1, " type ":   " Playmaker365ScatterWidget ", " wid ":   " scatter_1738850942477 " }, { " key ":   " sc_1740410880 ", " settings ":   { " alwaysShownPoints ":   [] , " datasetId ":   season_id_players , " datasetName ":   f "{ home_team }   vs   { away_team }   ( player ) ", " selectedBars ":   [ " Dribbles ", " xG " ], " selectedPoints ":   [] , " teamFilter ":   [] },

=== 페이지 107 ===

JSON Dashboards | 91  " tab_index ":   1, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1738851221993 " }, { " key ":   " sc_1740410881 ", " settings ":   { " alwaysShownPoints ":   [] , " datasetId ":   season_id_players , " datasetName ":   f "{ home_team }   vs   { away_team }   ( player ) ", " selectedBars ":   [ " Interceptions ", " Aerials ", " ChallengePct " ], " selectedPoints ":   [] , " teamFilter ":   [] }, " tab_index ":   1, " type ":   " Playmaker365BarChartWidget ", " wid ":   " bar_1738851222154 " } ], " tab_names ":   [ " General ", " Players " ], " emails ":   [] , " labels ":   [] } #   print ("\ nMatch   Analysis   Template :") print ( json . dumps ( analysis ,   indent =2) ) print (f" dashboard_url :   https :// playmaker . ai / dashboards /{ label . replace ( '   ' ,   ' - ' ). lower () }") return   True except   Exception   as   e: print (f" Error   creating   match   analysis :   {e }") return   False

=== 페이지 108 ===

92 | Benchmarking Queries and Their Target Competitions  Appendix C Benchmarking Queries and Their Target Competitions  1.   Analyze the last game that was played between Newcastle and Nottingham   —   Newcastle – N Forrest (team)  2.   Make an opponent analysis of Arsenal 2024   —   Premier League, 2024  3.   Analyse Manchester United   —   Premier League, 2024  4.   How does Manchester United play?   —   Premier League, 2024  5.   Analyze Real Madrid’s style of play in the Champions League 24   —   UEFA Champions League, 2024  6.   Make a match analysis for Newcastle United vs Nottingham Forrest   —   Newcastle – N Forrest (team)  7.   Who has the highest xG90 in Svenska Cupen 2025?   —   Svenska Cupen Herr, 2025  8.   Who has scored the most goals in Svenska Cupen?   —   Svenska Cupen Herr, 2025  9.   Which player has scored the most goals in Ligue 1?   —   Ligue 1, 2024  10.   Which player has played the most minutes in Ligue 1?   —   Ligue 1, 2024  11.   Which player scored the most goals in the game between Newcastle and Nottingham?   —   Newcastle – N Forrest  12.   How does Napoli play their football?   —   Serie A, 2024  13.   Help me analyze my next game against Bayern Munich   —   Bundesliga, 2024  14.   Which league does FC St. Pauli play in?   —   Bundesliga, 2024  15.   What is Borussia Dortmund’s usual formation?   —   Bundesliga, 2024  16.   How does PSG typically attack?   —   Ligue 1, 2024  17.   Who has the best defensive record in Serie A?   —   Serie A, 2024  18.   Compare the playing styles of Juventus and Inter Milan   —   Serie A, 2024  19.   Which team has the most clean sheets in La Liga?   —   La Liga, 2024

=== 페이지 109 ===

Benchmarking Queries and Their Target Competitions | 93  20.   What is Atletico Madrid’s main strength?   —   La Liga, 2024  21.   How does Xabi Alonso’s Bayer Leverkusen play?   —   Bundesliga, 2024  22.   Which team has the highest possession in the Champions League?   —   UEFA Champions League, 2024  23.   What is the playing style of Brighton under De Zerbi?   —   Premier League, 2024  24.   Which player has the most assists in the Eredivisie?   —   Eredivisie, 2024  25.   How does Feyenoord compare to Ajax?   —   Eredivisie, 2024  26.   Which team in MLS has the highest goal-scoring average?   —   MLS, 2024  27.   Who has the highest pass-completion rate in Ligue 1?   —   Ligue 1, 2024  28.   Which players should I watch out for in my next match against Atletico Madrid?   —   La Liga, 2024  29.   Analyze the playing style of RB Leipzig in the Bundesliga   —   Bundesliga, 2024  30.   How many goals has Kylian Mbappé scored this season?   —   La Liga, 2024  31.   How does Sporting CP’s high press work?   —   Primeira Liga, 2024  32.   What tactics does Benfica use against Porto?   —   Primeira Liga, 2024  33.   Who is the top scorer in the Championship?   —   Championship, 2024  34.   Which player has the best aerial-duel success in Serie A?   —   Serie A, 2024  35.   Analyze the last El Clásico game   —   La Liga, 2024  36.   Which teams are most likely to be relegated from the Premier League?   —   Premier League, 2024  37.   Who has the highest xA in the Bundesliga?   —   Bundesliga, 2024  38.   Which team has conceded the most penalties in the Champions League?   —   UEFA Champions League, 2024  39.   What is the most common formation used in MLS?   —   MLS, 2024  40.   Analyze how Union Berlin defends against counter-attacks   —   Bundesliga, 2024  41.   Which players are most involved in build-up play for AC Milan?   —   Serie A, 2024  42.   Who has the most interceptions in La Liga?   —   La Liga, 2024  43.   What is the average possession of Barcelona this season?   —   La Liga, 2024  44.   Gör en motståndaranalys av Arsenal 2024   —   Premier League, 2024  45.   Analysera Manchester United   —   Premier League, 2024  46.   Hur spelar Manchester United?   —   Premier League, 2024  47.   Analysera Real Madrids spelstil i Champions League 24   —   UEFA Champions League, 2024  48.   Gör en matchanalys av Newcastle United vs Nottingham Forrest   —   Newcastle – N Forrest (team)  49.   Vem har högst xG90 i Svenska Cupen 2025?   —   Svenska Cupen Herr, 2025

=== 페이지 110 ===

94 | Benchmarking Queries and Their Target Competitions  50.   Vem har gjort flest mål i Svenska Cupen?   —   Svenska Cupen Herr, 2025  51.   Vilken spelare har gjort flest mål i Ligue 1?   —   Ligue 1, 2024  52.   Vilken spelare har spelat flest minuter i Ligue 1?   —   Ligue 1, 2024  53.   Analysera den senaste matchen mellan Newcastle och Nottingham   —   Newcastle – N Forrest (team)  54.   Vilken spelare gjorde flest mål i matchen mellan Newcastle och Nottingham?   —   Newcastle – N Forrest  55.   Hur spelar Napoli sin fotboll?   —   Serie A, 2024  56.   Hjälp mig analysera min nästa match mot Bayern Munich   —   Bundesliga, 2024  57.   Vilken liga spelar FC St. Pauli i?   —   Bundesliga, 2024  58.   Vilken formation använder Borussia Dortmund oftast?   —   Bundesliga, 2024  59.   Hur anfaller PSG vanligtvis?   —   Ligue 1, 2024  60.   Vilket lag har bäst defensivt facit i Serie A?   —   Serie A, 2024  61.   Jämför spelstilarna mellan Juventus och Inter Milan   —   Serie A, 2024  62.   Vilket lag har flest hållna nollor i La Liga?   —   La Liga, 2024  63.   Vad är Atlético Madrids största styrka?   —   La Liga, 2024  64.   Hur spelar Xabi Alonsos Bayer Leverkusen?   —   Bundesliga, 2024  65.   Vilket lag har mest bollinnehav i Champions League?   —   UEFA Champions League, 2024  66.   Hur spelar Brighton under De Zerbi?   —   Premier League, 2024  67.   Vilken spelare har flest assist i Eredivisie?   —   Eredivisie, 2024  68.   Hur jämför sig Feyenoord med Ajax?   —   Eredivisie, 2024  69.   Vilket lag i MLS har högst målsnitt?   —   MLS, 2024  70.   Vem har högst passningssäkerhet i Ligue 1?   —   Ligue 1, 2024  71.   Vilka spelare bör jag se upp för i min nästa match mot Atlético Madrid?   —   La Liga, 2024  72.   Analysera RB Leipzigs spelstil i Bundesliga   —   Bundesliga, 2024  73.   Hur många mål har Kylian Mbappé gjort denna säsong?   —   La Liga, 2024  74.   Hur fungerar Sporting CP:s höga press?   —   Primeira Liga, 2024  75.   Vilken taktik använder Benfica mot Porto?   —   Primeira Liga, 2024  76.   Vem är den bästa målskytten i Championship?   —   Championship, 2024  77.   Vilken spelare har bäst statistik i luftdueller i Serie A?   —   Serie A, 2024  78.   Analysera den senaste El Clásico-matchen   —   La Liga, 2024  79.   Vilka lag ligger mest riskerade att åka ur Premier League?   —   Premier League, 2024

=== 페이지 111 ===

Benchmarking Queries and Their Target Competitions | 95  80.   Vem har högst expected assists (xA) i Bundesliga?   —   Bundesliga, 2024  81.   Vilket lag har orsakat flest straffar i Champions League?   —   UEFA Champions League, 2024  82.   Vilken är den vanligaste formationen i MLS?   —   MLS, 2024  83.   Analysera hur Union Berlin försvarar sig mot kontringar   —   Bundesliga, 2024  84.   Vilka spelare är mest involverade i uppbyggnadsspelet för AC Milan?   —   Serie A, 2024  85.   Vem har flest brytningar i La Liga?   —   La Liga, 2024  86.   Hur stort bollinnehav har Barcelona i genomsnitt denna säsong?   —   La Liga, 2024

=== 페이지 112 ===

96 | System Instructions  Appendix D System Instructions  Listing D.1: System Instruction for OpenAI Assistant  1   """  2   Role & Purpose:  3   You are an analytic football data chatbot for PlaymakerAI. Provide precise, data-driven responses, with insights tailored for football analytics beginners.  4 5   Behavior & Response Guidelines:  6   Maintain a neutral, analytic tone. Only offer opinions on football analytics if explicitly asked and fact-based from data.  7   Explain all KPIs and football terms simply for beginners: define them, what they measure, and their relevance. Prioritize definitions from provided data/documents: if a common term/KPI definition is missing, use your general knowledge.  8   Call available functions when appropriate for user queries, ensuring only one function call is made per user request. Avoid generalizations; provide only verifiable insights from the query ' s specific data. Clearly state any inferred values or assumptions.  9 10   Data Analysis & KPI Handling (for detailed opponent/match analyses): KPI Guidance: If provided data includes a positional_kpi_guide (maps positions to recommended KPIs), use it. For each player, identify their position (e.g., from a   ' Position '   field) and use the guide to focus on these recommended KPIs. Outlier Detection: Crucially, also review all other statistics in the dataset. Identify and discuss any noteworthy outlier KPIs (e.g., exceptionally high/low, or otherwise significant) that are not in the focus guide. Data Discrepancies: If the main entities in provided data clearly mismatch the user ' s explicit request, state this and avoid detailed analysis of the mismatched data. If a league/cup is indicated as system-defaulted, proceed with analysis but clearly acknowledge the default.  11   Opinions & Speculation Handling: Clearly distinguish speculation from facts.  12   Do not express opinions on topics outside of football analytics.  13 14   Response Formatting: Deliver structured, concise, technical responses with minimal filler; use tables/bullet points for clarity.For comparisons, present statistical differences. Offer simple interpretations for beginners unless only raw data is requested.  15 16   Core Principle: Prioritize precision. Base analysis strictly on provided query data. Avoid unsupported assumptions.  17   """

=== 페이지 113 ===

Data Provided | 97  Appendix E Data Provided  Listing E.1: Anonymised match-level data  [ { "aggregate": true, "data": [ { "Aerials": 1, "Assists": 1, "Avatar": "Creator", "CarryProgDist": 71.2, "ChallengePct": 0.95, "Dribbles": 3, "Game Units": 0.88, "Games": 1, "Goals": 0, "Interceptions": 4, "Minutes": 85, "Nationality": "Sweden", "PassPct": 0.80, "PassProgDist": 262.1, "Passes": 37, "Player": "Player A", "Pos": "LW", "Pos Group": "OW", "ShotAssists": 2, "Tackles": 2, "Team": "Team 1", "XTAssisted": 0.90, "XTReceived": 0.04, "id": 0, "index": 0, "xG": 0.10, "xT": 0.16 },{} ], "headers": [ { "field": "Player", "headerName": "Player", "resizable": true, "type": "string" }, { "field": "Team", "headerName": "Team", "resizable": true, "type": "string" }, { "field": "Games", "headerName": "Games", "resizable": true, "type": "number" },

=== 페이지 114 ===

98 | Data Provided  { "field": "Minutes", "headerName": "Minutes", "resizable": true, "type": "number" }, { "field": "Game Units", "headerName": "Game Units", "resizable": true, "type":  , →   "number" }, { "field": "Pos", "headerName": "Pos", "resizable": true, "type": "string" }, { "field": "Pos Group", "headerName": "Pos Group", "resizable": true, "type":  , →   "string" }, { "field": "Avatar", "headerName": "Avatar", "resizable": true, "type": "string" }, { "field": "Nationality", "headerName": "Nationality", "resizable": true, "type":  , →   "string" }, { "field": "Aerials", "headerName": "Aerials", "resizable": true, "type": "number" }, { "field": "Assists", "headerName": "Assists", "resizable": true, "type": "number" }, { "field": "CarryProgDist", "headerName": "CarryProgDist", "resizable": true, "type":  , →   "number" }, { "field": "ChallengePct", "headerName": "ChallengePct", "resizable": true, "type":  , →   "number" }, { "field": "Dribbles", "headerName": "Dribbles", "resizable": true, "type": "number" }  , →   , { "field": "Goals", "headerName": "Goals", "resizable": true, "type": "number" }, { "field": "Interceptions", "headerName": "Interceptions", "resizable": true, "type":  , →   "number" }, { "field": "Passes", "headerName": "Passes", "resizable": true, "type": "number" }, { "field": "PassPct", "headerName": "PassPct", "resizable": true, "type": "number" }, { "field": "PassProgDist", "headerName": "PassProgDist", "resizable": true, "type":  , →   "number" }, { "field": "ShotAssists", "headerName": "ShotAssists", "resizable": true, "type":  , →   "number" }, { "field": "Tackles", "headerName": "Tackles", "resizable": true, "type": "number" }, { "field": "xG", "headerName": "xG", "resizable": true, "type": "number" }, { "field": "xT", "headerName": "xT", "resizable": true, "type": "number" }, { "field": "XTAssisted", "headerName": "XTAssisted", "resizable": true, "type":  , →   "number" }, { "field": "XTReceived", "headerName": "XTReceived", "resizable": true, "type":  , →   "number" } ], "name": "Team 1 vs. Team 2", "seasons": [], "status": "ready", "type": "player", "updated": "DateTime", "dataset_id": "XXX" } ]  Listing E.2: Anonymised league-level data  [ { "aggregate": true, "data": [ { "Aerials90Padj": 2.4, "Age": 25, "Avatar": "Ball Playing CB", "CarryProgDist90": 66.3, "Chall90Padj": 1.10, "Game Units": 24.0, "Games": 28, "Goals90": 0.02, "Int90Padj": 11.8,

=== 페이지 115 ===

Data Provided | 99  "MeanKPIScore": 0.56, "Minutes": 2410, "Nationality": "Sweden", "PassProgDist90": 530.8, "Player": "Player C", "PlaymakerRank": 4, "Pos": "CB", "Pos Group": "CB", "ShotAssists90": 0.22, "Team": "Team 3", "XTReceived90": -0.08, "id": 0, "index": 0, "xG90": 0.02, "xT90": 0.15, "xTPrevented90Padj": 0.45 }, {} ], "headers": [ { "field": "Player", "headerName": "Player", "resizable": true, "type": "string" }, { "field": "Games", "headerName": "Games", "resizable": true, "type": "number" }, { "field": "Minutes", "headerName": "Minutes", "resizable": true, "type": "number" }, { "field": "Game Units", "headerName": "Game Units", "resizable": true, "type":  , →   "number" }, { "field": "Pos", "headerName": "Pos", "resizable": true, "type": "string" }, { "field": "Pos Group", "headerName": "Pos Group", "resizable": true, "type":  , →   "string" }, { "field": "Avatar", "headerName": "Avatar", "resizable": true, "type": "string" }, { "field": "Nationality", "headerName": "Nationality", "resizable": true, "type":  , →   "string" }, { "field": "PlaymakerRank", "headerName": "PlaymakerRank", "resizable": true, "type":  , →   "number" }, { "field": "Age", "headerName": "Age", "resizable": true, "type": "number" }, { "field": "Goals90", "headerName": "Goals90", "resizable": true, "type": "number" }, { "field": "CarryProgDist90", "headerName": "CarryProgDist90", "resizable": true,  , →   "type": "number" }, { "field": "PassProgDist90", "headerName": "PassProgDist90", "resizable": true,  , →   "type": "number" }, { "field": "ShotAssists90", "headerName": "ShotAssists90", "resizable": true, "type":  , →   "number" }, { "field": "Int90Padj", "headerName": "Int90Padj", "resizable": true, "type":  , →   "number" }, { "field": "Chall90Padj", "headerName": "Chall90Padj", "resizable": true, "type":  , →   "number" }, { "field": "xTPrevented90Padj", "headerName": "xTPrevented90Padj", "resizable": true,  , →   "type": "number" }, { "field": "Aerials90Padj", "headerName": "Aerials90Padj", "resizable": true, "type":  , →   "number" }, { "field": "XTReceived90", "headerName": "XTReceived90", "resizable": true, "type":  , →   "number" }, { "field": "xT90", "headerName": "xT90", "resizable": true, "type": "number" }, { "field": "xG90", "headerName": "xG90", "resizable": true, "type": "number" }, { "field": "MeanKPIScore", "headerName": "MeanKPIScore", "resizable": true, "type":  , →   "number" }, { "field": "Team", "headerName": "Team", "resizable": true, "type": "string" } ], "name": "League 2024", "seasons": ["League 2024"],

=== 페이지 116 ===

100 | Data Provided  "status": "ready", "type": "player", "updated": "DateTime", "dataset_id": "XXX" } ]

=== 페이지 117 ===

UAT Form | 101  Appendix F UAT Form

=== 페이지 118 ===

T esting the Chatbot  T h a n k s   f o r   c o m p l e t i n g   t h e   fi r s t   q u e s t i o n n a i r e .   N o w   w e ' l l   m o v e   o n   t o   i n t e r a c t i n g   w i t h   t h e   f o o t b a l l   c h a t b o t .  1.   Y o u r   G o a l :   I ' l l   g i v e   y o u   a   s e r i e s   o f   t a s k s   i n   w r i t i n g ,   o n e   a t   a   t i m e .   P l e a s e   u s e   t h e   c h a t b o t   t o   c o m p l e t e   e a c h   t a s k   a s   y o u  n o r m a l l y   w o u l d .  2.   T h i n k   A l o u d :   A s   y o u   d o   t h e   t a s k s ,   p l e a s e   t h i n k   o u t   l o u d   –   t e l l   m e   w h a t   y o u ' r e   t r y i n g   t o   d o ,   w h a t   y o u   e x p e c t ,   a n d   w h a t  y o u ' r e   s e e i n g   o r   t h i n k i n g .   T h i s   h e l p s   m e   u n d e r s t a n d   y o u r   e x p e r i e n c e .   I f   y o u   g o   q u i e t ,   I   m i g h t   r e m i n d   y o u .  3.   M y   R o l e :   I ' l l   b e   o b s e r v i n g   a n d   t a k i n g   n o t e s .   I   w o n ' t   g u i d e   y o u   u n l e s s   y o u   g e t   c o m p l e t e l y   s t u c k ,   a s   w e   w a n t   t o   s e e   y o u r  n a t u r a l   i n t e r a c t i o n .  4.   N o   R i g h t / W r o n g :   R e m e m b e r ,   w e ' r e   t e s t i n g   t h e   c h a t b o t ,   n o t   y o u .   T h e r e ' s   n o   t i m e   l i m i t ,   s o   w o r k   a t   y o u r   o w n   p a c e .  W e ' l l   u s e   t h e   c h a t b o t   o n   t h i s   c o m p u t e r .  A n y   q u e s t i o n s   b e f o r e   w e   s t a r t   t h e   fi r s t   t a s k ?  T ask 1: Football Metrics  E N :   P l e a s e   u s e   t h e   c h a t b o t   t o   d o   t h e   f o l l o w i n g :  1.   A s k   t h e   c h a t b o t   t o   e x p l a i n   w h a t   o n e   o f   t h e   K P I s   b e l o w   m e a n s   i n   f o o t b a l l .  2.   A f t e r   y o u   r e c e i v e   a n   e x p l a n a t i o n ,   a s k   t h e   c h a t b o t   f o r   a   s i m p l e r   o r   m o r e   b a s i c   e x p l a n a t i o n   o f   t h e   s a m e   c o n c e p t .  S V :   A n v ä n d   c h a t b o t e n   f ö r   a t t   g ö r a   f ö l j a n d e :  1.   Be   c h a t b o t e n   f ö r k l a r a   v a d   e n   a v   n e d a n s t å e n d e   K P I   ä r   f ö r   n å g o t   i n o m   f o t b o l l .  2.   E f t e r   a t t   d u   h a r   f å t t   e n   f ö r k l a r i n g ,   b e   c h a t b o t e n   o m   e n   e n k l a r e   e l l e r   m e r   g r u n d l ä g g a n d e   f ö r k l a r i n g   a v   s a m m a   b e g r e p p .  L i s t   o f   KP I ' s   t h a t   y o u   c a n   u s e   ( i f   y o u   k n o w   o t h e r s ,   t r y   t h e m   o u t ) :  -   K e e p   Ba l l   %,   L o n g   Ba l l   C o n t r o l ,   M e a n   K P I   S c o r e ,   P a s s e s   A   U n d e r   P r e s s u r e   P G ,   S a f e t i e s   P G ,   S h o t   A s s i s t s   i n   A Z   P G ,   x T ,   x G  T ask 2: Scout Inquiry  E N :   I m a g i n e   y o u   a r e   a   f o o t b a l l   s c o u t .   U s e   t h e   c h a t b o t   t o   a s k   w h i c h   K e y   P e r f o r m a n c e   I n d i c a t o r s   (K P I s )   a r e   g e n e r a l l y   i m p o r t a n t   t o  l o o k   a t   f o r   a n   a t t a c k i n g / d e f e n s i v e   p l a y e r   o r   o t h e r   p l a y e r   t y p e .  S V :   F ö r e s t ä l l   d i g   a t t   d u   ä r   e n   f o t b o l l s s c o u t .   A n v ä n d   c h a t b o t e n   f ö r   a t t   f r å g a   v i l k a   n y c k e l t a l   (K P I : e r )   s o m   g e n e r e l l t   s e t t   ä r   v i k t i g a   a t t  t i t t a   p å   f ö r   e n   a n f a l l s s p e l a r e / f ö r s v a r s p e l a r e   e l l e r   e n   a n n a n   t y p .  Evaluation of Chatbot  T h a n k   y o u   f o r   t a k i n g   t i m e .   By   a n s w e r i n g   a n d   t a k i n g   p a r t   o f   t h i s   e v a l u a t i o n   y o u   c o n s e n t   t o   t h e   d a t a   f r o m   t h i s   e v a l u a t i o n  b e i n g   p u b l i s h e d   o n l i n e   a n d   y o u r   a n s w e r s   w i l l   b e   a n o n y m i s e d .

=== 페이지 119 ===

T ask 3: Opponent analysis  E N :   I m a g i n e   y o u   a r e   a   s c o u t   f o r   a   t e a m   t h a t   p l a y s   i n   e i t h e r   A l l s v e n s k a n   o r   t h e   P r e m i e r   L e a g u e .   Y o u r   t a s k   i s   t o   u s e   t h e   c h a t b o t   t o  g e t   a n   o p p o n e n t   a n a l y s i s .   T h e   s p e c i fi c   o p p o n e n t   i s   t h e   t e a m   c u r r e n t l y   t w o   p l a c e s   b e l o w   y o u r   t e a m   i n   t h e   l e a g u e   t a b l e .  I f   t h e r e   i s n ' t   a   t e a m   e x a c t l y   t w o   p l a c e s   b e l o w ,   p l e a s e   a s k   f o r   a n   a n a l y s i s   o f   t h e   t e a m   t h a t   i s   o n e   p l a c e   a b o v e   y o u r   t e a m   i n  t h e   t a b l e   i n s t e a d .  Y o u   a r e   e x p e c t i n g   t h i s   a n a l y s i s   p r i m a r i l y   a s   a   d a s h b o a r d   r e s p o n s e .  S V :   F ö r e s t ä l l   d i g   a t t   d u   ä r   e n   s c o u t   f ö r   e t t   l a g   s o m   s p e l a r   i   a n t i n g e n   A l l s v e n s k a n   e l l e r   P r e m i e r   L e a g u e .   D i n   u p p g i f t   ä r   a t t   a n v ä n d a  c h a t b o t e n   f ö r   a t t   f å   e n   m o t s t å n d a r a n a l y s .   M o t s t å n d a r e n   d u   s k a   a n a l y s e r a   ä r   d e t   l a g   s o m   f ö r   n ä r v a r a n d e   l i g g e r   t v å   p l a c e r i n g a r  u n d e r   d i t t   l a g   i   t a b e l l e n .  O m   d e t   i n t e   fi n n s   n å g o t   l a g   e x a k t   t v å   p l a c e r i n g a r   u n d e r ,   b e   i s t ä l l e t   o m   e n   a n a l y s   a v   d e t   l a g   s o m   l i g g e r   e n   p l a c e r i n g   o v a n f ö r  d i t t   l a g   i   t a b e l l e n .  D u   f ö r v ä n t a r   d i g   a t t   s e   d e n n a   a n a l y s   p r i m ä r t   s o m   e t t   s v a r   i   d a s h b o a r d - f o r m a t  T ask 4: Opponent analysis  E N :  1.   C h o o s e   a n y   t e a m   t h a t   p l a y s   i n   A l l s v e n s k a n .  2.   U s e   t h e   c h a t b o t   t o   a s k   f o r   a n   a n a l y s i s   o f   t h i s   t e a m ' s   p l a y i n g   s t y l e .   S p e c i f y   t h a t   y o u   w a n t   t o   r e c e i v e   t h e   a n s w e r   b o t h   a s  a   d a s h b o a r d   v i e w   o c h   i n   t e x t   f o r m .  3.   O n c e   y o u   h a v e   r e c e i v e d   t h e   a n s w e r ,   a s k   a   r e l e v a n t   f o l l o w - u p   q u e s t i o n   b a s e d   o n   t h e   i n f o r m a t i o n   o r   a n a l y s i s   t h e   c h a t b o t  p r o v i d e d .  S V :  1.   V ä l j   e t t   v a l f r i t t   l a g   s o m   s p e l a r   i   A l l s v e n s k a n .  2.   A n v ä n d   c h a t b o t e n   f ö r   a t t   b e   o m   e n   a n a l y s   a v   d e t t a   l a g s   s p e l s ä t t .   S p e c i fi c e r a   a t t   d u   v i l l   f å   s v a r e t   b å d e   s o m   e n   d a s h b o a r d - v y  o c h   i   t e x t f o r m .  3.   N ä r   d u   h a r   f å t t   s v a r e t ,   s t ä l l   e n   r e l e v a n t   f ö l j d f r å g a   b a s e r a t   p å   d e n   i n f o r m a t i o n   e l l e r   a n a l y s   s o m   c h a t b o t e n   g a v .  T ask 5: Opponent analysis  E N :   F o r   a n y   t e a m ,   n o w   a s k   t h e   c h a t b o t   i f   i t ' s   p o s s i b l e   t o   g e t   a n   o p p o n e n t   a n a l y s i s   o n l y   i n   t e x t   f o r m ,   w i t h o u t   a n y   d a s h b o a r d .  S V :   F ö r   v a l f r i t t   l a g ,   f r å g a   n u   c h a t b o t e n   o m   d e t   ä r   m ö j l i g t   a t t   f å   e n   m o t s t å n d a r a n a l y s   e n d a s t   i   t e x t f o r m ,   u t a n   n å g o n   d a s h b o a r d .

=== 페이지 120 ===

T ask 6: Match analysis  E N :   T h e   c h a t b o t   h a s   a c c e s s   t o   d a t a   f r o m   a   n u m b e r   o f   m a t c h e s .   U s e   t h e   c h a t b o t   t o   p e r f o r m   a n   a n a l y s i s   o f   a n y   m a t c h   f r o m   t h e  a v a i l a b l e   m a t c h e s .   Y o u   a r e   p r i m a r i l y   e x p e c t i n g   a   d a s h b o a r d   r e s p o n s e .  S V :   C h a t b o t e n   h a r   t i l l g å n g   t i l l   d a t a   f r å n   e t t   a n t a l   m a t c h e r .   A n v ä n d   c h a t b o t e n   f ö r   a t t   g ö r a   e n   a n a l y s   a v   e n   v a l f r i   m a t c h   f r å n   d e  t i l l g ä n g l i g a   m a t c h e r n a .   D u   f ö r v ä n t a r   d i g   f r ä m s t   e t t   s v a r   i   d a s h b o a r d - f o r m a t .  L i s t   o f   m a t c h e s   w e   h a v e   a v a i l a b l e   a t   t h i s   t i m e   ( w h i c h   y o u   c a n   c h o o s e   f r o m ) :  -   F i o r e n t i n a   -   N a p o l i  -   V i l l a r r e a l   -   V a l e n c i a  -   M o n a c o - Br e s t  -   W e s t   H a m   U n i t e d   v s   N e w   C a s t l e  -   Bo r u s s i a   M ö n c h e n g l a d b a c h   v s .   S t u t t g a r t  -   G ö t e b o r g   v s .   A I K  -   R e n n e s   -   P S G  -   J u v e n t u s   v s .   A C   M i l a n  -   M a l m ö   F F   v s .   H a m m a r b y  -   L i v e r p o o l   v s .   M a n   C i t y  -   R e a l   M a d r i d   -   Ba r c e l o n a  -   Ba y e r n   M u n i c h   v s .   R B   L e i p z i g  -   N a p o l i   v s .   I n t e r   M i l a n  -   W o l v e s   v s .   M a n c h e s t e r   C i t y  -   N e w c a s t l e   -   N   F o r r e s t   (t e a m )  -   N e w c a s t l e   -   N   F o r r e s t  T ask 7: Focused match analysis  E N :   N o w ,   p e r f o r m   a n o t h e r   m a t c h   a n a l y s i s   o f   a   d i f f e r e n t   a v a i l a b l e   m a t c h .   T h i s   t i m e ,   a s k   t h e   c h a t b o t   t o   f o c u s   i t s   a n a l y s i s   o n   a  s p e c i fi c   a s p e c t ,   f o r   e x a m p l e ,   a   p a r t i c u l a r   KP I ,   a   t e a m ' s   d e f e n s i v e   p l a y ,   o r   s o m e t h i n g   s i m i l a r .   S p e c i f y   t h a t   y o u   w a n t   t o   r e c e i v e   t h e  a n s w e r   b o t h   a s   a   d a s h b o a r d   v i e w   a n d   i n   t e x t   f o r m .  S V :   G ö r   n u   y t t e r l i g a r e   e n   m a t c h a n a l y s   a v   e n   a n n a n   v a l f r i   t i l l g ä n g l i g   m a t c h .   D e n n a   g å n g ,   b e   c h a t b o t e n   a t t   f o k u s e r a   s i n   a n a l y s   p å  e n   s p e c i fi k   a s p e k t ,   t i l l   e x e m p e l   e t t   v i s s t   KP I ,   e t t   l a g s   f ö r s v a r s s p e l ,   e l l e r   l i k n a n d e .   S p e c i fi c e r a   a t t   d u   v i l l   f å   s v a r e t   b å d e   s o m  e n   d a s h b o a r d - v y   o c h   i   t e x t f o r m .  L i s t   o f   m a t c h e s   w e   h a v e   a v a i l a b l e   a t   t h i s   t i m e   ( w h i c h   y o u   c a n   c h o o s e   f r o m ) :   S a m e   a s   a b o v e  Det här innehållet har v ark en skapats eller godkänts a v Google. F o r m u l ä r

=== 페이지 121 ===

Full comparison of responses | 105  Appendix G Full comparison of responses  Below follows a section for a specific user query, all questions taken from the UAT. Then two subsections follows, the PlaymakerAI chatbots answer and the baseline LLM that the PlaymakerAI is built upon.  G.1   "What is xG?"  G.1.1   PlaymakerAI  xG, or Expected Goals, is a statistical metric used in football to measure the quality of goal-scoring chances. It estimates the likelihood that a shot will result in a goal based on various factors. Here’s a simple breakdown:  , →  , →  ### Key Components of xG: - Shot Location: Where the shot is taken from on the pitch. - Type of Shot: Whether it ' s a header, volley, or a shot with the foot. - Defensive Pressure: How many defenders are near the shooter. - Angle of the Shot: The angle at which the shot is taken relative to the goal. ### What xG Measures: - Quality of Chances: Each shot is assigned an xG value between 0 and 1, where 0 means no chance of scoring and 1 means a certain goal. For example, a close-range shot might have an xG of 0.8, indicating an 80% chance of scoring.  , →  , →  - Team and Player Performance: By analyzing xG, analysts can assess how well a team or player is converting chances into goals compared to what would be expected based on the quality of those chances.  , →  , →  ### Relevance: - Performance Analysis: xG helps in understanding whether a team is overperforming or underperforming based on the chances they create and convert. , →  - Strategic Insights: Coaches and analysts use xG to improve tactics and player positioning by focusing on high-quality scoring opportunities. , →  In summary, xG is a valuable tool for analyzing goal-scoring efficiency and overall performance in football. , →

=== 페이지 122 ===

106 | Full comparison of responses  G.1.2   GPT-4-Turbo  xG stands for "expected goals." It is a statistical measure used in soccer (football) to quantify the likelihood of a shot resulting in a goal. This metric evaluates the quality of a scoring opportunity, considering factors such as the type of assist, the angle of the shot, the distance from the goal, and whether it was a header or a shot with the foot.  , →  , →  , →  The xG value is typically a number between 0 and 1, where 0 indicates no chance of scoring and 1 means a certain goal. For example, a shot with an xG of 0.3 suggests that, on average, the shot would be expected to result in a goal 30% of the time.  , →  , →  Expected goals can be used to assess team and player performance beyond traditional metrics like goals scored. It helps in understanding how many goals a team or player should have scored on average, given the shots they took, providing a deeper insight into the effectiveness of their attacking play. It is also used for predictive analysis, helping to forecast future performance based on the quality of chances created and conceded.  , →  , →  , →  , →  G.2   "Create   an   opponent   analysis   of   Real Madrid in the champions league"  G.2.1   PlaymakerAI  ### Opponent Analysis: Real Madrid CF Overview: This analysis focuses on Real Madrid CF, one of the most successful football clubs in history. The insights will help understand their playing style, strengths, weaknesses, and key player performances in the context of their recent matches.  , →  , →  ### Team Style and Key Strengths: - Attacking Play: Real Madrid is known for its fast-paced attacking style, often utilizing quick transitions from defense to offense. They rely on skilled players to create goal-scoring opportunities.  , →  , →  - Defensive Solidity: The team has a strong defensive structure, with players who are capable of winning duels and intercepting passes, which helps in regaining possession quickly. , →  ### Key Performance Indicators (KPIs): Here are some important KPIs used to analyze Real Madrid ' s performance, along with their meanings: , →  | KPI   | Definition   | Importance   | , →  |------------------------- |------------------------- -------------------------- ------------------------|- -------------------------- -------------------------- -------------| | xG (Expected Goals) | Measures the quality of goal-scoring chances based on various factors.   | Indicates how well the team converts chances into goals.   | , →  | xA (Expected Assists)| Estimates the likelihood that a pass will lead to a goal. | Shows the creativity and effectiveness of playmakers.   | , →  | Pass Progression Distance (PassProgDist) | Average distance of passes that move the ball forward.   | Reflects the team ' s ability to advance the ball and create chances.| , →

=== 페이지 123 ===

Full comparison of responses | 107  | Interceptions | Number of times a player intercepts the ball during a match.   | Highlights defensive capabilities and ability to regain possession.| , →  | Goals | Total number of goals scored by a player in a match. | Indicates scoring efficiency of players.   | , →  | Shot Assists | Number of assists a player provides in a match. | Measures a player ' s contribution to creating scoring opportunities. | , →  ### Player Analysis: Here are some notable players from Real Madrid, focusing on their roles and performance metrics: , →  1. Antonio Rudiger (Center Back) - Interceptions: 4 - Pass Progression Distance: 180.46 m - Tackles: 1 - Strengths: Strong in defensive actions, capable of winning aerial duels and making crucial interceptions. , →  2. Vinicius Junior (Left Winger) - xG: 0.09 - Shot Assists: 1 - Pass Progression Distance: 54.18 m - Strengths: Quick and skillful, he is effective in creating chances and can take on defenders. , →  ### Outlier KPIs: - Antonio Rudiger ' s Interceptions (4): This is a notable value, indicating his significant defensive impact and ability to disrupt the opponent ' s play. , →  - Vinicius Junior ' s Shot Assists (1): This shows his role in creating scoring opportunities for teammates, highlighting his importance in the attacking phase. , →  ### Conclusion: Real Madrid CF showcases a blend of attacking flair and defensive resilience. Key players like Antonio Rudiger and Vinicius Junior play vital roles in their respective positions, contributing significantly to both defense and attack. Understanding these metrics provides insight into how Real Madrid operates and can inform strategies for facing them in matches.  , →  , →  , →  , →  G.2.2   GPT-4-Turbo  Creating an opponent analysis for a high-profile team like Real Madrid, especially in a competitive setting like the UEFA Champions League, involves a detailed look at various aspects of their gameplay, tactics, player form, and more. Here’s how you can structure a comprehensive analysis:  , →  , →  , →  1.Team Overview History in the Champions League: Real Madrid has a storied history in the Champions League, being the competition ' s most successful club. Analyzing their historical performance can provide insights into their experience and pedigree in European competitions.  , →  , →  Recent Performance: Look at their performance in the most recent seasons of the Champions League and domestic league (La Liga). Note any trends or changes in their game under the current management.  , →  , →  2.Tactical Analysis Preferred Formation and Style: Real Madrid has traditionally played a 4-3-3 or a 4-2-3-1 formation. They are known for their attacking style of play, emphasizing ball possession and quick transitions.  , →  , →  Strengths: Typically, Real Madrid ' s strengths lie in their attacking prowess, midfield control, and experienced players who can manage game tempo. , →

=== 페이지 124 ===

108 | Full comparison of responses  Weaknesses: Identify potential weaknesses such as pace in the backline, dependency on key players, or how they handle high-press tactics from opponents. , →  Set Pieces: Analyze their effectiveness in both defending and attacking set pieces. 3.Key Players Impact Players: Identify key players like their forwards, midfield maestros, or a particularly influential defender. Understanding the roles of players like Karim Benzema, Luka Modric, and others is crucial.  , →  , →  Potential Weak Links: Any players who might be less consistent or could be targeted defensively. , →  Injuries and Suspensions: Keep track of any key players who might miss upcoming matches, which could affect their usual play. , →  4.Statistical Analysis Goals Scored and Conceded: Review their goal statistics in the current Champions League season and in domestic competition. , →  xG and xGA: These metrics will provide deeper insights into their scoring efficiency and defensive solidity. , →  Possession, Passing Accuracy, and Other Metrics: These stats can help understand their style and efficiency in games. , →  5.Recent Matches Form in Last 5-10 Games: Look at their most recent matches in all competitions to identify any tactical shifts or patterns. , →  Performance Against Similar Opponents: How have they performed against teams that are similar in quality or style to your team? , →  6.Psychological Factors Mental Toughness: Real Madrid is known for their comeback ability and performing under pressure, particularly in knockout stages. , →  Home vs. Away: Assess their performance variations between home and away games. 7.Fan and Media Perspective Fan Support: The Bernabéu is a fortress with strong fan support that can influence performances. , →  Media Coverage: Media narratives can sometimes impact team morale or pressure. Tools and Resources Video Analysis Software: Utilize tools like Wyscout or Hudl for detailed game footage analysis. , →  Statistical Databases: Websites like Opta, SofaScore, or WhoScored offer extensive statistical data and player analytics. , →  Local Reports and News: Following local sports journalists and fan forums can provide insider insights and updates. , →  This structured analysis will help in understanding Real Madrid ' s current form, tactics, and potential vulnerabilities, aiding in tactical planning for facing them in the Champions League.  , →  , →  G.3   "can you give me a match analysis for the latest game between Rennes-PSG."  G.3.1   PlaymakerAI  ### Match Analysis: Rennes vs. Paris Saint-Germain (PSG) Match Confirmation: Both teams were found in the dataset: "Paris Saint-Germain, Rennes". ### Detailed Match Analysis 1. Player Data & Scores:

=== 페이지 125 ===

Full comparison of responses | 109  In football, scores are derived from the goals scored by players during the match. Each goal is credited to the player who scores it, and assists are given to players who provide the final pass leading to a goal.  , →  , →  2. Final Score & Key Contributors: - Final Score: Paris Saint-Germain 4 - 1 Rennes - Key Goal Scorers: - Paris Saint-Germain: - Ousmane Dembele: 2 goals - Bradley Barcola: 1 goal - Goncalo Ramos: 1 goal - Rennes: - Lilian Brassier: 1 goal 3. Key Metrics Analysis: - Expected Goals (xG): - Paris Saint-Germain: 4.0 (indicating they had high-quality chances that were likely to result in goals) , →  - Rennes: 1.0 (indicating fewer quality chances) What is xG?: Expected Goals (xG) measures the quality of goal-scoring chances based on factors like shot location and type. It helps to understand how well a team converts chances into actual goals.  , →  , →  - Expected Threat (xT): - Paris Saint-Germain: 0.31 - Rennes: 0.12 What is xT?: Expected Threat (xT) measures the potential of a pass to lead to a goal-scoring opportunity. It reflects how dangerous a team ' s passing is during the match. , →  4. Overall Game Summary: In this match, Paris Saint-Germain showcased their attacking prowess, scoring four goals. Ousmane Dembele was particularly effective, scoring twice and contributing to the overall offensive strategy. Rennes managed to score once, but their chances were limited compared to PSG ' s. The match highlighted PSG ' s ability to create high-quality chances, as reflected in their xG, while Rennes struggled to match that intensity.  , →  , →  , →  , →  ### Conclusion: This match analysis illustrates how Paris Saint-Germain dominated the game, both in terms of goals scored and the quality of chances created. Understanding metrics like xG and xT provides insight into the effectiveness of each team ' s performance and can help in evaluating future matches.  , →  , →  , →  G.3.2   GPT-4-Turbo  As of my last update in December 2023, I don ' t have access to real-time data, including specific match results or detailed events from recent games such as Rennes vs. PSG. However, I can guide you on how to conduct a post-match analysis using general principles and what to look for in analyzing a football match. Here’s how you can structure your analysis:  , →  , →  , →  , →  1.Pre-Match Context League Position and Form: Review both teams '   positions in the league and their form leading up to the match. , →  Head-to-Head: Look at recent meetings between the two teams to gauge any psychological advantages or recurring tactical themes. , →  Lineups and Formations: Note any surprising selections or tactical setups that could influence the game ' s outcome. , →

=== 페이지 126 ===

110 | Full comparison of responses  2.Match Events Goals: Analyze the build-up and execution of each goal. Look at defensive setups and any individual errors or outstanding plays. , →  Key Incidents: This includes penalties, red cards, VAR decisions, and injuries that significantly impacted the match. , →  Substitutions: Consider the timing and impact of substitutions. Did they change the game ' s flow or tactical setup? , →  3.Statistical Analysis Possession: Which team dominated possession, and how did this affect the game ' s outcome? Shots and On-Target: Look at the total shots and shots on target for insights into each team ' s attacking effectiveness. , →  Passing Accuracy: High passing accuracy might indicate better control of the game. Chances Created and xG (Expected Goals): These metrics help assess how effectively teams created scoring opportunities. , →  4.Tactical Analysis Defensive and Offensive Strategies: Analyze how each team approached defense and attack. Did PSG press high, or did Rennes use counter-attacks? , →  Midfield Control: Which team controlled the midfield area? How did this influence the game ' s dynamics? , →  Width and Depth: How did teams use the width of the pitch? Were there significant overlaps or underlaps by fullbacks? , →  5.Individual Performances Standout Players: Identify players who had a significant impact on the game, positively or negatively. , →  Weak Links: Note any players who struggled, either defensively or in contributing to the attack. , →  6.Post-Match Reactions Manager Comments: What did the managers say post-match? Their insights can reveal what they thought were the decisive factors. , →  Player Interviews: Players might provide additional perspectives on key moments or tactical instructions. , →  Media Analysis: Media often highlights critical tactical points or turning moments in the match. , →  7.Visual Tools Heat Maps: These can show player movements and concentrations on the pitch. Pass Maps: Analyze the passing networks and link-ups between players. Action Zones: Review which areas of the pitch saw the most action. Resources for Data and Analysis Football Analysis Websites: Platforms like WhoScored, SofaScore, and Opta provide detailed match statistics and player analytics. , →  Video Highlights: Watching highlights can help visualize goals and key moments. Tactical Analysis Blogs and Videos: These can offer deeper insights into the tactical aspects of the game. , →  By following this structure, you can create a comprehensive analysis of the Rennes vs. PSG match, even without specific details from the latest encounter. This approach helps in understanding not just what happened, but why it happened.  , →  , →

=== 페이지 127 ===



=== 페이지 128 ===

TRITA   –   EECS-EX   202 5:471  Stockholm,   Sweden   2025 www.kth.se