{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e559a3d",
   "metadata": {},
   "source": [
    "# KBO ì•¼êµ¬ íŠ¹í™” ì±—ë´‡ \n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë…¼ë¬¸ \"A Chatbot for Football Analytics\" ì•„í‚¤í…ì²˜ë¥¼ KBO ì•¼êµ¬ì— ì ìš©í•œ ì±—ë´‡ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n",
    "1. **ë°ì´í„° ì ì¬ (Ingest)**: JSON â†’ ChromaDB ë²¡í„° ì €ì¥ì†Œ\n",
    "2. **ì¿¼ë¦¬ ë¶„ë¥˜ (Classifier)**: general / season_analysis / match_analysis\n",
    "3. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Retriever)**: Semantic + BM25\n",
    "4. **ì‘ë‹µ ìƒì„± (Chain)**: LLM ê¸°ë°˜ ë¶„ì„\n",
    "5. **ì—ì´ì „íŠ¸ (Agent)**: Function Calling ì§€ì›\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9451a4e",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "\n",
    "í™˜ê²½ ë³€ìˆ˜ë¥¼ `.env` íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê³ , API í‚¤ê°€ ì•ˆì „í•˜ê²Œ ê´€ë¦¬ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cbed6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸: c:\\Coding\\PAINS-LLM\\PAINS-LLM\n",
      "âœ… OPENAI_API_KEY ë¡œë“œ: ì„±ê³µ âœ“\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# API í‚¤ í™•ì¸ (ê°’ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ - ë³´ì•ˆ)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}\")\n",
    "print(f\"âœ… OPENAI_API_KEY ë¡œë“œ: {'ì„±ê³µ âœ“' if api_key else 'ì‹¤íŒ¨ âœ— (.env íŒŒì¼ í™•ì¸ í•„ìš”)'}\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"\\nâš ï¸ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”:\")\n",
    "    print(\"   OPENAI_API_KEY=sk-your-api-key-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8244469c",
   "metadata": {},
   "source": [
    "## 2. í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡° ë¶„ì„\n",
    "\n",
    "í˜„ì¬ í”„ë¡œì íŠ¸ì˜ íŒŒì¼ êµ¬ì¡°ë¥¼ íŠ¸ë¦¬ í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46eef580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°:\n",
      "\n",
      "â”œâ”€â”€ data\n",
      "â”‚   â””â”€â”€ raw\n",
      "â”‚       â”œâ”€â”€ matches\n",
      "â”‚       â”‚   â”œâ”€â”€ 2025_POST_MATCH_PITCHING_STATS.json\n",
      "â”‚       â”‚   â””â”€â”€ 2025_REGULAR_MATCH_PITCHING_STATS.json\n",
      "â”‚       â””â”€â”€ seasons\n",
      "â”‚           â”œâ”€â”€ 2025_POST_PITCHING_STATS.json\n",
      "â”‚           â””â”€â”€ 2025_REGULAR_PITCHING_STATS.json\n",
      "â”œâ”€â”€ src\n",
      "â”‚   â”œâ”€â”€ __init__.py\n",
      "â”‚   â”œâ”€â”€ agent.py\n",
      "â”‚   â”œâ”€â”€ chain.py\n",
      "â”‚   â”œâ”€â”€ classifier.py\n",
      "â”‚   â”œâ”€â”€ config.py\n",
      "â”‚   â”œâ”€â”€ ingest.py\n",
      "â”‚   â”œâ”€â”€ retriever.py\n",
      "â”‚   â”œâ”€â”€ tools.py\n",
      "â”‚   â””â”€â”€ utils.py\n",
      "â”œâ”€â”€ .env\n",
      "â”œâ”€â”€ .gitignore\n",
      "â”œâ”€â”€ A Chatbot for Football Analytics A deep dive into RAG, LLM Orchestration and Function Calling_extracted.txt\n",
      "â”œâ”€â”€ README.md\n",
      "â”œâ”€â”€ main.ipynb\n",
      "â”œâ”€â”€ main.py\n",
      "â””â”€â”€ requirements.txt\n"
     ]
    }
   ],
   "source": [
    "def print_tree(path: Path, prefix: str = \"\", ignore: set = None):\n",
    "    \"\"\"ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ íŠ¸ë¦¬ í˜•íƒœë¡œ ì¶œë ¥\"\"\"\n",
    "    if ignore is None:\n",
    "        ignore = {'.git', '__pycache__', '.venv', 'node_modules', '.vscode', 'chroma_db'}\n",
    "    \n",
    "    items = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name))\n",
    "    items = [i for i in items if i.name not in ignore]\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
    "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "        \n",
    "        if item.is_dir():\n",
    "            next_prefix = \"    \" if is_last else \"â”‚   \"\n",
    "            print_tree(item, prefix + next_prefix, ignore)\n",
    "\n",
    "print(\"ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°:\\n\")\n",
    "print_tree(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cef62",
   "metadata": {},
   "source": [
    "## 3. í•µì‹¬ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "\n",
    "`src/` ë””ë ‰í† ë¦¬ì˜ í•µì‹¬ ëª¨ë“ˆë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cdd79d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì„¤ì • ì •ë³´:\n",
      "   - ì„ë² ë”© ëª¨ë¸: intfloat/multilingual-e5-large-instruct\n",
      "   - LLM ëª¨ë¸: gpt-4o\n",
      "   - ChromaDB ê²½ë¡œ: c:\\Coding\\PAINS-LLM\\PAINS-LLM\\data\\chroma_db\n",
      "   - ì»¬ë ‰ì…˜ ì´ë¦„: kbo_data\n",
      "   - ì‹œì¦Œ ë°ì´í„°: c:\\Coding\\PAINS-LLM\\PAINS-LLM\\data\\raw\\season_2025\n",
      "   - ê²½ê¸° ë°ì´í„°: c:\\Coding\\PAINS-LLM\\PAINS-LLM\\data\\raw\\matches\n",
      "   - Top-K ê²€ìƒ‰: 5\n"
     ]
    }
   ],
   "source": [
    "# í•µì‹¬ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from src.config import (\n",
    "    CHROMA_DB_DIR, COLLECTION_NAME, EMBEDDING_MODEL,\n",
    "    SEASON_DATA_DIR, MATCH_DATA_DIR, LLM_MODEL, RETRIEVAL_TOP_K\n",
    ")\n",
    "\n",
    "print(\"ğŸ“¦ ì„¤ì • ì •ë³´:\")\n",
    "print(f\"   - ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL}\")\n",
    "print(f\"   - LLM ëª¨ë¸: {LLM_MODEL}\")\n",
    "print(f\"   - ChromaDB ê²½ë¡œ: {CHROMA_DB_DIR}\")\n",
    "print(f\"   - ì»¬ë ‰ì…˜ ì´ë¦„: {COLLECTION_NAME}\")\n",
    "print(f\"   - ì‹œì¦Œ ë°ì´í„°: {SEASON_DATA_DIR}\")\n",
    "print(f\"   - ê²½ê¸° ë°ì´í„°: {MATCH_DATA_DIR}\")\n",
    "print(f\"   - Top-K ê²€ìƒ‰: {RETRIEVAL_TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332ebcc",
   "metadata": {},
   "source": [
    "## 4. ë°ì´í„° ì ì¬ (Ingest)\n",
    "\n",
    "JSON ë°ì´í„°ë¥¼ ChromaDB ë²¡í„° ì €ì¥ì†Œì— ì ì¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ ì „ëµ (ë…¼ë¬¸ Section 4.2)\n",
    "- JSONì„ ê·¸ëŒ€ë¡œ ì„ë² ë”©í•˜ì§€ ì•ŠìŒ\n",
    "- **ì„œìˆ í˜• ë¬¸ì¥(Descriptive Sentence)** ì„ ìƒì„±í•˜ì—¬ ì„ë² ë”©\n",
    "- ì›ë³¸ ë°ì´í„°ëŠ” metadataì— ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8de0d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ KBO ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘\n",
      "============================================================\n",
      "ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ: c:\\Coding\\PAINS-LLM\\PAINS-LLM\\data\\chroma_db\n",
      "\n",
      "ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "âš ï¸ ì‹œì¦Œ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: c:\\Coding\\PAINS-LLM\\PAINS-LLM\\data\\raw\\season_2025\n",
      "ğŸ“‚ ê²½ê¸° ë°ì´í„° íŒŒì¼ ë°œê²¬: 2ê°œ\n",
      "   - 2025_POST_MATCH_PITCHING_STATS.json: 168ê°œ ë ˆì½”ë“œ ë°œê²¬\n",
      "   - 2025_REGULAR_MATCH_PITCHING_STATS.json: 7077ê°œ ë ˆì½”ë“œ ë°œê²¬\n",
      "\n",
      "ğŸ“Š ë¡œë“œëœ ë°ì´í„° ìš”ì•½:\n",
      "   - ì‹œì¦Œ ë°ì´í„°: 0ê±´\n",
      "   - ê²½ê¸° ë°ì´í„°: 7245ê±´\n",
      "\n",
      "ğŸ”„ ì‹œì¦Œ ë°ì´í„° ë³€í™˜ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Season: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ ê²½ê¸° ë°ì´í„° ë³€í™˜ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Match: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7245/7245 [00:00<00:00, 52652.00it/s]\n",
      "c:\\Coding\\PAINS-LLM\\PAINS-LLM\\src\\ingest.py:48: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ ChromaDB ì´ˆê¸°í™” ì¤‘... (ë¬¸ì„œ ìˆ˜: 7245)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mingest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ingest_all_data, initialize_vector_store\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ë°ì´í„° ì ì¬ ì‹¤í–‰\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# clear_existing=True: ê¸°ì¡´ DB ì‚­ì œ í›„ ì¬ì ì¬\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[43mingest_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclear_existing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m      8\u001b[39m     doc_count = result._collection.count()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\src\\ingest.py:424\u001b[39m, in \u001b[36mingest_all_data\u001b[39m\u001b[34m(season_dir, match_dir, clear_existing)\u001b[39m\n\u001b[32m    421\u001b[39m documents = prepare_documents(season_data, match_data)\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# 4. ChromaDB ì ì¬\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m vector_store = \u001b[43minitialize_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[38;5;66;03m# 5. ê²°ê³¼ í™•ì¸\u001b[39;00m\n\u001b[32m    427\u001b[39m doc_count = vector_store._collection.count()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\src\\ingest.py:330\u001b[39m, in \u001b[36minitialize_vector_store\u001b[39m\u001b[34m(documents, persist_directory, collection_name)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m documents:\n\u001b[32m    327\u001b[39m     \u001b[38;5;66;03m# ìƒˆë¡œìš´ ë¬¸ì„œë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\u001b[39;00m\n\u001b[32m    328\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ“¦ ChromaDB ì´ˆê¸°í™” ì¤‘... (ë¬¸ì„œ ìˆ˜: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     vector_store = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… ChromaDB ì ì¬ ì™„ë£Œ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersist_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    340\u001b[39m     \u001b[38;5;66;03m# ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:1237\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1236\u001b[39m     ids = [doc.id \u001b[38;5;28;01mif\u001b[39;00m doc.id \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(uuid.uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:1190\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m   1185\u001b[39m         api=chroma_collection._client,\n\u001b[32m   1186\u001b[39m         ids=ids,\n\u001b[32m   1187\u001b[39m         metadatas=metadatas,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1188\u001b[39m         documents=texts,\n\u001b[32m   1189\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1193\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1196\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:532\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    530\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    536\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:115\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    113\u001b[39m     sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:623\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m features.update(extra_features)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    625\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:690\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    688\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    689\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:393\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[32m    391\u001b[39m     trans_features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m] = features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m output_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m output_tokens = output_states[\u001b[32m0\u001b[39m]\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:977\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    974\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    975\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    989\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    990\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:632\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    621\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    622\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    623\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m         output_attentions,\n\u001b[32m    630\u001b[39m     )\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:563\u001b[39m, in \u001b[36mXLMRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    560\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    561\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    245\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:576\u001b[39m, in \u001b[36mXLMRobertaLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    575\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:487\u001b[39m, in \u001b[36mXLMRobertaOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    489\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\PAINS-LLM\\PAINS-LLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from src.ingest import ingest_all_data, initialize_vector_store\n",
    "\n",
    "# ë°ì´í„° ì ì¬ ì‹¤í–‰\n",
    "# clear_existing=True: ê¸°ì¡´ DB ì‚­ì œ í›„ ì¬ì ì¬\n",
    "print(\"â±ï¸ ë°ì´í„° ì ì¬ ì‹œì‘... (CPU ì„ë² ë”©ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "start_time = time.time()\n",
    "\n",
    "result = ingest_all_data(clear_existing=True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "if result:\n",
    "    doc_count = result._collection.count()\n",
    "    print(f\"\\nğŸ‰ ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "    print(f\"   - ë¬¸ì„œ ìˆ˜: {doc_count}\")\n",
    "    print(f\"   - ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ\")\n",
    "    print(f\"   - ë¬¸ì„œë‹¹ í‰ê· : {elapsed_time/doc_count*1000:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf55e26",
   "metadata": {},
   "source": [
    "## 5. ì¿¼ë¦¬ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ (Classifier)\n",
    "\n",
    "ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ `general`, `season_analysis`, `match_analysis` ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classifier import classify_query\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤\n",
    "test_queries = [\n",
    "    \"ì•¼êµ¬ ê·œì¹™ ì„¤ëª…í•´ì¤˜\",              # general\n",
    "    \"í•œí™” ì´ê¸€ìŠ¤ ì˜¬ì‹œì¦Œ ì„±ì  ë¶„ì„í•´ì¤˜\",   # season_analysis\n",
    "    \"ì–´ì œ í•œí™” LG ê²½ê¸° ì–´ë• ì–´?\",        # match_analysis\n",
    "]\n",
    "\n",
    "print(\"ğŸ” ì¿¼ë¦¬ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸:\\n\")\n",
    "for query in test_queries:\n",
    "    result = classify_query(query)\n",
    "    print(f\"ì¿¼ë¦¬: {query}\")\n",
    "    print(f\"  â†’ ë¶„ë¥˜: {result.query_type}\")\n",
    "    print(f\"  â†’ íŒ€: {result.teams}\")\n",
    "    print(f\"  â†’ ì‹ ë¢°ë„: {result.confidence:.0%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810267cc",
   "metadata": {},
   "source": [
    "## 6. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Retriever)\n",
    "\n",
    "Semantic Search + BM25ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retriever import retrieve_for_query\n",
    "\n",
    "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "test_query = \"í•œí™” ì´ê¸€ìŠ¤ ì‹œì¦Œ ì„±ì \"\n",
    "\n",
    "print(f\"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{test_query}'\\n\")\n",
    "results = retrieve_for_query(test_query, top_k=3)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"ê²°ê³¼ {i}:\")\n",
    "    print(f\"  íƒ€ì…: {doc.metadata.get('type')}\")\n",
    "    print(f\"  íŒ€: {doc.metadata.get('teams', doc.metadata.get('team'))}\")\n",
    "    print(f\"  ë‚´ìš©: {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0263fb",
   "metadata": {},
   "source": [
    "## 7. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Chain)\n",
    "\n",
    "ë¶„ë¥˜ â†’ ê²€ìƒ‰ â†’ ìƒì„±ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f455cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.chain import run_analysis\n",
    "\n",
    "# ë¶„ì„ ì‹¤í–‰\n",
    "query = \"í•œí™” ì´ê¸€ìŠ¤ ì˜¬ì‹œì¦Œ ì„±ì  ë¶„ì„í•´ì¤˜\"\n",
    "print(f\"ğŸ’¬ ì§ˆë¬¸: {query}\\n\")\n",
    "\n",
    "result = run_analysis(query)\n",
    "\n",
    "print(f\"ğŸ“Š ë¶„ë¥˜: {result.query_type}\")\n",
    "print(f\"ğŸ“Š íŒ€: {result.teams}\")\n",
    "print(f\"ğŸ“Š ê²€ìƒ‰ ì ìˆ˜: {result.retrieval_score:.2%}\")\n",
    "print(f\"ğŸ“Š ëŒ€ì‹œë³´ë“œ í•„ìš”: {result.needs_dashboard}\")\n",
    "print(f\"\\nğŸ¤– ì‘ë‹µ:\\n{result.response[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288824f",
   "metadata": {},
   "source": [
    "## 8. ëŒ€í™”í˜• ì—ì´ì „íŠ¸ (Agent)\n",
    "\n",
    "Function Callingì„ ì§€ì›í•˜ëŠ” ëŒ€í™”í˜• ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7430c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent import chat\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ëŒ€í™” í…ŒìŠ¤íŠ¸\n",
    "queries = [\n",
    "    \"ì•ˆë…•! KBOì— ëŒ€í•´ ì•Œë ¤ì¤˜\",\n",
    "    \"í•œí™” ì´ê¸€ìŠ¤ ì˜¬ì‹œì¦Œ ì„±ì  ë¶„ì„í•´ì¤˜\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"ğŸ‘¤ User: {q}\")\n",
    "    response = chat(q)\n",
    "    print(f\"ğŸ¤– Assistant: {response.response[:300]}...\")\n",
    "    if response.dashboard:\n",
    "        print(f\"   ğŸ“Š ëŒ€ì‹œë³´ë“œ ìƒì„±ë¨!\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb35289",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ë…¼ë¬¸ ëŒ€ë¹„ êµ¬í˜„ ì°¨ì´ì  ë° ChromaDB êµ¬ì„±\n",
    "\n",
    "### ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ êµ¬í˜„í•œ ë¶€ë¶„\n",
    "| ë…¼ë¬¸ ì„¹ì…˜ | êµ¬í˜„ ë‚´ìš© |\n",
    "|----------|----------|\n",
    "| Section 4.2 (Embedding) | `multilingual-e5-large-instruct` ëª¨ë¸ ì‚¬ìš©, L2 ì •ê·œí™” |\n",
    "| Section 4.3.1 (Query Classification) | CoT í”„ë¡¬í”„íŒ…ìœ¼ë¡œ general/season/match ë¶„ë¥˜ |\n",
    "| Section 4.3.2 (Query Normalization) | RapidFuzz QRatio ìŠ¤ì½”ëŸ¬ë¡œ íŒ€ëª… í¼ì§€ ë§¤ì¹­ |\n",
    "| Section 4.4.1 (Instruct Format) | `\"Instruct: ... Query: ...\"` í¬ë§· ì‚¬ìš© |\n",
    "| Section 4.4.2 (Hybrid Retrieval) | Semantic (0.8) + BM25 (0.2) ì•™ìƒë¸” |\n",
    "| Section 4.5.4 (Function Calling) | ëŒ€ì‹œë³´ë“œ ìƒì„±ìš© ë„êµ¬ ì •ì˜ |\n",
    "\n",
    "### ë…¼ë¬¸ê³¼ ë‹¤ë¥´ê²Œ (KBOì— ë§ê²Œ) ìˆ˜ì •í•œ ë¶€ë¶„\n",
    "\n",
    "| í•­ëª© | ë…¼ë¬¸ (ì¶•êµ¬) | ì´ í”„ë¡œì íŠ¸ (ì•¼êµ¬) |\n",
    "|-----|-----------|------------------|\n",
    "| **ë„ë©”ì¸** | ìœ ëŸ½ ì¶•êµ¬ ë¦¬ê·¸ | KBO í•œêµ­ í”„ë¡œì•¼êµ¬ |\n",
    "| **TEAM_MAP** | ì¶•êµ¬íŒ€ + ëŒ€íšŒ ë§¤í•‘ | 10ê°œ KBO íŒ€ + í•œê¸€ ë³„ì¹­ |\n",
    "| **ë°ì´í„° êµ¬ì¡°** | ë¦¬ê·¸/ê²½ê¸° JSON | ì‹œì¦Œ/ê²½ê¸° JSON (ì•¼êµ¬ ìŠ¤íƒ¯) |\n",
    "| **KPI** | xG, íŒ¨ìŠ¤ ë“± | íƒ€ìœ¨, ë°©ì–´ìœ¨, OPS ë“± |\n",
    "| **Instruct í”„ë¡¬í”„íŠ¸** | \"football dataset\" | \"baseball dataset\" |\n",
    "\n",
    "### ChromaDB ë²¡í„° ì €ì¥ì†Œ êµ¬ì„±\n",
    "\n",
    "```\n",
    "data/\n",
    "â””â”€â”€ chroma_db/           â† ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ ë¡œì»¬ ì˜êµ¬ ì €ì¥ì†Œ ì‚¬ìš©\n",
    "    â”œâ”€â”€ chroma.sqlite3   â† ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "    â””â”€â”€ [UUID í´ë”ë“¤]/   â† ë²¡í„° ì„ë² ë”© ì €ì¥\n",
    "```\n",
    "\n",
    "- **ì €ì¥ ìœ„ì¹˜**: `data/chroma_db/` (config.pyì˜ `CHROMA_DB_DIR`)\n",
    "- **ì»¬ë ‰ì…˜ ì´ë¦„**: `kbo_data` (config.pyì˜ `COLLECTION_NAME`)\n",
    "- **ì„ë² ë”© ì°¨ì›**: 1024 (multilingual-e5-large-instruct)\n",
    "- **ì¸ë±ìŠ¤ ë°©ì‹**: HNSW (ChromaDB ê¸°ë³¸ê°’)\n",
    "\n",
    "> ğŸ’¡ ë…¼ë¬¸ì—ì„œëŠ” PlaymakerAI APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ì§€ë§Œ, ì´ í”„ë¡œì íŠ¸ëŠ” ë¡œì»¬ JSON íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "> ì´ëŠ” API ì—†ì´ë„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ í•œ ì„¤ê³„ ê²°ì •ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB ìƒíƒœ í™•ì¸\n",
    "from src.ingest import initialize_vector_store\n",
    "\n",
    "vector_store = initialize_vector_store(documents=None)  # ê¸°ì¡´ ìŠ¤í† ì–´ ë¡œë“œ\n",
    "collection = vector_store._collection\n",
    "\n",
    "print(\"ğŸ—„ï¸ ChromaDB ë²¡í„° ì €ì¥ì†Œ ì •ë³´:\")\n",
    "print(f\"   - ì €ì¥ ê²½ë¡œ: {CHROMA_DB_DIR}\")\n",
    "print(f\"   - ì»¬ë ‰ì…˜ ì´ë¦„: {collection.name}\")\n",
    "print(f\"   - ë¬¸ì„œ ìˆ˜: {collection.count()}\")\n",
    "print(f\"   - ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL}\")\n",
    "print(f\"   - ì„ë² ë”© ì°¨ì›: 1024 (multilingual-e5-large-instruct)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
